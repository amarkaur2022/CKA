Kubernetes  is a Open-Source system for Automating the deployment, scaling and management of containerized application.
fEATURES 
  - Conatiner Orchestration:
      Manages Conatines  accross a cluster of maschine.
  - Self-healing:
      Resatrts failed containers, repolace them., Kills unresponsive ones.
  - Sacling:
      Automatically increase/descrease resources based on load
  - Load Balancing :
      Distrub straffic to keep the system responsive.
  - Service Discovery: 
      Makes it easy for services to find and talk each other.
  - Configuration Management: 
      Store and  manages secret and application settings.
_____________________________________________________________________________________

CORE CONCEPT:
  1) Container: 
      Container is the basic unit of software in Kubernetes, packaging applications with their necessary code, libraries, and dependencies.
  2) POD:
      The smalles  deployment unit;
      wraps one or more containers.
  3) Node (server):
      A Single maschine (Physical or virtual) in the cluster.
  4) Cluster:
      A group of nodes managede by Kubernetes.
  5) Deployment:
      Tells Kubernetes how to create and manage Pods.
  6) Service:
      Exposes Pod to the network and handels traffic routing.
_____________________________________________________________________________________
Typical Use Case:
- Bild your app in Dockers container. Then you use Kubernetes to:

1 Deploy the containers across fleet to server.
2 Ensure they stay running.
3 Scale them up during traffic spikes.
4 Roll out new versions without downtime.
_____________________________________________________________________________________
KUBERNETS HAS TWO MAIN PARTS 
  1) MASTER(CONTROL PLANE)
  2) WORKER NODES
_____________________________________________________________________________________

COMPONENETS OF MASTER:

- API Server:
    It Provide the interface (Command line Interface) also known as qubecontrol.
- Schedular:
    Assign node to newly created Pods. ((Pod is a small Unit where the container run it. We have multiple nodes and like to run a new container.
    Which node will be assigned this work has been done by the schedular.)
- ETCD:
    Key value, store having all cluster Data.
    we have all the cluster, Different Nodes, Applications , containers and Pods that store the information ETCD
-  Control Manager
    control Manager is responsible for managing the state of the cluster (When something is damage or kaputt then Control manager repair )
_____________________________________________________________________________________
WORKER NODES

- KUBLET: 
    Agent,make sure that containers are running in Pods.
    (It checks wheather the conatianers are working Properly in the Pods yes or NO)

- POD: 
    the Containers are running in the Pod.
    (Checks that all the conatainers are working Properly  with in the POD)
    A single Instance of a running process in a cluster. It can rum one or more conatiners and share the same resources.

- Kube-Proxy:
    Maintain the Network rules for communication with POds.
    eg. we have a cluster, we have inside Network, we have Outside Network, There are many Pods running in the NOde(to run/Maintain all the Things we use Kube-Proxy. 

- Container-runtime:
    A tool is responsible for running containers er. Dockers 
    
_____________________________________________________________________________________
Difference between Docker and the ConatinerD



_____________________________________________________________________________________
ETCD:


_____________________________________________________________________________________
NOTE: if we not move a one pod from one node to another node.
      we can only delete the pod and create a new pod on anothe node where u want 


F: How to get the help for the commands 
    kubectl run --help
F: On which image used to create the new Pods?
    kubectl describe pod newpodname 
    kubectl  get pods-o wide 
F: How many containers are in the pod 
    kubectl describe pod 
F: Why do you think the computer agenty in pod webapp is in error

F: What does the READY column in the output of the kubectl get Pods commans indicate 
    1/2  Running container in PODS/Total container in POD 
F: 
F: How to create the Pods?
    kubectl run ngnix --image=ngnix

F:  How to delete the pod?
      kubectl delete pod webapp
      kubectl delete pods --all 
F: How to make the change in the pod?
      kubectl edit pod podname
F: How to Create a new Pod with the name of the rednis and with the image rednis123?
   Note: Use the POD-Defination YaAML file. And yes the image name is wrong
    kubectl run rednis --image=rednis123 --dry-run -o yaml
    kubectl run rednis --image=rednis123 --dry-run=client -o yaml
    kubectl run rednis --image=rednis123 --dry-run=client -o yaml > rednis.yaml
    cat rednis.yaml

--dry-run: It is only used to check th command wheather it is right or wrong and this is the 
  clint side command. The request will not go to the server 


Note: Use the POD-Defination YaAML file. And yes the image name is wrong
a) how to create a yaml file 
    kubectl create -f rednis.yaml
b) see weather thepod s file is created or not 
    kubectl get pods


F: How to change the image on this pod to rednis. Once done, the pod should be in a running state?
    cat redis.yaml
    vi redis.yaml
    kubectl applay -f rednis..yaml
    kubectl get pods

F: How to show the labels 
    kubectl describe pod rednis
    kubectl describe pod --show-labels 

--dry-run: It is only used to check th command wheather it is right or wrong and this is the 
  clint side command. The request will not go to the server 

 

____________________________________________________________________

A ReplicationController is a legacy Kubernetes controller that ensures a specified number of pod replicas are running at any given time.

--Function of replicaController
    Automatically replaces failed pods
    Ensures the desired number of pods are always running  
    Used in earlier versions of Kubernetes

-REPlication Controller:(what and Why we need this)
    - Replication controller helps us to run multiple instances of a single pod in the kubernetes cluster,
      thus providing high availability.
    - Replication controller can help by automatically bringing up a new pod when existing one fails
      thus the replication controller ensure that the specified numbers of pods are running all the times
      even it 1 or 100

NOTE: we can also use the replication controller when we use the one pod not only multiple pods 
There are two similar terms(same perpouse but not same)
1) Repelation Controller: is the older technology that is replaced by Replica Set 

2) Replica Set: is the new Recomended way to set up replication.

- How to create a Replication Controller defination file 
    rc-defination.yml
- As we any kubernets defination file we have 4 section
  1)  API version 
  2)  Kind
  3)  Metadata 
  4)  Specification  but written as spec NOTE we cn tested the two defination file together 

1) API version: is specific to what we are creating 
2) Kind : As we know oit is Replacation Controller.
3) Meta Data: Add Name, Labels, app type and assign values to them 
4) spec. What is inside the object we are creating.
NOTE: we can copy from the pod -defination file the data into/under the spec template  
____________________________________________________________________
                          LAB 2 Replication controller

F: How to create the Replication  Controller
    kubectl create -f rc-defination.yml 
F: How to see the Replication Controller list 
    kubectl get replicationcontroler 
F: How many Pods are created by the replication Controller
    kubectl get pods
____________________________________________________________________
REPLICA_SET
    A ReplicaSet is the newer version of ReplicationController that 
    also ensures a specified number of pod replicas are running continuously, 
    
    but with more powerful and flexible label selectors.

there is only one differencence between replication controller and Replica set 
- Replica Set requires a selector defination the selector section helps the Replica Set 
  identify what pods fall under it.

Festures of the Replica Set 
    Same purpose as RC: maintain a stable number of pods
    Supports set-based label selectors
    Commonly used as part of a Deployment

- In Replica set there are three sections 
    1) Template 
    2) replicas 
    3) Selector 

_____________________________________________________________________


____________________________________________________________________
                            LAB Replica Set 

F: How to create the Replica Set
    kubectl create -f replicaset-defination.yml 
F: How to see the Replic set list 
    kubectl get replicaset 
F: How to create a Pod in the replicaSet?
    kubectl describe replicaset
F: How to delete the Replicaset Pod
    kubectl delete replicaset myapp-replioaset
    (Also delete all Underlying pod)
F: How to replace and update the Replica set
    Kubectl replace -f relicaset-defination.yml
F: How to kube control scale command to scale the Replicaset 
    kubectl scale -replicas=6 -f replicaset-defination.yml
F: How to check the apiversion of replicaset 
    kubectl api-resources | grep replicaset
F: 
____________________________________________________________________

                        LABELS AND SELECTORS 

         
____________________________________________________________________
                    LAB  Lables and Selsctors  
F: How to create an Labels 
       kubectl describe pod nginx | less
F: How to attach the new Labels 
      kubectl label pod nginx env=testing 
F: How to overright the label 
      kubectl label --overwrite pod ngnix env=prod
F: How to delete the Label
      kubectl label pod ngnix env-
F: How to see/display the Labels 
      kubectl get pods --show-labels 


____________________________________________________________________
                    DEPLOYMENT

Deployment is used to manage the automatic life cycle of the pods. 
It create updates scaling and rollback of the Instances when any eroor occur
NOTE: Deployment create an Replicaset automaticall. When we run
      kubectl get replicaset
      You will see the replicaset file.


Features of the Deployment
1. Manages Replica Pods
    It makes sure that a specified number of Pods are always running.
2. Self Healing
    If a pod crashes or gets deleted, the Deployment will automatically create a new one to replace it.
3. Rolling Updates
      You can update your application image or configuration, and the Deployment will roll out changes gradually with zero downtime.
4. Rollback support 
      If something goes wrong during an update, you can easily rollback to a previous stable version.

_____________________________________________________________________
              How to create a Deployment file 
- The Deployment file is same like a Replicaset 
    only in Kind: DEPLOYMENT
- Contents of the File
    API Version:
    Metadata: 
      name 
      labels
    spec:
        template:
          metadata:
          
    replica: 
    selector:
_____________________________________________________________________
              LAB 3 deployment

F: How many pods are created
      kubectl get POD 
F: HOw many replica set are created 
      kubectl get rs
F: HOw many deployments are created 
      kubectl get deployments
F: How many deployments exists on the system. We just creaed a one Deployment 
      kubectl get deployments 
F: How many rs are created
      kubectl get rs
F: How many PODS are created 
      kubectl get Pod
F: Out of all the PODs how many are READY
      kubectl get deployments 
F: What is the image used to create the POD in the new deployments 
      kubectl describe pod frontend-deployment-7fd8cdb696-stmbx  
F: Create a new Deployment using the deployment-definition-1.yaml file located at /root/.
A:    pwd (to go to the root)
      ls (see the list of the deployments files)
      kubectl create -f deployment-defination-1.yaml (to create a yaml file on the root)
      vi deployment-defination-1.yaml ( to make the changes on the yaml file)
      kubectl create -f deployment-definition-1.yaml (to create a .yaml file on the root)

 
F: Create a new Deployment with the bwlow attributes using your own deployment defination file 
    NAME : http-frontend:
    replica: 3 
    image:httpd:2.4-alpine
A:  kubectl create  deployment --help ( to get the help of the deployments commands)
    kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replica=3

NOTE: If u forget to create a replica then use 
      kubectl scale deployment httpd-frontend --replicas=3


_____________________________________________________________________


            Kubernetes services
Kubernetes services enable the communication betwen the various components and with in the outside the application 
it helps us to connect applications together
-- typically using a stable DNS name and IP address.


Types of services

  1) NodePort
  2) ClusterIP
  3) LoadBalancer 

Curl: Curl is used to talk with the AIP server (u can send or get the data from the networl
curl is a command-line tool for transferring data to or from a server using protocols like HTTP, HTTPS, etc.

 1) Why we need Services?

    - Pods are temporary â€” they can die, restart, or be replaced.
    - Each Pod gets a unique IP, which may change.
    - A Service provides a stable way to reach Pods, regardless of changes in the underlying Pod IPs.

2) Types of Services:

    - ClusterIP (default): Exposes the Service on an internal IP in the cluster. Accessible only within the cluster.
    - NodePort: Exposes the Service on a static port on each Nodeâ€™s IP. Allows external traffic to access the Service via <NodeIP>:<NodePort>.
    - LoadBalancer: Uses a cloud provider's load balancer to expose the Service externally.
    - ExternalName: Maps a Service to a DNS name (e.g., external database).

3) How Services Work:

    -  A Service uses labels to find the set of Pods it targets.
    -  For example, if all backend Pods have a label app=backend, the Service will route traffic to all matching Pods.

4) DNS Names in Kubernetes:

    - Services are automatically assigned a DNS name.
    - For example, a Service named db-service in the marketing namespace can be accessed at:
          db-service.marketing.svc.cluster.local
5) Ports:

    - Services define a port to expose and optionally a targetPort (the port on the Pod).
    - Example: A Redis Service might expose port 6379.

NOTE: If an application wants to connect to a Redis database, it doesnâ€™t need to know the IP of the Redis Pod.
      It just connects to redis-service:6379, and the Service routes traffic to the correct Pod(s).

________________________________________________________________
                LAB 4 Services

F: How many Services exist on the system?
   In the current(default) namespace
A:   Kubectl get service
     kubectl get svc

F: What is the type of the default kubernetes service?
      kubectl get svc

F: What is the targetport configured on the kubernetes service?
      kubectl describe svc kubernetes

F: How many labels are configured on the kubernetes service?
     kubectl describe svc kubernetes

F: How many Endpoints are attached on the kubernetes service?
    kubectl describe svc kubernetes

F: How many Deployments exist on the system now?
   In the current(default) namespace
    kubectl get deploy

F: What is the image used to create the pods in the deployment?
      kubectl describe deploy simple-webapp-deployment


Create a new service to access the web application using the service-definition-1.yaml file.
Name: webapp-service
Type:       NodePort
targetPort: 8080
port:       8080
nodePort:   30080
selector:
 name:      simple-webapp

A:  ls
    cat service-defination-1.yaml
    vi  service-defination-1.yaml'(update the file) 
    kubectl create -f service-defination-1.yaml
    
   _____________________________________________________________________
What is a Namespace in Kubernetes?
    A namespace is a way to divide a Kubernetes cluster into multiple virtual environments,(separate, logical parts)
    like creating separate compartments for different teams, projects, or applications.

F: Why use Namespaces?
   - To separate environments (e.g., dev, test, prod)
   - To manage multiple teams or projects in one cluster
   - To apply resource limits and access control per group

1) Isolation:(à¤µà¤¿à¤­à¤¾à¤œà¤¨ à¤”à¤° à¤…à¤²à¤—à¤¾à¤µ)
    - Namespaces provide logical isolation between groups of resources (like Pods, Services, Deployments).
    - For example, two teams can each have a web-app running in separate namespaces (team-a and team-b) without interfering with each other.

2) Resource Scoping:
    - Most Kubernetes resources (like Pods, Services, ConfigMaps) live within a namespace.
    - Some resources (like Nodes, PersistentVolumes) are not namespaced.

3) Access Control:
    - Role-Based Access Control (RBAC) rules can be applied per namespace, giving fine-grained permissions.

4) Resource Quotas:
    - Namespaces can have quotas (CPU, memory, number of objects) to prevent one team from using up all cluster resources.

5) Default Namespace:
    - If you donâ€™t specify a namespace, Kubernetes uses the default namespace.
______________________________________________________________________
              LAB 5 Namespaces 

F: How many Namespaces exist on the system?
      kubectl get namespaces 
      kubectl get ns

F: How many pods exist in the research namespace?
      kubectl get pods --namespace=research
      kubectl get pods --n=research

F: Create a POD in the finance namepsace. Use the spec given below
      kubectl run redis --image=redis -n=finance
    

F: To see wheather the pod is created in the finance
      kubectl get pod -n=finance

F: Which namespace has the blue POD in it?
      kubectl get ns 
      kubectl get pods  --all-namespaces (this is used to display the namespaces AND all the pods
      kubectl get pods -A

F: What DNS Name should the Blue application use to access the database db-service in its own namespace-marketing
   You can try it in the web application UUUI. USe port 6379
      kubectl get pods -n=marketing
      kubectl get svc -n=marketing

F: What DNS Name should the Blue application use to access the database 'db-service' in the devnamespace
   You can try it in the web application UUUI. USe port 6379
      kubectl get svc -n=dev
      

__________________________________________________________

Imperative und Declarative 



__________________________________________________________
                LAB 6 (Impariative Commands)
F: Deploy a pod named nginx-pod using the nginx:alpine image.
      kubectl run nginx-pod --image=nginx:alpine

F: Deploy a redis pod using the redis:alpine image with the labels set to tier=db.
      kubectl run redis --image=redis:alpine --labels="tier=db"

F: Create a service named redis-service to expose the existing redis pod within the cluster on port 6379.
      kubectl expose pod redis --port 6379 --name redis-service
      kubectl get svc

F: Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas.
       kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3
       kubectl get deploy

F: Create a new pod called custom-nginx using the nginx image and run it on container port 8080.
       kubectl run custom-nginx --image=nginx --port=8080

F: Create a new namespace called dev-ns.
        kubectl create namespace dev-ns

F: Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2 replicas.
        kubectl create deployment redis-deploy --image=redis --replicas=2 -n  dev-ns
        kubectl get deployment -n dev-ns

F: Create a pod named httpd using the image httpd:alpine in the default namespace.
Then, create a service of type ClusterIP with the same name (httpd) that exposes the pod on port 80.
      kubectl run httpd --image=http:alpine --port 80 --expose=true 
__________________________________________________________________

Chapter 2 Schudeling 

LAB 7

NOTE When u want to schedule the port means(u delete the pod from one Node to another)
      Youu habe to do 4 steps 
        kubectl get pods 
        Kubectl get pods -o wide(to see in which node is the pod)
        vi nginx.yaml(open the pod and change the node name Manually and save it)
        kubectl replace --force -f nginx.yaml(delete the pod pod from one node and create a new pod to another node)
                                             (this command delete the data from the pod )
                                             ( when we want that the data wilkl not be deleted we have to use  Persistent Volume (PV) )

F: A pod definition file nginx.yaml is given. Create a pod using the file.
   Only create the POD for now. We will inspect its status next.
      kubectl get pods
      ls (to see the files)
      cat nginx.yaml (display all the data from the file)
      kubectl create -f nginx.yaml (to create a pod though the file)
      kubectl get pods

F: What is the status of the created POD?
      kubectl get pods 

F: Why the pod is the pending state?
   Inspect the environment for various kubernetes control plane components.
A:    kubectl describe pod nginx
      kubectl get pods -n kube-system

F: Manually schedule the pod on node01.
   Delete and recreate the POD if necessary.
      ls
      vi nginx.yaml (make changes nodeName: node01)
      kubectl get pods 
      kubectl  get pods --watch (to watch the pods wheather it make any changes )
      kubectl replace --force -f n
F: Now schedule the same pos on the controlplane node
   Delete and recreae the pod if necessary
      kubectl get pods 
      vi nginx.yaml(make the changes) 
      kubectl replace --force -f nginx.yaml
      kubectl get pods
      kubectl get pods -o wide 

      _________________________________________________________________________________
                    Labels and selectors  LAB  7

F: We have deployed a number of PODs. They are labelled with tier, env and bu. How many PODs exist in the dev environment (env)?
   Use selectors to filter the output
        kubectl get pods 
        kubectl get pods --selector env=dev 
        kubectl get pods --selector env=dev | wc -l (it display number of lines with header)
        kubectl get pods --selector env=dev --no-headers| wc -l(it display the nummber of lines without header)
NOTE: first  label=podName 
             env=dev 

F: How many PODs are in the finance business unit (bu)?
        kubectl get pods --selector bu=finance --no-header | wc -l

F: How many objects are in the prod environment including PODs, ReplicaSets and any other objects?
        kubectl get all  --selector env=prod --no-header | wc -l
NOTE all is use to see all the obejects are created in the environment prod


F: Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
        kubectl get all --selector env=prod,bu=finance,tier=frontend

F: A ReplicaSet definition file is given replicaset-definition-1.yaml. Attempt to create the replicaset; you will encounter an issue with the file. Try to fix it.
  Once you fix the issue, create the replicaset from the definition file.
        ls
        cat replicaset-definition-1.yaml 
        kubectl create -f replicaset-definition-1.yaml 
        vi replicaset-definition-1.yaml (change the name of the tierName in the selector)
        kubectl create -f replicaset-definition-1.yaml
        kubectl get rs

___________________________________________________________________
tant and tolaration:

tant and tolerations areused to set the restrictions.
On which Node the pod can be scheduled

Tains and toleration doesnot tell the pod to go to a particuler Node,
instead tell the node to accept only pods with certain tolerations
NOTE: 
the tanits are set on Nodes.
the toleration are set on the Pods
Now the 

Syntax:   kubectl taint nodes name key=value:taint-effect
eg.       kubectl taint nodes node1 app=blue:Noschedule

1) taint-effect: 
    defines what would be happend to the pods, if they do not tolerates the taint there are three taint effects.
    - NoSchedule:
      Which means that the pod will not be scheduled on the node.
    - PreferNOSchedule:
      Which means the system willl try to avoid placing a pod on the nodes and that is no gauranted.
    - NoExectute:
      Which menas that the new pod will not be scheduled on the node and existing pods on the node. if any will be evicted
      means they do not tolerate the taints 

Toleration :
      Tolaration are added to pods. To add the toleration to pods first pull the defination file and change the tolerations: under spec:
      How to write in the file:
      toleration: 
      key:"app"
      operator: "Equal"
      value:"blue"
      effect: "NoSchedule"

F:   how to see wheather the tent are attached there 
     kubectl describe nodes worker01 | less 
__________________________________________________________________
      Lab 8 Taint and toleration 

 F: How many nodes exist on the system?
    Including the controlplane node.
 A:    kubectl get nodes

 F: Do any taints exist on node01 node?
 A:     kubectl describe node node01

F: Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule
A:      kubectl taint node node01 spray=mortein:NoSchedule
        kubectl describe node node01  (to check wheather the taints are created are not)

F:  Create a new pod with the nginx image and pod name as mosquito.
A:      kubectl run mosquito --image=nginx

F: What is the state of the POD?
A:      kubectl get pods

F:  Why do you think the pod is in a pending state?
A:      kubectl describe pod mosquito

F: Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.
   Image name: nginx
   Key: spray
   Value: mortein
   Effect: NoSchedule
   Status: Running

A:      kubectl run bee --image=nginx --dry-run=client -o yaml
        kubectl run bee --image=nginx --drs-run=client -o yaml > bee.yaml (recreate a file)
        vi bee.yaml
              tolerations:  
               - key: spray
               value: mortein
               effect: NoSchedule
               operator: Equal
        kubectl create -f bee.yaml

F: Observe that the bee pod has been scheduled on node node01 due to the toleration that has been configured for the pod.
        kubectl get pods 
        kubectl get pods --watch

F: Do you see any taints on controlplane node?
A:       kubectl describe pod controlplane

F: Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
A:       kubectl taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule-

F: What is the state of the pod mosquito now?
         kubectl get pods 

F: Which node is the POD mosquito on now?
          kubectl get pods -o wide


__________________________________________________________
Node Selector 
  A node selector is a simple way to tell a Pod which nodes it can run on based on labels.
      Nodes have labels (e.g., disktype=ssd).
      Node selector in a Pod specifies these labels.
      Kubernetes will schedule the Pod only on nodes matching those labels.

We can select the pods that the pod run only on the specific nodes: there are two ways to doe that
1) Using the node selector (Simple and eiser method)
    - open the pod defination yaml file 
    -  to limt this pod we add a new propertie as a     nodeSelector  under spec: section
    -  you have to write nodeSelector: Large (where the large comes from)
    -  you have to declare the large as a label and selector 
    -  to specify the labels on the Nodes 

Syntax:      Kubectl label nodes <node-name> <label-key> = <label-value>
eg:          lubectl label nodes node-1 size=Large

    - this is very simple example What is when we want to put the pod on the not small and not on the large Node
      In this case we have to use NODE AFFINITY
------------------------------------------------------
NODE AFFINITY:

Node Affinity is used to control which nodes a Pod can be scheduled on--- based on the labels assigned on the nodes.
This is the best to tell the Kubernetes Scheduler.
eg i want this pod run on the nodes that  matches the things(location , speicher, hardware.......)

ðŸ”¸ How is it Different from nodeSelector?

Feature	            nodeSelector	            nodeAffinity
Type	              Simple key-value match	  Advanced expressions supported
Flexibility	        Limited	                  More flexible (AND, OR, ranges)
Priority Support	  No	                      Supports soft rules (preferred)


ðŸ”¸ Types of Node Affinity
1) requiredDuringSchedulingIgnoredDuringExecution(Hard rule)
        Pod will not be scheduled if node doesn't match
        Once running, the rule is ignored

-------   DuringScheduling: is the state where a pod does not exist and created for the first time.

F: What happend what if the nodes  with matching labels are not available or forgot it ?
A: now i have to use the node Affinity. If i used 
    1) requiredDuringSchedulingIgnoredDuringExecution  (Hard rule)
        the Scheduler will mandate that the pod be placed on a node with the given affinity rules 
        if it cannot find one, the pod will not be scheduled.
NOTE: this type is ued when the placement of pod is crucial. if the maching nodes does not exists the pod will not be scheduled.


---- if the placement of Pod is less important than running the workload itself in the t case we used
    2)  preferredDuringSchedulingIgnoredDuringExecution   (Soft rule)
          where the matching node is not found. the Scheduler will not simply ignore node affinity rules and place the pod of any node.
NOTE: this is the way to tell the scheduler 

---------- DuringExecution:  is a state where the pod has been running and a chane is mede in the Environment that affects on the node affinity 
          Such as change in the label of the node.
eg. Some one remove the label, but we are not able to change the labels on the Node.

---- if you want to change any Pod that are running on Nodes 

        Kubernetes tries to schedule to a matching node, but can skip if none match

3)  requiredDuringSchedulingRequiredDuringExecution (not stable)
        Rule is enforced both at scheduling and runtime
___________________________________________________

____________________________________________________
LAB 9 
_____________________________________________________
Daemon Set 
  -  Daemon sets are like the replica sets
  -  It helps you to deploy multiple instances of pods.
  -  But it runs one copy of pod on each node in your cluster.
  -  Whenever the new  node is added to the cluster a replica of the pod is automatically added to that node.
  -  And when a node is removed then the pod is automatocally removed.
  -  It ensure that the one copy of the  pods is always present in all nodes in the cluster 

NOTE: runs one copy of pod---- What is in the pod --- What it do 
ðŸ” What is inside that Pod?  
      The Pod contains whatever you define in the DaemonSet YAML configuration.
      Typically, DaemonSets are used to run background or system-level tasks on every node.

ðŸ”§ Common examples of what's inside:
      -  Log collectors (e.g., Fluentd, Filebeat)
      -  Monitoring agents (e.g., Prometheus Node Exporter)
      -  Security agents or antivirus
      -  Network plugins or storage daemons
Good example is
  (Kube-proxy) for every workerNode 
  (Weave-net) for Networking Solution agent on each Node in a Cluster 
____________________________________________________________
      LAB 10
F:  How many DaemonSets are created in the cluster in all namespaces?
    Check all namespaces
A:   kubectl get Daemonsets -A

F: Which namespace is the kube-proxy Daemonset created in?
A:   kubectl get Daemonsets -A

F: Which of the below is a DaemonSet?
A:   kubectl get all --all-namespaces
     kubectl describe Daemonsets kube-proxy -n kube-system

F: On how many nodes are the pods scheduled by the DaemonSet kube-proxy?
A:   kubectl describe daemonset kube-proxy --namespace=kube-system

F: What is the image used by the POD deployed by the kube-flannel-ds DaemonSet?
A:   kubectl describe daemonset kube-flannel-ds --namespace=kube-flannel 

F: Deploy a DaemonSet for FluentD Logging.
   Use the given specifications.
   Name: elasticsearch
   Namespace: kube-system
   Image: registry.k8s.io/fluentd-elasticsearch:1.20
A: - kubectl create deployment elasticsearch -n kube-system --image=k8s.gcr.io/fluentd-elasticsearch:1.20 --dry-run=client -o yaml 
   - kubectl create deployment elasticsearch -n kube-system --image=k8s.gcr.io/fluentd-elasticsearch:1.20 -o yaml > fluent.yaml
        MAKE CHANDES IN THE FILE #
        kind: Daemonset 
        delete the replica/delete line 13(strategy {})/ lat line status:{}
    -  kubectl create -f fluentd.yaml
    -  kubectl get ds -n kube-system



    
______________________________________________
  Static pod (Chapter 76)

Static pods are directly managed by the kubectl on a specific node, rather than the kubernetes API server/scheduler
All the master Nodes are the Components of the static pod 
- Kube/API-server
- Kube Scheduler
- Kube-controller-manager
- ETCD
- Cloud Controll Manager(Optional)

F: What happend when all the above Components  in the cluster  are not there. How kubelet(Worker Node) create the Pods USW.....
A:  -  The Kubectl can manage the Node Independently
    -  we Installec the Kubelet and docker(Containers) Installed 
    -  There is no kubernetes cluster and all the Components of the Master Nodes
    -  To create a Pod we need the detail of the Pod in a Pod defination file.
    -  Now we have to configure the Kubelet to read Directory on the server 
    -  Place the defination file in htis directorsy
    -  It checks and read the file and create the Pods on the host.
    - If the application crasch the kubectl restart it.
    -  If changes are made the kubelet recreate the pod 
    - If remove the file, the pod will be deleted 
    - THESE PODS ARE KNOWN AS STATIC PODS
NOTE : we cannot create the pods with the help of relicas, deployments usw----

IMPortant 
    1) View and Config theis option
    2) use Methods to set the cluster 
        -  Check the pod-manifest-path in the kubelet services file 
        -  When not find look at the config file 
        -  Within the Configfile find StaticPodPath
        -  We Cannot use the Kubectl command because it is used with the Api-server 
        -  


_________

LAB 11
F: How many static pods exist in this cluster in all namespaces?
A: kubectl get pods -A
      To know that these are the static PODs
      The Pod name is extened/append with the node name.
F: How to see the static pod in detail 
    kubectl get pod coredns-77456fff-brwnd -n kube-system -o yaml 

F: Which of the below components is NOT deployed as a static pod?
A:    kubectl get pods -A

F: Which of the below components is NOT deployed as a static POD?
A:   kubectl get pods -A
 
F: On which nodes are the static pods created currently?
A: kubectl get pods -A

F: How to see where is the config file is srtored (I think it is falsch )
AAAA: kubectl get configmap -n kube-system kubeadm-config -o yaml

__________________________________

Priority


_______
LAB 12
______
Multiple schedular 

We use more than one Schedular in a Cluster.
Every Schedular uses a seprate Configuration file and each file having its own Schedular name(my-schedular-2.service)
F: How its work if u deploy the schedular as a pod?
A:  -  We create a pod defination file and specifiy the kubeconfig property
    -  Under Sspec: Conatiner:  Command: write the path of the config file.
    -  that has the authentication information to connect to the kubernetes API server.
    -  create a each Scheduler as  yaml file and write the (LleaderElection) 
    -  leader erection is used when u have multiple copies of the scheduler running on the different master node.
    -  If multiple copies of the same Scheduler are running on different nodes, only one can be activ at a time 
       and thats why the leaderElection help who will lead the scheduling activities.
Note:  If the scheduler was not configureed correctl, then the pod will continue to remain in a 
       pending state(look Pods logs under Kubectl describe command)
       an if good is it will in the running state.


F: How to look that which schedular is picked up 
    kubectl get events -o wide(Source)
F: If we have the running issues how to see the logs ?
    kubectl logs my-custom-schedular --name-space=kube-system
_______________________________________________________________________________________
          LAB 13
F: What is the name of the POD that deploys the default kubernetes scheduler in this environment?
A:    Kubectl get pos -A

F: What is the image used to deploy the kubernetes scheduler?
   Inspect the kubernetes scheduler pod and identify the image
A:    kubectl describe pod kube-schedular-controlplane -n kube-system


F: We have already created the ServiceAccount and ClusterRoleBinding that our custom scheduler will make use of.
   Checkout the following Kubernetes objects:
      ServiceAccount: my-scheduler (kube-system namespace)
      ClusterRoleBinding: my-scheduler-as-kube-scheduler
      ClusterRoleBinding: my-scheduler-as-volume-scheduler
Run the command: kubectl get serviceaccount -n kube-system and kubectl get clusterrolebinding
A:   kubectl get sa my-scheduler -n kube-system    ( to check the service account)

F: Please create a ConfigMap that the new scheduler will utilize, implementing the concept of ConfigMap as a volume.
   A ConfigMap definition file named my-scheduler-configmap.yaml has
   been provided at the /root/ path. This file will be used to create a
   ConfigMap with the name my-scheduler-config, utilizing the content
   from the file located at /root/my-scheduler-config.yaml.
A:     kubectl create configmap my-scheduler-configmap --from-file=/root/my-scheduler-config.yaml -n kube-system
       kubectl get configmap my-scheduler-configmap -n kube-system


F: Deploy an additional scheduler to the cluster following the given specification.
    Utilize the manifest file located at /root/my-scheduler.yaml. Ensure that you are using the same image as that of the default Kubernetes scheduler.  
    To verify the image used by the default Kubernetes scheduler, execute the following command:
    kubectl describe pod kube-scheduler-controlplane --namespace=kube-system
Note : Deploying the new scheduler may take a few seconds to reach a running state.
A.     kubectl get pods -A
       kubectl describe pods my-scheduler-controlplane -n kube-system | grep Image  (to see only imagename)(copy the image name)
       vi my-scheduler.yaml (change the image name)
       kubectl create -f my-scheduler.yaml( to create the pod as yaml file) 
        kubectl get pods  -n kube-system( to see the yaml file wheather the file is created or not) 

F:   Please modify the provided Pod manifest file located at /root/nginx-pod.yaml 
     to specify that the Pod should be scheduled by your custom scheduler, which is named my-scheduler.
     After updating, create the Pod in the default namespace and verify it is scheduled by your custom scheduler.
Note : The pod may take a few seconds to reach a running state.

A: ls
   vi nginx-pod.yaml( add the schedular name under spec:)
   kubectl create -f nginx-pod.yaml
  (u have to create the command because the yaml file is always saved in the local and pods are on the cluster thats why u have to upadate the cluste through this command)
    kubectl get pods (check wheather the pods is running or not) 
_________________________________________
      Admission Controllers      (Securing kubernetes)

-  We are running the commands from the command lines.
-  Kubectl utility to perform various kind of Operations on our kubernetes cluster.
1) we send a request to create a pod ---> the request goes to API server ---> then the pod is created ---> ths information is saved in the Etcd database
2)  when request hit the API server ---> goes through an anthentication process ---> create a pod 
    (authentication process is responsible identify the user who send the request and making sure the user is valid 
3) kubectl ---> Authentication ---> Authorization ---> create a pod 
    request goes to Authorization process check if the user has permission performed that operation this is done throught th e role-based access controls.
F: What is role-based control:
    we can place different kind of restrictions such as allow or deny with particular role like create list or delete depÃ¼loyments or services.
F: Why we use the Admission Controllers
    - Security
    - Policy Enforcement
    - Automatic Modification
    - Consistency
4) kubectl ---> Authentication ---> Authorization ---> Admission Controllers ---> create pod 

-  Admission controllers has additional operations before the pod is created 
      -- Always pullimages (when the pod is created it will pull the images)

      -- DefaultStorageClass ( observe the creation of the PVC automatically add default storage class to them)

      -- EventRateLimit

      --NAmespaceExists

    -- NamespaceAutoProvision (create automatically the namespace if it doesnot exists)

eg..  We craete a pod with name nginx and image name is also nginx and in the namespace blue 
      the namespace will not be find in the it will be rejected 
       kubectl ---> Authentication ---> Authorization ---> Admission Controllers checks wheather the namspace is find when yes it created a pod otherwise it will be rejected


F: How to see the enable Admission Controller bydefault 
    kube-apiserver-h | grep enable-admission-plugins

_________________
LAB  14

F:  Which admission controller is not enabled by default?
A:    kubectl get pods -n kube-system
      kubectl exec -it kube-apiserver-controlplane -n kube-system --kube-apiserver -h | grep 'enable-admission-plugins'

F: Which admission controller is enabled in this cluster which is normally disabled?
      vi /etc/kubernetes/manifests/kube-apiserver.yaml
      grep enable-admission-plugins /etc/kubernetes/manifests/kube-apiserver.yaml

F: Create an nginx pod in the blue namespace. Please note that the blue namespace does not currently exist. Do not create the blue namespace at this time.
   Execute the following command to deploy a pod using the nginx image within the blue namespace:

F: 
      vi /etc/kubernetes/manifests/kube-apiserver.yaml 
      kubectl get ns

F:  Delete the existing PVC named myclaim:
      kubectl delete pvc myclaim

F: Reapply the same manifest:
      kubectl apply -f myclaim.yaml
F: Check the status of the PVC:
    kubectl get pvc myclaim

F:   Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins.
A:   ps -ef | grep kube-apiserver | grep admission-plugins
______________________________________________
Validating Controller (how we can controll our Admission Controller)
example Namespace:
It can  help valdate if a namespace already exists, and reject the request when it doesnot exists

There are two type of bildin Admission Controller 
1) Mutating Admission Controller 
      are those that can change the request 
      generally, mutating admission Controller are invoked first followed by valadating admission Controllers.if it was run th eother way then it will always be rejected.

2) Valaditing Admission Controller
      are those that can validate the request and allow or deny it and 
      there may be Admission controller that can both that can mutate a request as well as Validate the request

We can also create our own Admission Controller. 
there are two special external admission Controller availabe

1) MutatingAdmissionWebhook
2) ValidatingAdmissionWebhook

F: What is a webhook
    it is a service on the server to create our own code own logic for Admission Controll
    we can configure the webhook to point andit hosted ob in the kubernetes cluster or outside our server
    When i send the request it always hit the webhook ---> make admission webhook server ---> by passing Admission Review object ---> json format
    it the request is allowed it will be TRUE and if it rejected it will be FAULT
F: How do we set this up 
A:   depoly the webhook server with own logic and configure the webhook on the kubernetes by creating the webhook configuration 

-
____________________________


____________________________

Application Logs
Various loggin mechansims in Kubernetes
we create a pod with the same Docker image using the pod defination file 

F: How to loggin Docker?
    docker run kodekloud/event-simulator

F: If i run the docker container in the background in a detechmode using -d option, I would see the logs,
    docker -d kodekloud/event - simulator 

F: If i wolud like see the log in the 
      docker logs -f ecf   (-f display the live log trail)

NOTE: we create a pod with the same Docker image using the pod defination file once the pod is running, we can view the logs 
    -  kubectl create -f event-simulation.yaml (create a podd)

    -  kubectl logs -f event-sim ( see the log files)
       

If the multiple conatiners in the pod you must specify the name of the conatiner to see the log file 
    -    kubectl logs -f event-simulator-pod  event-simulator 
          - event-simulator-pod: this is the name of the pod 
          - event-simulator: this is the name of the first container in the pod 

________________________________________
LAB 

F:  To see the logs?
A:      kubectl logs webapp-1

F:   We have deployed a new POD - webapp-2 - hosting an application. Inspect it. Wait for it to start.
A:      kubectl logs webapp-2 event-simulator ( it there are two container are running on the pod then write the name of the container to see the logs)

_______________________________________________
Rolling Updates and Rollbacks in a Deployment

F:  What is rollout and versioning in Deployment?
A:    -  When u first create a deployment, It triggers a rollout
      -  A new Rollout create a new deployment revision, (revision 1)
      -  In future the container is updated to a new one, a new rollout is triggered and 
      -  new deployment revision is ceated  (_Revision 2) 

______________________________________________
LAB Rolling Update 

F:  We have deployed a simple web application. Inspect the PODs and the Services
    Wait for the application to fully deploy and view the application using the link called Webapp Portal above your terminal.
A:        kubectl get pods 
          kubectl get deploy 

F:  Run the script named curl-test.sh to send multiple requests to test the web application. Take a note of the output.
    Execute the script at /root/curl-test.sh.
A:         ./root/curl-test.sh

F: Inspect the deployment and identify the nummber of the Posds deployed by it 
A:         kubectl get deploy 

F:   What container image is used to deploy the applications?
A:         kubectl describe deploy  frontend 

F:   Inspect the deployment and identify the current strategy
A:         kubectl describe deploy frontend 

F:   If you were to update the application now what would happen?
A:         pods are upgrading few at a time
            (by default is that first pod willl be down and than upgrade it . then secon will down and then  upgrade it..

F:   Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2
     Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

A:   1 way
         kubectl describe deploy frontend (first see the container name)
         kubectl aet image deploy frontend simple-webapp=kodekloud/webapp-color:v2
      2 Way
          kubectl edit deploy frontend (and change the image Name)

F:    Run the script curl-test.sh again. Notice the requests now hit both the old and newer versions (If checked immediately). However none of them fail.
      Execute the script at /root/curl-test.sh.
A:         ./curl-test.sh
            bash /root/curl-test.sh ( that means the application is updated)

F:    Up to how many PODs can be down for upgrade at a time
      Consider the current strategy settings and number of PODs - 4

          
F:     Change the deployment strategy to Recreate
      Delete and re-create the deployment if necessary. Only update the strategy type for the existing deployment.
A:         kubectl edit deploy frontend 
          and change the thing what they need (delkete the after strategie 3 lines bis type)

F:     Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v3
       Do not delete and re-create the deployment. Only set the new image name for the existing deployment.
A:             kubectl set image deploy frontend simple-webapp=kodekloud/webapp-color:v3

F:     Run the script curl-test.sh again. Notice the failures. Wait for the new application to be ready. Notice that the requests now do not hit both the versions
      Execute the script at /root/curl-test.sh.
A:               ./curl-test.sh
_____________________________________________________________
Commands and Arguments in Dockers
BYDEFAULT it sleep s for 5 seconds

To run a docker containers 
      - docker run ubuntu( to run the instance and image immediadely)
      - docker PS (list to see the running containers)
      -  docker ps -a (list all the conatiners including stopped containers)

F: What we run the conatiners 
      - Containers have a specific task/ process 
      -  to run a host an instance of a web server 
      -   application server or a database 
      -  or simpoly to carry out some computation or analysis.
      - when it completed its wtask it exists 
Note   the Container only lives as long as the process inside it is alive,
      if the website inside the conatiner is stopped or crashes, the container exits 

F: We can also do it automattically that when th container run the sleep command will run/invoke  automaticallly 
     -  We have to save in the startup.
     -  TRNTRYPOINT is the command thet is run at startup 
     -  CMD  is the default parameter passed to the command 
     -  the entry point instruction is a command instruction thet you can specify the programm that will run when the containers starts
     -  ARGS option in the pod defination file we override the cms instruction in the docker file 
    
____________________________________________________________
  Commands and Arguments in Kubernetes 

      -  CMD  is the default parameter passed to the command 
      -  the entry point instruction is a command instruction thet you can specify the programm that will run when the containers starts
      -  ARGS option in the pod defination file we override the cms instruction in the docker file 
F: What you do if u would like to override the Entry point 
      -  under spec: --> containers: ---> name: --> Image : 
      -  command: [ "sleep2.0" ]   ----------    Entrypoint
      -  args: [ "10" ]            ----------    CMD
- You have to create apod yaml file so that u can change the Manual sleep time 
-  
______________________________________________________
        LAb 

F:  How many PODs exist on the system?
    In the current(default) namespace
A:        kubectl get pods 

F:  What is the command used to run the pod ubuntu-sleeper?
A:        kubectl describe pod ubuntu-sleeper
          cat ubuntu-sleeper (look at under container)

F:   
A:        cat ubuntu-sleeper-2.yaml
          vi  ubuntu-sleeper-2.yaml (make the changes under spec write command: [ "sleep","5000" ]
          kubectl create -f  ubuntu-sleeper-2.yaml
         kubectl describe pod ubuntu-sleeper-2

F:   Create a pod using the file named ubuntu-sleeper-3.yaml. There is something wrong with it. Try to fix it!
     Note: Only make the necessary changes. Do not modify the name.
A:         cat ubuntu-sleeper-3.yaml
           vi ubuntu-sleeper-3.yaml (make the changes "1200")
           kubectl create -f ubuntu-sleeper-3.yaml
           kubectl describe pod ubuntu-sleeper-3 (only to check wheather all the things are right )

F:  Update pod ubuntu-sleeper-3 to sleep for 2000 seconds.
    Note: Only make the necessary changes. Do not modify the name of the pod. Delete and recreate the pod if necessary.
A:         kubectl edit pod ubuntu-sleeper-3 (change the data but it gives error u copy the path)
           cat  /tmp/kubectl-edit-180931990.yaml (write the path) it make the changes
          kubectl replace --force -f /tmp/kubectl-edit-180931990.yaml (now receate the pod)

F: Inspect the file Dockerfile given at /root/webapp-color directory. What command is run at container startup?
A:         cat /root/webapp-color/Dockerfile

F:   Inspect the file Dockerfile2 given at /root/webapp-color directory. What command is run at container startup
A:          cat /root/webapp-color/Dockerfile2

F:   Inspect the two files under directory webapp-color-2. What command is run at container startup?
     Assume the image was created from the Dockerfile in this directory.
A:         ls web-color-2
             cat web-color-2/Docketfile          (First we create an image and then we use this in the yaml file)
             cat web-color-2/webapp-color-pod.yaml
             (The command will alwyas override the Entrypoint)
              Entrypoint Overwrite the ---> command
              CMD  overwrite the  args


F:  Inspect the two files under directory webapp-color-3. What command is run at container startup?
    Assume the image was created from the Dockerfile in this directory.
A:           ls web-color-3
             cat web-color-3/Docketfile          (First we create an image and then we use this in the yaml file)
             cat web-color-3/webapp-color-pod.yaml
             (The command will alwyas override the Entrypoint)

F:  Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green.
A:          kubectl run webapp-green --image=kodekloud/webapp-color --dry-run=client -o yaml
            kubectl run webapp-green  --image=kodekloud/webapp-color --  --color green
            kubectl describe pod webapp-green
_________________________________________________
ENV Variables in Kubernetes 

To set an variable Environment variable, use ENV property.
    -  env: is an Arry and every property starts - dash indicating an item in the array .
    -  each item hase a name and a value of properties 
    -   the naem is the ENV variable (avilable in a container) and value is its value 

There are three ways to setting the Environment variables 
    -  plain key Value     
          eg  env: 
                -   name: APP_COLOR
                    value: pink
    -  ConfigMap
          eg env: 
                -   name: APP_COLOR
                    valueFrom: 
                        configMapKeyRef:
    -  Secrets
          eg  env: 
                -   name: APP_COLOR
                    valueFrom: 
                        secretKeyRef:

__________________________________________
create ConfigMaps:

F: How to Configur the data in kubernetes? 
   when u have a lot of pod definations file it will be difficult to manage the environment data stored within the queries file

  In Kubernetes, a ConfigMap is an object used to store configuration data as key-value pairs.
  In Configmaps we can change the settings without rebuilding the conatiner image.
     
  There are two phases in Configmap
      - create the config map
      - inject them into pod 
  
  There are two ways to create an 
      1)  kubectl create configmap\  (Imperative)
              sitConfigname --from-literals=APP_COLOR=blue
                            --from-literals=PASSWORD=&Harekam0003
  NOTE:  It will be complicated when u have too many configuration thats why we can also use the file 
         When u want to use the file u have to specify the path 

      2)  kubectl create -f configmap.yaml  (Declarative)
            we have to create a yaml file 
                    kind: configdata 
                    metadata:
                        name: app-config
                    data: 
                        APP_COLOR: blue
                        PASSWORD: &Harekam0003

F: How to view the Config map
        kubectl get configmap

F: How to see in the detail?
        kubectl describe configmaps
F: NOw how to combine/inject pod and the configmap file?
   add new property in the pod yaml file
        envFrom:
          - configMapRef:
              name:
        
        
F: 

______________________________________
F:  What is the environment variable name set on the container in the pod?
        kubectl describe pod webapp-color
F:  What is the value set on the environment variable APP_COLOR on the container in the pod?
        kubectl describe pod webapp-color (under Environment : pink
F:  Update the environment variable on the POD to display a green background.
    Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.
A:     kubectl edit pod  webapp-color ( used to edit the pod and give the error)
       cat /tmp/kubectl-edit-1695555210.yaml (now make the changes but it did not save it)
       kubectl replace --force -f /tmp/kubectl-edit-1695555210.yaml (fÃ¼r changes we have to create the replace and delete the pod)
F:  How many config maps are there?
A:     kubectl get ms

F:  Identify the database host from the config map db-config
A:     kubectl describe cm db-config 

f:  Create a new ConfigMap for the webapp-color POD. Use the spec given below.
A:     kubectl create configmap webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard

F:   Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap.
    Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.
A:         kubectl edit pod webapp-color 

        kubectl replace --force -f /tmp/kubectl-edit-2160797237.yaml

______________________________________
            Secrets
In conig maps stores configuration data in plain text format.its ok for host name and user name but it is not good for the password 
Secrets: 
      -  are used to store the sensative information like password and keys. 
      -  It stores the data in entcoded form 
      -  First, create the secret and 
      -  second, inject it into pod 

There are two ways to create a secrets 
      1) The imperative way, Without using secret defination file.
      2) the Declarative way, by using a declarative file.

1)  The imperative way, First way Without using secret defination file.
          you can directy specify the key value pair in the commands line itself
          eg. kubectzl create secret generic  <secret name> --from-literal=<key>=<value>

Second way with using secret defination file where the path is stored 
          eg. kubectzl create secret generic  <secret name> --from-file=<path-to-file>

2) the Declarative way, by using a declarative file.
        kind: secret
        data:
            DB_HOst: mysql
            DB_User: root
            DB_PAssword: paswrd
Note in secret we have to store the in encoded form not in plain text
When we enter the data in the secret file we have to enter in the encoded form

F: How can we encode the data from plain text?
     On Linux just run the 
      echo -n 'mysql' | base64
      o/p    bXlzcWw=
      echo -n 'root' | base64
      o/p    cm9vdA==
      echo -n 'paswrd' | base64
      o/p   cGFzd3JK
 




      



    
