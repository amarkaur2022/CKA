vi Kubernetes  is a Open-Source system for Automating the deployment, scaling and management of containerized application.
fEATURES 
  - Conatiner Orchestration:
      Manages Conatines  accross a cluster of maschine.
  - Self-healing:
      Resatrts failed containers, repolace them., Kills unresponsive ones.
  - Sacling:
      Automatically increase/descrease resources based on load
  - Load Balancing :
      Distrub straffic to keep the system responsive.
  - Service Discovery: 
      Makes it easy for services to find and talk each other.
  - Configuration Management: 
      Store and  manages secret and application settings.
_____________________________________________________________________________________

CORE CONCEPT:
  1) Container: 
      Container is the basic unit of software in Kubernetes, packaging applications with their necessary code, libraries, and dependencies.
  2) POD:
      The smalles  deployment unit;
      wraps one or more containers.
  3) Node (server):
      A Single maschine (Physical or virtual) in the cluster.
  4) Cluster:
      A group of nodes managede by Kubernetes.
  5) Deployment:
      Tells Kubernetes how to create and manage Pods.
  6) Service:
      Exposes Pod to the network and handels traffic routing.
_____________________________________________________________________________________
Typical Use Case:
- Bild your app in Dockers container. Then you use Kubernetes to:

1 Deploy the containers across fleet to server.
2 Ensure they stay running.
3 Scale them up during traffic spikes.
4 Roll out new versions without downtime.
_____________________________________________________________________________________
KUBERNETS HAS TWO MAIN PARTS 
  1) MASTER(CONTROL PLANE)
  2) WORKER NODES
_____________________________________________________________________________________

COMPONENETS OF MASTER:

- API Server:
    It Provide the interface (Command line Interface) also known as qubecontrol.
- Schedular:
    Assign node to newly created Pods. ((Pod is a small Unit where the container run it. We have multiple nodes and like to run a new container.
    Which node will be assigned this work has been done by the schedular.)
- ETCD:
    Key value, store having all cluster Data.
    we have all the cluster, Different Nodes, Applications , containers and Pods that store the information ETCD
-  Control Manager
    control Manager is responsible for managing the state of the cluster (When something is damage or kaputt then Control manager repair )
_____________________________________________________________________________________
WORKER NODES

- KUBLET: 
    Agent,make sure that containers are running in Pods.
    (It checks wheather the conatianers are working Properly in the Pods yes or NO)

- POD: 
    the Containers are running in the Pod.
    (Checks that all the conatainers are working Properly  with in the POD)
    A single Instance of a running process in a cluster. It can rum one or more conatiners and share the same resources.

- Kube-Proxy:
    Maintain the Network rules for communication with POds.
    eg. we have a cluster, we have inside Network, we have Outside Network, There are many Pods running in the NOde(to run/Maintain all the Things we use Kube-Proxy. 

- Container-runtime:
    A tool is responsible for running containers er. Dockers 
    
_____________________________________________________________________________________
Difference between Docker and the ConatinerD



_____________________________________________________________________________________
ETCD:


_____________________________________________________________________________________
NOTE: if we not move a one pod from one node to another node.
      we can only delete the pod and create a new pod on anothe node where u want 


F: How to get the help for the commands 
    kubectl run --help 
F: On which image used to create the new Pods?
    kubectl describe pod newpodname 
    kubectl  get pods-o wide 
F: How many containers are in the pod 
    kubectl describe pod 
F: Why do you think the computer agenty in pod webapp is in error

F: What does the READY column in the output of the kubectl get Pods commans indicate 
    1/2  Running container in PODS/Total container in POD 
F: 
F: How to create the Pods?
    kubectl run ngnix --image=ngnix

F:  How to delete the pod?
      kubectl delete pod webapp
      kubectl delete pods --all 
F: How to make the change in the pod?
      kubectl edit pod podname
F: How to Create a new Pod with the name of the rednis and with the image rednis123?
   Note: Use the POD-Defination YaAML file. And yes the image name is wrong
    kubectl run rednis --image=rednis123 --dry-run -o yaml          <-- it display only in yaml format meand it did not create a yaml file
                                                                        it display only when u u create vayaml file how it looks like 
    kubectl run rednis --image=rednis123 --dry-run=client -o yaml
    kubectl run rednis --image=rednis123 --dry-run=client -o yaml > rednis.yaml   <<-- it did not display the yaml format but it save the output in the file 
    cat rednis.yaml

> kubectl get pods redis -o yaml | grep image    <-- it dsplay an image of the single pod

> kubectl run redis --image=redis123 --dry-run=client  -o yaml   
		<-- it just create a file in yaml format only client side and display on the screen

> kubectl run redis --image=redis123 --dry-run=client  -o yaml > redis.yaml   
		<-- save the output in a redis.yaml file but only client side

> kubectl apply -f redis.yaml
		<-- After that u have to create a yaml file o that the changes are save
		    on the clust so it will be saved on the cluster

> cat redis.yaml
		<-- display the Inhalt on the screen 

--dry-run: It is only used to check th command wheather it is right or wrong and this is the 
  clint side command. The request will not go to the server 


Note: Use the POD-Defination YaAML file. And yes the image name is wrong
a) how to create a yaml file 
    kubectl create -f rednis.yaml
b) see weather thepod s file is created or not 
    kubectl get pods


F: How to change the image on this pod to rednis. Once done, the pod should be in a running state?
    cat redis.yaml
    vi redis.yaml
    kubectl applay -f rednis..yaml
    kubectl get pods

F: How to show the labels 
    kubectl describe pod rednis
    kubectl describe pod --show-labels 

--dry-run:
	- It is only used to check th command wheather it is right or wrong and this is the 
      clint side command. The request will not go to the server 
    - the resources will be created.
--dry-run=client:
	- This will not create the resource, instead, tell you whether the resource can be created and if your command is right.
-o yaml:
	- This will output the resource definition in YAML format on screen.


 

____________________________________________________________________

A ReplicationController is a legacy Kubernetes controller that ensures a specified number of pod replicas are running at any given time.

--Function of replicaController
    Automatically replaces failed pods
    Ensures the desired number of pods are always running  
    Used in earlier versions of Kubernetes

-REPlication Controller:(what and Why we need this)
    - Replication controller helps us to run multiple instances of a single pod in the kubernetes cluster,
      thus providing high availability.
    - Replication controller can help by automatically bringing up a new pod when existing one fails
      thus the replication controller ensure that the specified numbers of pods are running all the times
      even it 1 or 100

NOTE: we can also use the replication controller when we use the one pod not only multiple pods 
There are two similar terms(same perpouse but not same)
1) Repelation Controller: is the older technology that is replaced by Replica Set 

2) Replica Set: is the new Recomended way to set up replication.

- How to create a Replication Controller defination file 
    rc-defination.yml
- As we any kubernets defination file we have 4 section
  1)  API version 
  2)  Kind
  3)  Metadata 
  4)  Specification  but written as spec NOTE we cn tested the two defination file together 

1) API version: is specific to what we are creating 
2) Kind : As we know oit is Replacation Controller.
3) Meta Data: Add Name, Labels, app type and assign values to them 
4) spec. What is inside the object we are creating.
NOTE: we can copy from the pod -defination file the data into/under the spec template  
____________________________________________________________________
                          LAB 2 Replication controller

F: How to create the Replication  Controller
    kubectl create -f rc-defination.yml 
F: How to see the Replication Controller list 
    kubectl get replicationcontroler 
F: How many Pods are created by the replication Controller
    kubectl get pods
____________________________________________________________________
REPLICA_SET
    A ReplicaSet is the newer version of ReplicationController that 
    also ensures a specified number of pod replicas are running continuously, 
    
    but with more powerful and flexible label selectors.

there is only one differencence between replication controller and Replica set 
- Replica Set requires a selector defination the selector section helps the Replica Set 
  identify what pods fall under it.

Festures of the Replica Set 
    Same purpose as RC: maintain a stable number of pods
    Supports set-based label selectors
    Commonly used as part of a Deployment

- In Replica set there are three sections 
    1) Template 
    2) replicas 
    3) Selector 

_____________________________________________________________________


____________________________________________________________________
                            LAB Replica Set 

>  kubectl get pods         
        <-- to display the pods on the screen from default defaut namespace
>  kubectl get rs
>  kubectl get replicaset
>  kubectl get ReplicaSets
        <-- to diplay the replicas set
> kubectl describe rs new-replica-set 
> kubectl get pods --show-labels | grep new-replica-set
        <- to display the pod in the particular replicaset
> kubectl describe rs
> kubectl describe rs <name-rs>
> kubectl describe pod new-replias-set | grep image
        <-- to see the image 
> kubectl get rs 
        <-- also displöay how many pods are ready under READY field
> kubectl describe rs
> kubectl describe rs <name of rs>
> kubectl describe pod <podname>
        <-- to wheather the pods are ready or not when NO
            u have see the paticular rs under that u will find the pod name
            then u have see in the pod why the pods are not ready 
> kubectl delete pod <podname>
        <-- to delete the pod
> we can delete the pod from the replica set but it will create automatically 
  because we already set the desied number of replicas (4)
> kubectl delete rs replicaset-1
        <-- to delete the replica 

F: Create a ReplicaSet using the replicaset-definition-1.yaml file located at /root/.
   There is an issue with the file, so try to fix it.
>   ls /root
        <-- it display the files on the root
    k create -f /root/replicaset-definition-1.yaml
        <-- the file is already created just try to save on the cluster but error
    cat  /root/replicaset-definition-1.yaml
        <-- display the inhalt on the screen so u can check for error 
    kubectl explain replicasset
        <-- whe u donot known the syntax and bot able to find the error it display the right syntax
    vi  -f /root/replicaset-definition-1.yaml
        <-- make the changes in the file 
    k create -f /root/replicaset-definition-1.yaml
        <-- save the changes on the cluster 
F: Fix the issue in the replicaset-definition-2.yaml file and create a ReplicaSet using it.
    This file is located at /root/.
>    run the same commands from the upper question 
      but only things is label name and the tier name must be same in the rs
      u just make the changes nad run it 

F: Fix the original replica set new-replica-set to use the correct busybox image.
   Either delete and recreate the ReplicaSet or update the existing ReplicaSet and then delete all PODs, so new ones with the correct image will be created.
   If you opt to delete the ReplicaSet and recreate it, please refer to the file named new-replica-set.yaml, which is saved in the /root/ directory for your convenience and fix it.
>   kubectl edit rs replicas-set
		<-- make the changes in the file under spec: change the name of the image.
		<-- if u update the rs the image name is changed 
			but the pods are not created automatically
NOTE u have to two ways
	1) either u delete and recreate the replicaset
	2) u have create all the pods with new image name
> k get pods 
		<-- u see all the pod are on error stage(STATUS)
> kubectl delete pod 1 2 3 4
		<-- now delete all the pods from the replicaset
> k get pods 
		<-- now it is in creating state and fetr some time 
		    the pods are on the running stage
> k get rs 
		<-- u will see that all the pods are on the ready state


> k get rs 




Important

Scale: 
-	The kubectl scale command is used to change the number of replicas (pods) in a deployment, replica set, or stateful set.
-	To increase or decrease the number of running instances (replicas) of your application.
> kubectl scale deployment my-deployment --replicas= 5
> kubectl scale replicaset my-replicaset --replicas=3
> kubectl scale statefule my-stateful --replicas=4

> kubectl edit pod redis      (edits a resources live in cluster)
   - Directly opens the resource in an editor (like vi or nano) so you can change it live.
   - It fetches the current YAML definition from the cluster.
   - Opens it in your editor (vi by default).
   - When you save and exit, it automatically applies the changes.

> kubectl apply -f redis.yaml   (create Or Update)applies file-based config to cluster)
   - Apply a configuration from a YAML file to the cluster.
   - Reads the resource definition from a file (e.g. redis.yaml).
   - If the resource doesn’t exist → creates it.
   - If it exists → updates only the changed fields.

> vi redis.yaml (it edit the file locally)
   -  vi is not a kubectl command — it’s just a Linux text editor.
   -  You can use it to manually open a file and edit YAML before applying.
   -  make changes
   -  save and exit
   -  then run

> kubectl create -f redis.yaml  (if pod doesnot exists it created error if exists)
   - It creates a new resource in the Kubernetes cluster from a YAML file or command.
   - If the resource doesn’t exist, it is created.
   - If it already exists, it will throw an error (it will not update or modify it).

> kubectl replace -f redis.yaml  (if pod exists it replaced if missing gives error)
   - If the resource exists → it’s deleted and recreated (with a new definition).
   - If the resource doesn’t exist → it throws an error.


____________________________________________________________________

                        LABELS AND SELECTORS 

         
____________________________________________________________________
                    LAB  Lables and Selsctors  
F: How to create an Labels 
       kubectl describe pod nginx | less
F: How to attach the new Labels 
      kubectl label pod nginx env=testing 
F: How to overright the label 
      kubectl label --overwrite pod ngnix env=prod
F: How to delete the Label
      kubectl label pod ngnix env-
F: How to see/display the Labels 
      kubectl get pods --show-labels 


____________________________________________________________________
                    DEPLOYMENT

Deployment is used to manage the automatic life cycle of the pods. 
It create updates scaling and rollback of the Instances when any eroor occur
NOTE: Deployment create an Replicaset automaticall. When we run
      kubectl get replicaset
      You will see the replicaset file.


Features of the Deployment
1. Manages Replica Pods
    It makes sure that a specified number of Pods are always running.
2. Self Healing
    If a pod crashes or gets deleted, the Deployment will automatically create a new one to replace it.
3. Rolling Updates
      You can update your application image or configuration, and the Deployment will roll out changes gradually with zero downtime.
4. Rollback support 
      If something goes wrong during an update, you can easily rollback to a previous stable version.

_____________________________________________________________________
              How to create a Deployment file 
- The Deployment file is same like a Replicaset 
    only in Kind: DEPLOYMENT
- Contents of the File
    API Version:
    Metadata: 
      name 
      labels
    spec:
        template:
          metadata:
          
    replica: 
    selector:
_____________________________________________________________________
              LAB 3 deployment

F: How many pods are created
      kubectl get POD 
F: HOw many replica set are created 
      kubectl get rs
F: HOw many deployments are created 
      kubectl get deployments
F: How many deployments exists on the system. We just creaed a one Deployment 
      kubectl get deployments 
F: How many rs are created
      kubectl get rs
F: How many PODS are created 
      kubectl get Pod
F: Out of all the PODs how many are READY
      kubectl get deployments 
F: What is the image used to create the POD in the new deployments 
      kubectl describe pod frontend-deployment-7fd8cdb696-stmbx  
F: Create a new Deployment using the deployment-definition-1.yaml file located at /root/.
A:    pwd (to go to the root)
      ls (see the list of the deployments files)
      kubectl create -f deployment-defination-1.yaml (to create a yaml file on the root)
      vi deployment-defination-1.yaml ( to make the changes on the yaml file)
      kubectl create -f deployment-definition-1.yaml (to create a .yaml file on the root)

 
F: Create a new Deployment with the bwlow attributes using your own deployment defination file 
    NAME : http-frontend:
    replica: 3 
    image:httpd:2.4-alpine
A:  kubectl create  deployment --help ( to get the help of the deployments commands)
    kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replica=3

NOTE: If u forget to create a replica then use 
      kubectl scale deployment httpd-frontend --replicas=3


_____________________________________________________________________


            Kubernetes services
Kubernetes services enable the communication betwen the various components and with in the outside the application 
it helps us to connect applications together
-- typically using a stable DNS name and IP address.


Types of services

  1) NodePort
  2) ClusterIP
  3) LoadBalancer 

Curl: Curl is used to talk with the AIP server (u can send or get the data from the networl
curl is a command-line tool for transferring data to or from a server using protocols like HTTP, HTTPS, etc.

 1) Why we need Services?

    - Pods are temporary — they can die, restart, or be replaced.
    - Each Pod gets a unique IP, which may change.
    - A Service provides a stable way to reach Pods, regardless of changes in the underlying Pod IPs.

2) Types of Services:

    - ClusterIP (default): Exposes the Service on an internal IP in the cluster. Accessible only within the cluster.
    - NodePort: Exposes the Service on a static port on each Node’s IP. Allows external traffic to access the Service via <NodeIP>:<NodePort>.
    - LoadBalancer: Uses a cloud provider's load balancer to expose the Service externally.
    - ExternalName: Maps a Service to a DNS name (e.g., external database).

3) How Services Work:

    -  A Service uses labels to find the set of Pods it targets.
    -  For example, if all backend Pods have a label app=backend, the Service will route traffic to all matching Pods.

4) DNS Names in Kubernetes:

    - Services are automatically assigned a DNS name.
    - For example, a Service named db-service in the marketing namespace can be accessed at:
          db-service.marketing.svc.cluster.local
5) Ports:

    - Services define a port to expose and optionally a targetPort (the port on the Pod).
    - Example: A Redis Service might expose port 6379.

NOTE: If an application wants to connect to a Redis database, it doesn’t need to know the IP of the Redis Pod.
      It just connects to redis-service:6379, and the Service routes traffic to the correct Pod(s).

________________________________________________________________
LAB 4 Services

F: How many Services exist on the system?
   In the current(default) namespace
A:   Kubectl get service
     kubectl get svc

F: What is the type of the default kubernetes service?
      kubectl get svc

F: What is the targetport configured on the kubernetes service?
      kubectl describe svc kubernetes

F: How many labels are configured on the kubernetes service?
     kubectl describe svc kubernetes

F: How many Endpoints are attached on the kubernetes service?
    kubectl describe svc kubernetes

F: How many Deployments exist on the system now?
   In the current(default) namespace
    kubectl get deploy

F: What is the image used to create the pods in the deployment?
      kubectl describe deploy simple-webapp-deployment


Create a new service to access the web application using the service-definition-1.yaml file.
Name: webapp-service
Type:       NodePort
targetPort: 8080
port:       8080
nodePort:   30080
selector:
 name:      simple-webapp

A:  ls
    cat service-defination-1.yaml
    vi  service-defination-1.yaml'(update the file) 
    kubectl create -f service-defination-1.yaml
    
Important:
expose:
The kubectl expose command creates a Service in Kubernetes that allows network access to a set of Pods.
	 
F: Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379
	kubectl expose pod redis --port=6379 --name redis-service  --dry-run=client -o yaml
		but u have to create apod first if it is not created (pod redis)

F: Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes:




_____________________________________________________________________
What is a Namespace in Kubernetes?
    A namespace is a way to divide a Kubernetes cluster into multiple virtual environments,(separate, logical parts)
    like creating separate compartments for different teams, projects, or applications.

F: Why use Namespaces?
   - To separate environments (e.g., dev, test, prod)
   - To manage multiple teams or projects in one cluster
   - To apply resource limits and access control per group

1) Isolation:(विभाजन और अलगाव)
    - Namespaces provide logical isolation between groups of resources (like Pods, Services, Deployments).
    - For example, two teams can each have a web-app running in separate namespaces (team-a and team-b) without interfering with each other.

2) Resource Scoping:
    - Most Kubernetes resources (like Pods, Services, ConfigMaps) live within a namespace.
    - Some resources (like Nodes, PersistentVolumes) are not namespaced.

3) Access Control:
    - Role-Based Access Control (RBAC) rules can be applied per namespace, giving fine-grained permissions.

4) Resource Quotas:
    - Namespaces can have quotas (CPU, memory, number of objects) to prevent one team from using up all cluster resources.

5) Default Namespace:
    - If you don’t specify a namespace, Kubernetes uses the default namespace.

Important
1) If i want to change the name space  so that every thing is automatically created in a new namespace not in the default namespace
	kubectl config set-context --current --namespace=dev

NOTE: Update my current kubectl context so that every command I run will now use the dev namespace by default.
	- kubectl	       : The Kubernetes command-line tool
	- config	       : Subcommand to work with kubeconfig (cluster connection settings)
	- set-context	   : Lets you modify or create a context
	- /current-context : The context you want to modify — in this case, the currently active one
	- --namespace=dev  : Sets the default namespace for that context to dev

2) kubectl config get-contexts
Context: A context tells kubectl which cluster, which user credentials, and which namespace to use by default.

______________________________________________________________________
              LAB 5 Namespaces 

F: How many Namespaces exist on the system?
      kubectl get namespaces 
      kubectl get ns
	  kubectl get namespace --no-header|wc

F: How many pods exist in the research namespace?
      kubectl get pods --namespace=research
      kubectl get pods -n research

F: Create a POD in the finance namepsace. Use the spec given below
      kubectl run redis --image=redis -n finance
    

F: To see wheather the pod is created in the finance
      kubectl get pod -n finance

F: Which namespace has the blue POD in it?
      kubectl get ns 
      kubectl get pods  --all-namespaces (this is used to display the namespaces AND all the pods
      kubectl get pods -A

F: What DNS Name should the Blue application use to access the database db-service in its own namespace-marketing
   You can try it in the web application UUUI. USe port 6379
      kubectl get pods -n=marketing
      kubectl get svc -n=marketing

F: What DNS Name should the Blue application use to access the database 'db-service' in the devnamespace
   You can try it in the web application UUUI. USe port 6379
NOTE FIrst of all u have to check the SVC in the namespace

      kubectl get svc -n=dev
      

__________________________________________________________

Imperative und Declarative 



__________________________________________________________
                LAB 6 (Impariative Commands)
F: Deploy a pod named nginx-pod using the nginx:alpine image.
      kubectl run nginx-pod --image=nginx:alpine

F: Deploy a redis pod using the redis:alpine image with the labels set to tier=db.
      kubectl run redis --image=redis:alpine --labels="tier=db"

F: Create a service named redis-service to expose the existing redis pod within the cluster on port 6379.
      kubectl expose pod redis --port 6379 --name redis-service
      kubectl get svc

F: Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas.
       kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3
       kubectl get deploy

F: Create a new pod called custom-nginx using the nginx image and run it on container port 8080.
       kubectl run custom-nginx --image=nginx --port=8080

F: Create a new namespace called dev-ns.
        kubectl create namespace dev-ns

F: Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2 replicas.
        kubectl create deployment redis-deploy --image=redis --replicas=2 -n  dev-ns
        kubectl get deployment -n dev-ns

F: Create a pod named httpd using the image httpd:alpine in the default namespace.
Then, create a service of type ClusterIP with the same name (httpd) that exposes the pod on port 80.
      kubectl run httpd --image=http:alpine --port 80 --expose=true 

All the imperative commands

F:  Deploy a pod named nginx-pod using the nginx:alpine image. 
	Kubectl run nginx-pod –image=nginx:alpine

F: Deploy a redis pod using the redis:alpine image with the labels tier=db. 
	kubectl run redis --image=redis:alpine -l tier=db

F: Create a service named redis-service to expose the existing redis pod within the cluster on port 6379. 
NOT: for service u have to first create a pod and then use expose in service
	kubectl run redis --image=nginx
	kubectl expose pod redis  --port=6379 --type=ClusterIP –name=redis-service

F: Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas. 
	kubectl create deployment webapp --image=kodekloud/webapp-color –replicas=3

F:Create a new pod called custom-nginx using the nginx image and run it on container port 8080. 
	kubectl run custom-nginx --image=nginx –port=8080

F: Create a new namespace called dev-ns. #
	kubectl create namespace dev-ns
	kubectl create ns dev-ns
F: Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2 replicas. 
	kubectl create deployment  redis-deploy --image=redis --replicas=2 -n dev-ns

F: Create a pod named httpd using the image httpd:alpine in the default namespace.
Then, create a service of type ClusterIP with the same name (httpd) that exposes the pod on port 80. 
	ubectl run httpd --image=httpd:alpine 
	 kubectl expose pod  httpd --type=ClusterIP –port=80

F: What is the SHORTNAME for horizontalpodautoscalers?
    This shortname can be used instead of typing the full resource name in kubectl commands.
	kubectl api-resources | grep horizontalpodautoscalers

F: Which command would you use to get a description and details of the Pod resource? 
	Kubectl explain pod 

F: When writing a YAML manifest for a Pod, you want to understand what fields are available under containers.
Which command would you use to explore the structure of the containers field?
	kubectl explain pod.spec.containers


F: You can drill down into nested fields using dot notation.  Use kubectl explain to explore   
     the spec field of a Pod. What is the TYPE of the containers field?
	

F: Let's explore the Deployment resource.
    What is the TYPE of the replicas field?
	Kubectl explore deployment.spec

F: Use kubectl explain with the --recursive flag to explore the service.spec.ports structure. 
            kubectl explain service.spec.ports --recursive
NOTE: The --recursive flag is particularly useful here because it shows you all available options for configuring a container without having to drill down field by field. 




_______________________________________________________________

				Kubectl API-resources
<-- to Display all the resources of api, if u donot know the shortname API version or where u find this in the yaml file 
	Kubectl api-resources    

<-- if u need more/detail information about the resources 
	kubectl explain pods
	O/P kind version,metadata,spec etc...

<-- if u want to see in more detail 
	kubectl explain pods.spec
	o/ it display the subfields of this command 

<-- to list all fields in the way that you would put them in a YAML file
	kubectl explain pods --recursive
		You want to see the complete structure quickly
		You're searching for a specific field but don't know its exact path
		You want to understand the full scope of available options
<-- 
__________________________________________________________
Apply: 
kubectl apply -f /path/to/config-files

Kubectl apply -f nginx.yaml
-	if the object does not already exists, the object is created.
-	and when the obeject is created on LIVE kubernetes cluster
-   and the YAML version of the local obeject configuration file we wrote is converted to a  JSON format
-   and store last applied configuration 
-	all three are compared to identify what changes are to be made on the live object.

NOTE: All the three are compared with the value in the live configuration and if there is adifference, the live configurationn is updated with the new value.
eg. Local file       (yaml file production) 
	Kubectnetes file (Live Object Configuration)
	Last Applied Configuration (Live version kubernetes converted into JSON format)
if local file have made the cahnges, changes on the kubernetes live file and then on the last Applied Configuration(JSON file)

NOW F: Why we need the last Applied Configuration file?
- if the field was delete (eg label in Local file)
- now run kubectl apply -f nginx.yaml
- we see that the last applied configuration had a label --- but it is not present in the local configuration
- this means the field was delete 
- This means that the field needs to be removed from the live configuration.
- So if a field was present in the live configuration and not present in the local or the last applied
another case if the file prent in the Last  see ((((((chapter nr  50 2:30)))))








Chapter 2 Schudeling 

LAB 7

NOTE When u want to schedule the port means(u delete the pod from one Node to another)
      Youu habe to do 4 steps 
        kubectl get pods 
        Kubectl get pods -o wide(to see in which node is the pod)
        vi nginx.yaml(open the pod and change the node name Manually and save it)
        kubectl replace --force -f nginx.yaml(delete the pod pod from one node and create a new pod to another node)
                                             (this command delete the data from the pod )
                                             ( when we want that the data wilkl not be deleted we have to use  Persistent Volume (PV) )

F: A pod definition file nginx.yaml is given. Create a pod using the file.
   Only create the POD for now. We will inspect its status next.
      kubectl get pods
      ls (to see the files)
      cat nginx.yaml (display all the data from the file)
      kubectl create -f nginx.yaml (to create a pod though the file)
      kubectl get pods

F: What is the status of the created POD?
      kubectl get pods 

F: Why the pod is the pending state?
   Inspect the environment for various kubernetes control plane components.
A:    kubectl describe pod nginx
      kubectl get pods -n kube-system

F: Manually schedule the pod on node01.
   Delete and recreate the POD if necessary.
      ls
      vi nginx.yaml (make changes nodeName: node01)
      kubectl get pods 
      kubectl  get pods --watch (to watch the pods wheather it make any changes )
      kubectl replace --force -f n
F: Now schedule the same pos on the controlplane node
   Delete and recreae the pod if necessary
      kubectl get pods 
      vi nginx.yaml(make the changes) 
      kubectl replace --force -f nginx.yaml
      kubectl get pods
      kubectl get pods -o wide 

      _________________________________________________________________________________
                    Labels and selectors  LAB  7

F: We have deployed a number of PODs. They are labelled with tier, env and bu. How many PODs exist in the dev environment (env)?
   Use selectors to filter the output
        kubectl get pods 
        kubectl get pods --selector env=dev 
        kubectl get pods --selector env=dev | wc -l (it display number of lines with header)
        kubectl get pods --selector env=dev --no-headers| wc -l(it display the nummber of lines without header)
NOTE: first  label=podName 
             env=dev 

F: How many PODs are in the finance business unit (bu)?
        kubectl get pods --selector bu=finance --no-header | wc -l

F: How many objects are in the prod environment including PODs, ReplicaSets and any other objects?
        kubectl get all  --selector env=prod --no-header | wc -l
NOTE all is use to see all the obejects are created in the environment prod


F: Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
        kubectl get all --selector env=prod,bu=finance,tier=frontend

F: A ReplicaSet definition file is given replicaset-definition-1.yaml. Attempt to create the replicaset; you will encounter an issue with the file. Try to fix it.
  Once you fix the issue, create the replicaset from the definition file.
        ls
        cat replicaset-definition-1.yaml 
        kubectl create -f replicaset-definition-1.yaml 
        vi replicaset-definition-1.yaml (change the name of the tierName in the selector)
        kubectl create -f replicaset-definition-1.yaml
        kubectl get rs

___________________________________________________________________
tant and tolaration:

tant and tolerations areused to set the restrictions.
On which Node the pod can be scheduled

Tains and toleration doesnot tell the pod to go to a particuler Node,
instead tell the node to accept only pods with certain tolerations
NOTE: 
the tanits are set on Nodes.
the toleration are set on the Pods
Now the 

Syntax:   kubectl taint nodes name key=value:taint-effect
eg.       kubectl taint nodes node1 app=blue:Noschedule

1) taint-effect: 
    defines what would be happend to the pods, if they do not tolerates the taint there are three taint effects.
    - NoSchedule:
      Which means that the pod will not be scheduled on the node.
    - PreferNOSchedule:
      Which means the system willl try to avoid placing a pod on the nodes and that is no gauranted.
    - NoExectute:
      Which menas that the new pod will not be scheduled on the node and existing pods on the node. if any will be evicted
      means they do not tolerate the taints 

Toleration :
      Tolaration are added to pods. To add the toleration to pods first pull the defination file and change the tolerations: under spec:
      How to write in the file:
      toleration: 
      key:"app"
      operator: "Equal"
      value:"blue"
      effect: "NoSchedule"

F:   how to see wheather the tent are attached there 
     kubectl describe nodes worker01 | less 
__________________________________________________________________
      Lab 8 Taint and toleration 

 F: How many nodes exist on the system?
    Including the controlplane node.
 A:    kubectl get nodes

 F: Do any taints exist on node01 node?
 A:     kubectl describe node node01

F: Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule
A:      kubectl taint node node01 spray=mortein:NoSchedule
        kubectl describe node node01  (to check wheather the taints are created are not)

F:  Create a new pod with the nginx image and pod name as mosquito.
A:      kubectl run mosquito --image=nginx

F: What is the state of the POD?
A:      kubectl get pods

F:  Why do you think the pod is in a pending state?
A:      kubectl describe pod mosquito

F: Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.
   Image name: nginx
   Key: spray
   Value: mortein
   Effect: NoSchedule
   Status: Running

A:      kubectl run bee --image=nginx --dry-run=client -o yaml
        kubectl run bee --image=nginx --drs-run=client -o yaml > bee.yaml (recreate a file)
        vi bee.yaml
              tolerations:  
               - key: spray
               value: mortein
               effect: NoSchedule
               operator: Equal
        kubectl create -f bee.yaml

F: Observe that the bee pod has been scheduled on node node01 due to the toleration that has been configured for the pod.
        kubectl get pods 
        kubectl get pods --watch

F: Do you see any taints on controlplane node?
A:       kubectl describe pod controlplane

F: Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
A:       kubectl taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule-

F: What is the state of the pod mosquito now?
         kubectl get pods 

F: Which node is the POD mosquito on now?
          kubectl get pods -o wide


__________________________________________________________
Node Selector (is used to limit/tell on the pods so that they only run on particular nodes)
		
  A node selector is a simple way to tell a Pod which nodes it can run on based on labels.
      Nodes have labels (e.g., disktype=ssd).
      Node selector in a Pod specifies these labels.
      Kubernetes will schedule the Pod only on nodes matching those labels.

We can select the pods that the pod run only on the specific nodes: there are two ways to doe that
1) Using the node selector (Simple and eiser method)
    - open the pod defination yaml file 
    -  to limt this pod we add a new propertie as a     nodeSelector  under spec: section
    -  you have to write nodeSelector: Large (where the large comes from)
    -  you have to declare the large as a label and selector 
    -  to specify the labels on the Nodes 

Syntax:      Kubectl label nodes <node-name> <label-key> = <label-value>
eg:          lubectl label nodes node-1 size=Large
			 kubectl create -f pod-defination.yaml


    - this is very simple example What is when we want to put the pod on the not small and not on the large Node
      In this case we have to use NODE AFFINITY
------------------------------------------------------
It si used when our requirements are more comples eg
	place the pod on Large or medium node
	place the pod NOT small node  etc.....
for this node affinity and anti-affinity were introduced:

NODE AFFINITY:
node affinity feature is to ensure that pods are hosted on particular nodes.
Node Affinity is used to control which nodes a Pod can be scheduled on--- based on the labels assigned on the nodes.
This is the best to tell the Kubernetes Scheduler.
eg i want this pod run on the nodes that  matches the things(location , speicher, hardware.......)

🔸 How is it Different from nodeSelector?

Feature	            nodeSelector	            nodeAffinity
Type	              Simple key-value match	  Advanced expressions supported
Flexibility	        Limited	                  More flexible (AND, OR, ranges)
Priority Support	  No	                      Supports soft rules (preferred)


There are two type of Types of Node Affinity
1) requiredDuringSchedulingIgnoredDuringExecution(Hard rule)
        Pod will not be scheduled if node doesn't match
        Once running, the rule is ignored
2)  preferredDuringSchedulingIgnoredDuringExecution   (Soft rule)

IMPORTANT: there are two states in the lifecycle of a pod when considering node affinity
			DuringScheduling: is aste where a pod does not exist and is created for the first time.
			DuringExecution
explain: 
-------   DuringScheduling: is the state where a pod does not exist and created for the first time.

F: What happend what if the nodes with matching labels are not available or forgot it ?
A: now i have to use the node Affinity. If i used 
    1) requiredDuringSchedulingIgnoredDuringExecution  (Hard rule)
        the Scheduler will mandate that the pod be placed on a node with the given affinity rules 
        if it cannot find one, the pod will not be scheduled.
NOTE: this type is ued when the placement of pod is crucial. if the maching nodes does not exists the pod will not be scheduled.

---- if the placement of Pod is less important than running the workload itself in the t case we used
    2)  preferredDuringSchedulingIgnoredDuringExecution   (Soft rule)
          where the matching node is not found. the Scheduler will not simply ignore node affinity rules and place the pod of any node.
NOTE: this is the way to tell the scheduler 

---------- DuringExecution:  is a state where the pod has been running and a chane is mede in the Environment that affects on the node affinity 
          Such as change in the label of the node.
eg. Some one remove the label, but we are not able to change the labels on the Node.

---- if you want to change any Pod that are running on Nodes 

        Kubernetes tries to schedule to a matching node, but can skip if none match

3)  requiredDuringSchedulingRequiredDuringExecution (not stable)
        Rule is enforced both at scheduling and runtime
___________________________________________________

____________________________________________________
LAB 9 
_____________________________________________________
Daemon Set 
  -  Daemon sets are like the replica sets
  -  It helps you to deploy multiple instances of pods.
  -  But it runs one copy of pod on each node in your cluster.
  -  Whenever the new  node is added to the cluster a replica of the pod is automatically added to that node.
  -  And when a node is removed then the pod is automatocally removed.
  -  It ensure that the one copy of the  pods is always present in all nodes in the cluster 

NOTE: runs one copy of pod---- What is in the pod --- What it do 
🔍 What is inside that Pod?  
      The Pod contains whatever you define in the DaemonSet YAML configuration.
      Typically, DaemonSets are used to run background or system-level tasks on every node.

🔧 Common examples of what's inside:
      -  Log collectors (e.g., Fluentd, Filebeat)
      -  Monitoring agents (e.g., Prometheus Node Exporter)
      -  Security agents or antivirus
      -  Network plugins or storage daemons
Good example is
  (Kube-proxy) for every workerNode 
  (Weave-net) for Networking Solution agent on each Node in a Cluster 
____________________________________________________________
      LAB 10
F:  How many DaemonSets are created in the cluster in all namespaces?
    Check all namespaces
A:   kubectl get Daemonsets -A

F: Which namespace is the kube-proxy Daemonset created in?
A:   kubectl get Daemonsets -A

F: Which of the below is a DaemonSet?
A:   kubectl get all --all-namespaces
     kubectl describe Daemonsets kube-proxy -n kube-system

F: On how many nodes are the pods scheduled by the DaemonSet kube-proxy?
A:   kubectl describe daemonset kube-proxy --namespace=kube-system

F: What is the image used by the POD deployed by the kube-flannel-ds DaemonSet?
A:   kubectl describe daemonset kube-flannel-ds --namespace=kube-flannel 

F: Deploy a DaemonSet for FluentD Logging.
   Use the given specifications.
   Name: elasticsearch
   Namespace: kube-system
   Image: registry.k8s.io/fluentd-elasticsearch:1.20
A: - kubectl create deployment elasticsearch -n kube-system --image=k8s.gcr.io/fluentd-elasticsearch:1.20 --dry-run=client -o yaml 
   - kubectl create deployment elasticsearch -n kube-system --image=k8s.gcr.io/fluentd-elasticsearch:1.20 -o yaml > fluent.yaml
        MAKE CHANDES IN THE FILE #
        kind: Daemonset 
        delete the replica/delete line 13(strategy {})/ lat line status:{}
    -  kubectl create -f fluentd.yaml
    -  kubectl get ds -n kube-system



    
______________________________________________
  Static pod (Chapter 76)

Static pods are directly managed by the kubectl on a specific node, rather than the kubernetes API server/scheduler
All the master Nodes are the Components of the static pod 
- Kube/API-server
- Kube Scheduler
- Kube-controller-manager
- ETCD
- Cloud Controll Manager(Optional)

F: What happend when all the above Components  in the cluster  are not there. How kubelet(Worker Node) create the Pods USW.....
A:  -  The Kubectl can manage the Node Independently
    -  we Installec the Kubelet and docker(Containers) Installed 
    -  There is no kubernetes cluster and all the Components of the Master Nodes
    -  To create a Pod we need the detail of the Pod in a Pod defination file.
    -  Now we have to configure the Kubelet to read Directory on the server 
    -  Place the defination file in htis directorsy
    -  It checks and read the file and create the Pods on the host.
    - If the application crasch the kubectl restart it.
    -  If changes are made the kubelet recreate the pod 
    - If remove the file, the pod will be deleted 
    - THESE PODS ARE KNOWN AS STATIC PODS
NOTE : we cannot create the pods with the help of relicas, deployments usw----

IMPortant 
    1) View and Config theis option
    2) use Methods to set the cluster 
        -  Check the pod-manifest-path in the kubelet services file 
        -  When not find look at the config file 
        -  Within the Configfile find StaticPodPath
        -  We Cannot use the Kubectl command because it is used with the Api-server 
        -  


_________

LAB 11
F: How many static pods exist in this cluster in all namespaces?
A: kubectl get pods -A
      To know that these are the static PODs
      The Pod name is extened/append with the node name.
F: How to see the static pod in detail 
    kubectl get pod coredns-77456fff-brwnd -n kube-system -o yaml 

F: Which of the below components is NOT deployed as a static pod?
A:    kubectl get pods -A

F: Which of the below components is NOT deployed as a static POD?
A:   kubectl get pods -A
 
F: On which nodes are the static pods created currently?
A: kubectl get pods -A

F: How to see where is the config file is srtored (I think it is falsch )
AAAA: kubectl get configmap -n kube-system kubeadm-config -o yaml

__________________________________

Priority


_______
LAB 12
______
Multiple schedular 

We use more than one Schedular in a Cluster.
Every Schedular uses a seprate Configuration file and each file having its own Schedular name(my-schedular-2.service)
F: How its work if u deploy the schedular as a pod?
A:  -  We create a pod defination file and specifiy the kubeconfig property
    -  Under Sspec: Conatiner:  Command: write the path of the config file.
    -  that has the authentication information to connect to the kubernetes API server.
    -  create a each Scheduler as  yaml file and write the (LleaderElection) 
    -  leader erection is used when u have multiple copies of the scheduler running on the different master node.
    -  If multiple copies of the same Scheduler are running on different nodes, only one can be activ at a time 
       and thats why the leaderElection help who will lead the scheduling activities.
Note:  If the scheduler was not configureed correctl, then the pod will continue to remain in a 
       pending state(look Pods logs under Kubectl describe command)
       an if good is it will in the running state.


F: How to look that which schedular is picked up 
    kubectl get events -o wide(Source)
F: If we have the running issues how to see the logs ?
    kubectl logs my-custom-schedular --name-space=kube-system
_______________________________________________________________________________________
          LAB 13
F: What is the name of the POD that deploys the default kubernetes scheduler in this environment?
A:    Kubectl get pos -A

F: What is the image used to deploy the kubernetes scheduler?
   Inspect the kubernetes scheduler pod and identify the image
A:    kubectl describe pod kube-schedular-controlplane -n kube-system


F: We have already created the ServiceAccount and ClusterRoleBinding that our custom scheduler will make use of.
   Checkout the following Kubernetes objects:
      ServiceAccount: my-scheduler (kube-system namespace)
      ClusterRoleBinding: my-scheduler-as-kube-scheduler
      ClusterRoleBinding: my-scheduler-as-volume-scheduler
Run the command: kubectl get serviceaccount -n kube-system and kubectl get clusterrolebinding
A:   kubectl get sa my-scheduler -n kube-system    ( to check the service account)

F: Please create a ConfigMap that the new scheduler will utilize, implementing the concept of ConfigMap as a volume.
   A ConfigMap definition file named my-scheduler-configmap.yaml has
   been provided at the /root/ path. This file will be used to create a
   ConfigMap with the name my-scheduler-config, utilizing the content
   from the file located at /root/my-scheduler-config.yaml.
A:     kubectl create configmap my-scheduler-configmap --from-file=/root/my-scheduler-config.yaml -n kube-system
       kubectl get configmap my-scheduler-configmap -n kube-system


F: Deploy an additional scheduler to the cluster following the given specification.
    Utilize the manifest file located at /root/my-scheduler.yaml. Ensure that you are using the same image as that of the default Kubernetes scheduler.  
    To verify the image used by the default Kubernetes scheduler, execute the following command:
    kubectl describe pod kube-scheduler-controlplane --namespace=kube-system
Note : Deploying the new scheduler may take a few seconds to reach a running state.
A.     kubectl get pods -A
       kubectl describe pods my-scheduler-controlplane -n kube-system | grep Image  (to see only imagename)(copy the image name)
       vi my-scheduler.yaml (change the image name)
       kubectl create -f my-scheduler.yaml( to create the pod as yaml file) 
        kubectl get pods  -n kube-system( to see the yaml file wheather the file is created or not) 

F:   Please modify the provided Pod manifest file located at /root/nginx-pod.yaml 
     to specify that the Pod should be scheduled by your custom scheduler, which is named my-scheduler.
     After updating, create the Pod in the default namespace and verify it is scheduled by your custom scheduler.
Note : The pod may take a few seconds to reach a running state.

A: ls
   vi nginx-pod.yaml( add the schedular name under spec:)
   kubectl create -f nginx-pod.yaml
  (u have to create the command because the yaml file is always saved in the local and pods are on the cluster thats why u have to upadate the cluste through this command)
    kubectl get pods (check wheather the pods is running or not) 
_________________________________________
      Admission Controllers      (Securing kubernetes)

-  We are running the commands from the command lines.
-  Kubectl utility to perform various kind of Operations on our kubernetes cluster.
1) we send a request to create a pod ---> the request goes to API server ---> then the pod is created ---> ths information is saved in the Etcd database
2)  when request hit the API server ---> goes through an anthentication process ---> create a pod 
    (authentication process is responsible identify the user who send the request and making sure the user is valid 
3) kubectl ---> Authentication ---> Authorization ---> create a pod 
    request goes to Authorization process check if the user has permission performed that operation this is done throught th e role-based access controls.
F: What is role-based control:
    we can place different kind of restrictions such as allow or deny with particular role like create list or delete depüloyments or services.
F: Why we use the Admission Controllers
    - Security
    - Policy Enforcement
    - Automatic Modification
    - Consistency
4) kubectl ---> Authentication ---> Authorization ---> Admission Controllers ---> create pod 

-  Admission controllers has additional operations before the pod is created 
      -- Always pullimages (when the pod is created it will pull the images)

      -- DefaultStorageClass ( observe the creation of the PVC automatically add default storage class to them)

      -- EventRateLimit

      --NAmespaceExists

    -- NamespaceAutoProvision (create automatically the namespace if it doesnot exists)

eg..  We craete a pod with name nginx and image name is also nginx and in the namespace blue 
      the namespace will not be find in the it will be rejected 
       kubectl ---> Authentication ---> Authorization ---> Admission Controllers checks wheather the namspace is find when yes it created a pod otherwise it will be rejected


F: How to see the enable Admission Controller bydefault 
    kube-apiserver-h | grep enable-admission-plugins

_________________
LAB  14

F:  Which admission controller is not enabled by default?
A:    kubectl get pods -n kube-system
      kubectl exec -it kube-apiserver-controlplane -n kube-system --kube-apiserver -h | grep 'enable-admission-plugins'

F: Which admission controller is enabled in this cluster which is normally disabled?
      vi /etc/kubernetes/manifests/kube-apiserver.yaml
      grep enable-admission-plugins /etc/kubernetes/manifests/kube-apiserver.yaml

F: Create an nginx pod in the blue namespace. Please note that the blue namespace does not currently exist. Do not create the blue namespace at this time.
   Execute the following command to deploy a pod using the nginx image within the blue namespace:

F: 
      vi /etc/kubernetes/manifests/kube-apiserver.yaml 
      kubectl get ns

F:  Delete the existing PVC named myclaim:
      kubectl delete pvc myclaim

F: Reapply the same manifest:
      kubectl apply -f myclaim.yaml
F: Check the status of the PVC:
    kubectl get pvc myclaim

F:   Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins.
A:   ps -ef | grep kube-apiserver | grep admission-plugins
______________________________________________
Validating Controller (how we can controll our Admission Controller)
example Namespace:
It can  help valdate if a namespace already exists, and reject the request when it doesnot exists

There are two type of bildin Admission Controller 
1) Mutating Admission Controller 
      are those that can change the request 
      generally, mutating admission Controller are invoked first followed by valadating admission Controllers.if it was run th eother way then it will always be rejected.

2) Valaditing Admission Controller
      are those that can validate the request and allow or deny it and 
      there may be Admission controller that can both that can mutate a request as well as Validate the request

We can also create our own Admission Controller. 
there are two special external admission Controller availabe

1) MutatingAdmissionWebhook
2) ValidatingAdmissionWebhook

F: What is a webhook
    it is a service on the server to create our own code own logic for Admission Controll
    we can configure the webhook to point andit hosted ob in the kubernetes cluster or outside our server
    When i send the request it always hit the webhook ---> make admission webhook server ---> by passing Admission Review object ---> json format
    it the request is allowed it will be TRUE and if it rejected it will be FAULT
F: How do we set this up 
A:   depoly the webhook server with own logic and configure the webhook on the kubernetes by creating the webhook configuration 

-
____________________________


____________________________

Application Logs
Various loggin mechansims in Kubernetes
we create a pod with the same Docker image using the pod defination file 

F: How to loggin Docker?
    docker run kodekloud/event-simulator

F: If i run the docker container in the background in a detechmode using -d option, I would see the logs,
    docker -d kodekloud/event - simulator 

F: If i wolud like see the log in the 
      docker logs -f ecf   (-f display the live log trail)

NOTE: we create a pod with the same Docker image using the pod defination file once the pod is running, we can view the logs 
    -  kubectl create -f event-simulation.yaml (create a podd)

    -  kubectl logs -f event-sim ( see the log files)
       

If the multiple conatiners in the pod you must specify the name of the conatiner to see the log file 
    -    kubectl logs -f event-simulator-pod  event-simulator 
          - event-simulator-pod: this is the name of the pod 
          - event-simulator: this is the name of the first container in the pod 

________________________________________
LAB 

F:  To see the logs?
A:      kubectl logs webapp-1

F:   We have deployed a new POD - webapp-2 - hosting an application. Inspect it. Wait for it to start.
A:      kubectl logs webapp-2 event-simulator ( it there are two container are running on the pod then write the name of the container to see the logs)

_______________________________________________
Rolling Updates and Rollbacks in a Deployment

F:  What is rollout and versioning in Deployment?
A:    -  When u first create a deployment, It triggers a rollout
      -  A new Rollout create a new deployment revision, (revision 1)
      -  In future the container is updated to a new one, a new rollout is triggered and 
      -  new deployment revision is ceated  (_Revision 2) 

______________________________________________
LAB Rolling Update 

F:  We have deployed a simple web application. Inspect the PODs and the Services
    Wait for the application to fully deploy and view the application using the link called Webapp Portal above your terminal.
A:        kubectl get pods 
          kubectl get deploy 

F:  Run the script named curl-test.sh to send multiple requests to test the web application. Take a note of the output.
    Execute the script at /root/curl-test.sh.
A:         ./root/curl-test.sh

F: Inspect the deployment and identify the nummber of the Posds deployed by it 
A:         kubectl get deploy 

F:   What container image is used to deploy the applications?
A:         kubectl describe deploy  frontend 

F:   Inspect the deployment and identify the current strategy
A:         kubectl describe deploy frontend 

F:   If you were to update the application now what would happen?
A:         pods are upgrading few at a time
            (by default is that first pod willl be down and than upgrade it . then secon will down and then  upgrade it..

F:   Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2
     Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

A:   1 way
         kubectl describe deploy frontend (first see the container name)
         kubectl aet image deploy frontend simple-webapp=kodekloud/webapp-color:v2
      2 Way
          kubectl edit deploy frontend (and change the image Name)

F:    Run the script curl-test.sh again. Notice the requests now hit both the old and newer versions (If checked immediately). However none of them fail.
      Execute the script at /root/curl-test.sh.
A:         ./curl-test.sh
            bash /root/curl-test.sh ( that means the application is updated)

F:    Up to how many PODs can be down for upgrade at a time
      Consider the current strategy settings and number of PODs - 4

          
F:     Change the deployment strategy to Recreate
      Delete and re-create the deployment if necessary. Only update the strategy type for the existing deployment.
A:         kubectl edit deploy frontend 
          and change the thing what they need (delkete the after strategie 3 lines bis type)

F:     Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v3
       Do not delete and re-create the deployment. Only set the new image name for the existing deployment.
A:             kubectl set image deploy frontend simple-webapp=kodekloud/webapp-color:v3

F:     Run the script curl-test.sh again. Notice the failures. Wait for the new application to be ready. Notice that the requests now do not hit both the versions
      Execute the script at /root/curl-test.sh.
A:               ./curl-test.sh
_____________________________________________________________
Commands and Arguments in Dockers
BYDEFAULT it sleep s for 5 seconds

To run a docker containers 
      - docker run ubuntu( to run the instance and image immediadely)
      - docker PS (list to see the running containers)
      -  docker ps -a (list all the conatiners including stopped containers)

F: What we run the conatiners 
      - Containers have a specific task/ process 
      -  to run a host an instance of a web server 
      -   application server or a database 
      -  or simpoly to carry out some computation or analysis.
      - when it completed its wtask it exists 
Note   the Container only lives as long as the process inside it is alive,
      if the website inside the conatiner is stopped or crashes, the container exits 

F: We can also do it automattically that when th container run the sleep command will run/invoke  automaticallly 
     -  We have to save in the startup.
     -  TRNTRYPOINT is the command thet is run at startup 
     -  CMD  is the default parameter passed to the command 
     -  the entry point instruction is a command instruction thet you can specify the programm that will run when the containers starts
     -  ARGS option in the pod defination file we override the cms instruction in the docker file 
    
____________________________________________________________
  Commands and Arguments in Kubernetes 

      -  CMD  is the default parameter passed to the command 
      -  the entry point instruction is a command instruction thet you can specify the programm that will run when the containers starts
      -  ARGS option in the pod defination file we override the cms instruction in the docker file 
F: What you do if u would like to override the Entry point 
      -  under spec: --> containers: ---> name: --> Image : 
      -  command: [ "sleep2.0" ]   ----------    Entrypoint
      -  args: [ "10" ]            ----------    CMD
- You have to create apod yaml file so that u can change the Manual sleep time 
-  
______________________________________________________
        LAb 

F:  How many PODs exist on the system?
    In the current(default) namespace
A:        kubectl get pods 

F:  What is the command used to run the pod ubuntu-sleeper?
A:        kubectl describe pod ubuntu-sleeper
          cat ubuntu-sleeper (look at under container)

F:   
A:        cat ubuntu-sleeper-2.yaml
          vi  ubuntu-sleeper-2.yaml (make the changes under spec write command: [ "sleep","5000" ]
          kubectl create -f  ubuntu-sleeper-2.yaml
         kubectl describe pod ubuntu-sleeper-2

F:   Create a pod using the file named ubuntu-sleeper-3.yaml. There is something wrong with it. Try to fix it!
     Note: Only make the necessary changes. Do not modify the name.
A:         cat ubuntu-sleeper-3.yaml
           vi ubuntu-sleeper-3.yaml (make the changes "1200")
           kubectl create -f ubuntu-sleeper-3.yaml
           kubectl describe pod ubuntu-sleeper-3 (only to check wheather all the things are right )

F:  Update pod ubuntu-sleeper-3 to sleep for 2000 seconds.
    Note: Only make the necessary changes. Do not modify the name of the pod. Delete and recreate the pod if necessary.
A:         kubectl edit pod ubuntu-sleeper-3 (change the data but it gives error u copy the path)
           cat  /tmp/kubectl-edit-180931990.yaml (write the path) it make the changes
          kubectl replace --force -f /tmp/kubectl-edit-180931990.yaml (now receate the pod)

F: Inspect the file Dockerfile given at /root/webapp-color directory. What command is run at container startup?
A:         cat /root/webapp-color/Dockerfile

F:   Inspect the file Dockerfile2 given at /root/webapp-color directory. What command is run at container startup
A:          cat /root/webapp-color/Dockerfile2

F:   Inspect the two files under directory webapp-color-2. What command is run at container startup?
     Assume the image was created from the Dockerfile in this directory.
A:         ls web-color-2
             cat web-color-2/Docketfile          (First we create an image and then we use this in the yaml file)
             cat web-color-2/webapp-color-pod.yaml
             (The command will alwyas override the Entrypoint)
              Entrypoint Overwrite the ---> command
              CMD  overwrite the  args


F:  Inspect the two files under directory webapp-color-3. What command is run at container startup?
    Assume the image was created from the Dockerfile in this directory.
A:           ls web-color-3
             cat web-color-3/Docketfile          (First we create an image and then we use this in the yaml file)
             cat web-color-3/webapp-color-pod.yaml
             (The command will alwyas override the Entrypoint)

F:  Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green.
A:          kubectl run webapp-green --image=kodekloud/webapp-color --dry-run=client -o yaml
            kubectl run webapp-green  --image=kodekloud/webapp-color --  --color green
            kubectl describe pod webapp-green
_________________________________________________
ENV Variables in Kubernetes 

To set an variable Environment variable, use ENV property.
    -  env: is an Arry and every property starts - dash indicating an item in the array .
    -  each item hase a name and a value of properties 
    -   the naem is the ENV variable (avilable in a container) and value is its value 

There are three ways to setting the Environment variables 
    -  plain key Value     
          eg  env: 
                -   name: APP_COLOR
                    value: pink
    -  ConfigMap
          eg env: 
                -   name: APP_COLOR
                    valueFrom: 
                        configMapKeyRef:
    -  Secrets
          eg  env: 
                -   name: APP_COLOR
                    valueFrom: 
                        secretKeyRef:

__________________________________________
create ConfigMaps:

F: How to Configur the data in kubernetes? 
   when u have a lot of pod definations file it will be difficult to manage the environment data stored within the queries file

  In Kubernetes, a ConfigMap is an object used to store configuration data as key-value pairs.
  In Configmaps we can change the settings without rebuilding the conatiner image.
     
  There are two phases in Configmap
      - create the config map
      - inject them into pod 
  
  There are two ways to create an 
      1)  kubectl create configmap\  (Imperative)
              sitConfigname --from-literals=APP_COLOR=blue
                            --from-literals=PASSWORD=&Harekam0003
  NOTE:  It will be complicated when u have too many configuration thats why we can also use the file 
         When u want to use the file u have to specify the path 

      2)  kubectl create -f configmap.yaml  (Declarative)
            we have to create a yaml file 
                    kind: configdata 
                    metadata:
                        name: app-config
                    data: 
                        APP_COLOR: blue
                        PASSWORD: &Harekam0003

F: How to view the Config map
        kubectl get configmap

F: How to see in the detail?
        kubectl describe configmaps
F: NOw how to combine/inject pod and the configmap file?
   add new property in the pod yaml file
        envFrom:
          - configMapRef:
              name:
        
        
F: 

______________________________________
F:  What is the environment variable name set on the container in the pod?
        kubectl describe pod webapp-color
F:  What is the value set on the environment variable APP_COLOR on the container in the pod?
        kubectl describe pod webapp-color (under Environment : pink
F:  Update the environment variable on the POD to display a green background.
    Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.
A:     kubectl edit pod  webapp-color ( used to edit the pod and give the error)
       cat /tmp/kubectl-edit-1695555210.yaml (now make the changes but it did not save it)
       kubectl replace --force -f /tmp/kubectl-edit-1695555210.yaml (für changes we have to create the replace and delete the pod)
F:  How many config maps are there?
A:     kubectl get ms

F:  Identify the database host from the config map db-config
A:     kubectl describe cm db-config 

f:  Create a new ConfigMap for the webapp-color POD. Use the spec given below.
A:     kubectl create configmap webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard

F:   Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap.
    Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.
A:         kubectl edit pod webapp-color 

        kubectl replace --force -f /tmp/kubectl-edit-2160797237.yaml

______________________________________
            Secrets
In conig maps stores configuration data in plain text format.its ok for host name and user name but it is not good for the password 
Secrets: 
      -  are used to store the sensative information like password and keys. 
      -  It stores the data in entcoded form 
      -  First, create the secret and 
      -  second, inject it into pod 

There are two ways to create a secrets 
      1) The imperative way, Without using secret defination file.
      2) the Declarative way, by using a declarative file.

1)  The imperative way, First way Without using secret defination file.
          you can directy specify the key value pair in the commands line itself
          eg. kubectzl create secret generic  <secret name> --from-literal=<key>=<value>

Second way with using secret defination file where the path is stored 
          eg. kubectzl create secret generic  <secret name> --from-file=<path-to-file>

2) the Declarative way, by using a declarative file.
        kind: secret
        data:
            DB_HOst: mysql
            DB_User: root
            DB_PAssword: paswrd
Note in secret we have to store the in encoded form not in plain text
When we enter the data in the secret file we have to enter in the encoded form

F: How can we encode the data from plain text?
     On Linux just run the 
      echo -n 'mysql' | base64
      o/p    bXlzcWw=
      echo -n 'root' | base64
      o/p    cm9vdA==
      echo -n 'paswrd' | base64
      o/p   cGFzd3JK
 
F: how to view secrets?
      kubectl get secrets 
F: How to see/view the more information?
      kubectl describe secrets
      It shows the attributs in the secret but hides the value themselves
F: How to views the values now u can see the encoded values ?
      kubectl get secret app-secret -o yaml

NOTE now u have to create a two files one is for po-defination.yaml and second is secret-data.yaml 

NOW step two 
F: HOw to inject N ENVIRONMENT VARIABLE and anew property to the container 
1) we can inject a single environment 
2) the whole secretes as a file in a volume
spec:
  containers  
    envFrom:
      -  secretRef:
            name: there is the name under metadata from secret-data.yaml

F: If u want to mount the secret as a volume in pod Secrets in pods as Volumes
A: we have to create a file with each attribute as a secret of the contents 
        ls /opt/app-secret-volumes
        cat /opt/app-secret-volumes/DB_Password
if there are three attributes in our secret that means three files are created.

Important on Secrets:
    - Secrets are not Encrypted. Only encoded.
          -    Do not check-in secret objects to SCM along with code.
    - Secrets are not encrypted in ETCD
    - 
    
_________________
      LAB
F:  How many Secrets exist on the system?
    In the current(default) namespace.
A:       kubectl get secrets

F: How many secrets (data keys) are defined in the dashboard-token secret?
A:     kubectl describe secret default-token-cr4sr

F: What is the type of the dashboard-token secret?
A:    kubectl describe secret default-token-cr4sr

F: Which of the following is not a secret data defined in dashboard-token secret?
A:      kubectl describe secret default-token-cr4sr

F:  The reason the application is failed is because we have not created the secrets yet. Create a new secret named db-secret with the data given below.
    You may follow any one of the methods discussed in lecture to create the secret.
A:        kubectl create secret generic db-secret  --from-literal=DB_Host=sql01 --from-literal=DB_User=root 
          --from-literal=DB_Password=password123



_______________________________________________


Multi container Pods 
    means to run more than one conatianers in a pod simuntencely 

MULTICONTAINER AND ITS  DESIGN PATTERN 
There are different patteren for Multi-container pods 
    1)  Co-located Containers 
    2)  Init Containers
    3)  Sidecar Container 

  1) Co-located Containers: 
        -  these are used, when two services are depnded on each other.
        -  This is the original form of mullticontainers means that two Containers are running in a pod.
        -  the conatiners are running till the pod lifecycle. theses are used  when two services are depended to each other 
        -  
   2)  Init Containers:
        -   are used when the initilization steps to be performed when a pod starts before the mian application itself.
        -   Init Conatiners starts and ends its job and then the main application starts  

   3)  Sidecar Container: 
        -  A small helper container that runs alongside the main application inside the same pod
        -  It is set up like the init Container 
        -  The side car starts first, 
        -  does its job,
        -  instead of ending, It continues to run throughtout the lifecycle of the pod and ends after the main app ends.
        -  The main application starts after the sidecar starts.
        -  Both containers start, stop, and share the same network and storage, working closely together.
NOTE: Sidecar = helper container running alongside the main app, like a log shipper.

The First and the last containes are same what is the difference between them.
      -  
_____________________________________________________
                                  INIT LAB

F: Identify the pod that has an initContainer configured
A:       kubectl  describe pods

F: What is the image used by the initContainer on the blue pod?
A:    kubectl  describe pod 
      kubectl describe pod green
      kubectl describe pods blue  | grep Image

F:  What is the state of the initContainer on pod blue?
      kubectl describe pod  blue

F:   Why is the initContainer terminated? What is the reason?
       kubectl describe pod blue 

F:   We just created a new pod named purple. How many initContainers does it have?
        kubectl describe  pod purple 

F:   What is the status of the purple POD?
        kubectl describe pod purple 

F:  Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds
    Delete and re-create the pod if necessary. But make sure no other configurations change.
A:      kubectl esit pod red 
        kubectl replace --force -f  /tmp/kubectl-edit-847706811.yaml

F:  Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds
    Delete and re-create the pod if necessary. But make sure no other configurations change.
A:      kubectl get pod orange 
        kubectl describe pod orange 
        kubectl logs orange -c  init-myservice
Explain it: kubectl → The Kubernetes command-line tool.
        logs → The command to view output (stdout/stderr) from a container.
        orange → The name of the pod you want to inspect.
        -c init-myservice → Specifies which container inside the pod you want logs from. This is necessary because pods can have multiple containers (including init containers).
        In this case, init-myservice sounds like an init container, which runs before the main app containers in the pod, often to set up configs, load data, or check dependencies.
        If you don’t include -c ..., kubectl will default to the first container in the pod—but for init containers, you must specify them explicitly.

  ______________________________________________________________
Auto Scaling in CKA
 
Scaling means: (thji si sthe example of the of version when the have our own data center or servers)
    When we used the physical server with a predefined  CPU memory capacity and what happend we load increases.
    First we need to scale up the server 
    so we took down the application and added more  CPU or more memeory resources to it and then powered it back up.

Vertical scaling :
    When we added more resources(more cpu and memory resources  etc... ) on our exisiting application is known as vertical scalling
Horizontal scaling:
    running more instances or more server to your server is known as horizontal scaling.


________________________________________________
There are two types of Scaling in Kuberenets:
      1) Scaling Workloads: you can scale the work loads by adding and removing containers or pods onto the cluster
      2) scaling cluster Infa:that adding nad removing more servers ot Infrastructure to our cluster

    1) Scaling Workloads: you can scale the work loads by adding and removing containers or pods onto the cluster
          There are two types of scaling 
            - Horizontal scaling: means adding more pods
            - Verticaling: increasing the resources allocated to existing pods          

    2) scaling cluster Infa:that adding nad removing more servers ot Infrastructure to our cluster.
            There rae two types of scaling 
            -  Horizontal scaling: the adding more nodes to the cluster.
            -  vertical scaling: increasing the resources on exiciting nodes in the cluster

 There are two ways of scaling (How we do the scaling)
      1) Manual way for Scaling Cluster Ifra;
            Manual approach of horizontally scalling cluster infra would be to manually provision new nodes.(Vartically scaliing is not be prefered)
            eg.  kubeadm join command    (to add the new nodes in the cluster.
      1) Manual way of Scaling Workloads
             Also two ways of Scaling(HOw we do the scaling)
            - Horizontal scaling eg. 
                kubectl scale command is used in the workload to scale up and down the nummber of Pods.
            - Vertical scaling: eg. 
                Kubectl edit command to go into that deployment or set the replica set and change the limits aand resources on the pods 
      
2) Automated way:
    2) Cluster Autoscaler:

    2) Horizontal Pod Autoscaler (HPA)
        
    2) Vertical Pod Autoscaler(VPA)



 ______________________________________________________________
Scaling a workload the Manual way

NOTE: In manual way we have to sit on my computer and continuously monitor resources usage and if there is a sudden traffic spike and want to break or add something
      i m not able to react fast thats why we used (HPA)  HORIZONTAL POD AUTOSSCALER
        kubectl top pod my-app-pod command:
                is used to see the resources consumption of the pod, if u have to do it maually.NOTE( you must have the metrics server running on the cluster)
        kubectl scale deployment my-app --replicas=3 : 
                is used to add  the additional pods, so this is the manual way to scale a worklooad(nun i have to sit on my computer and continuously monitor resources usage.

Automatacally run the HPA
1)   Horizontal pod Autoscaler(HPA):
          -  Observe metrics: HPA continuousaly monitors the matrics as we did manually with the top command.
          -  Add Pods: It can Automatacally ioncreases and decreases the pod in a deployment statefull set or replica set based on the CPU memory.
          -  Balances tresholds: If the memory or memory usage goes oo high, HPA creates more pods to handel that and if no needs it removes the extra pods 
          -  Tracks multiple metrics: this balance the thresholds and it can also track the multiple different types of metrics which we refer to in a few minutes.
      
Imperative way
          eg Kubectl autoscale deployment amy-app \ --cpu-percent-50 --min=1 --max=10
                It creates a horizontal Pod Autoscaler  for this deployment first reads the limits on the pod  500 millicore, second it can use the monitor mmeory bis zu 50%
                thirs, It can modify the numbers of the replica min1 and max 10
          eg. Kubectl get hpa
                to view/list the status of the current hpa.
          eg kubectl delete hpa my-app
                to delete the hpa

Declarative way:( we can create an hpa With the API version:)
        apiVersion :autoscaling/V2
        kind: HorizontalPosAutoscaler

Metrics server :
      A Metrics Server in Kubernetes is a lightweight service that collects resource usage data (like CPU and memory) from nodes and pods.
      It provides this data through the Kubernetes API, so tools like the Horizontal Pod Autoscaler (HPA) or the kubectl top command can use it.

👉 In short:
Metrics Server = resource usage collector for scaling and monitoring in Kubernetes.
____________________________________________________________
        LAB Manual Scaling 

F:  Create a Deployment
    Using the /root/deployment.yml manifest file provided , create a Kubernetes deployment for the Flask application.
    Discovery
        Use kubectl get deployments to observe the deployment status.
        Use kubectl get pods to see the running pods.
A: We use the apply command, if the the deployment is already written in yaml file it just appy on the cluster,
    if it is not written in tha yaml nfile it will create a deployment and applay on the cluster 
        kubectl apply -f /root/deployment.yml
        kubectl get pods 
        kubectl get deployments 

F:  What is the primary purpose of the kubectl scale command?
A:   to scale the number of replica and deployments 

F:  Can the kubectl scale command be used to scale down a statefulset in Kubernetes?
A:      The kubectl scale command can be used to scale both deployments and statefulsets.
        When scaling a statefulset, Kubernetes ensures that the state and order of the pods are maintained, 
        unlike in deployments where pods can be created and destroyed in any order.

F:   How to see that how many replicas are created?
A:       kubectl get deployments 

F:   If you scale a deployment using kubectl scale to a higher number of replicas, 
     but the cluster has insufficient resources to accommodate all new replicas, what will happen?
A:      When you scale a deployment to a higher number of replicas than the cluster can support due to resource constraints,
        Kubernetes will create as many replicas as possible within the available resources.
        The remaining replicas will be in a pending state until sufficient resources are freed up or added to the cluster. 
        This behavior allows Kubernetes to manage resources dynamically while maintaining the desired state as closely as possible.



F:   Create a Deployment
     Using the /root/deployment.yml manifest file provided , create a Kubernetes deployment for the nginx application.
     Click on Skooner to access the monitoring tool and view the resources in the Kubernetes cluster.
     Token for the Skooner can be found in /root/skooner-sa-token.txt
     Is the nginx deployment running?
A:       To deploy nginx application, run the below command.
          kubectl apply -f /root/deployment.yml

        To view the deployment and pods creation, use the below commands.
          kubectl get deploy or kubectl get pods


f: We have a manifest file to create autoscaling for the Nginx deployment located at /root/autoscale.yml. Review the manifest file and identify the current replicas and desired replicas?

A. Current replicas= 7
    Desired replicas= 3

B. Current replicas= 3
    Desired replicas= 7

C. Current replicas= 7
    Desired replicas= 1

D. Current replicas= 0
    Desired replicas= 0

A: kubectl get hpa  (D)

F: Create an autoscaler for the nginx-deployment with a maximum of 3 replicas and a CPU utilization target of 80%.
A:   kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80
            OR
      kubectl apply -f /root/autoscale.yml

F:  What is the primary purpose of the Horizontal Pod Autoscaler (HPA) in Kubernetes?


F:  What component in a Kubernetes cluster is responsible for providing metrics to the HPA?
A:     metrics server

F:  What is the current replica count of nginx-deployment after deploying the autoscaler?
A:       kubectl get deploy nginx-deployment

F:   What is the status of HPA target?
A:       kubectl get hpa

F:   The HPA status shows /80 for the CPU target. what could be a possible reason?
A:       If the status of the Horizontal Pod Autoscaler (HPA) target is <UNKNOWN>/80,
         it typically means that the HPA is unable to retrieve the current metrics for the specified target. 
         Run kubectl describe hpa nginx-deployment 
          to find more details.

F:  Since the HPA was failing due to the resource field missing in the nginx-deployment,
    the resource field has been updated in /root/deployment.yml. Update the nginx-deployment using this manifest.
    Watch the changes made to the nginx-deployment by the HPA after upgrading by using the kubectl get hpa --watch command.
A:      To update the nginx-deployment, run the below command:
              kubectl apply -f /root/deployment.yml
        To watch the changes made to the nginx-deployment by the HPA, use the below command:
              kubectl get hpa --watch

F:   What does the event ScalingReplicaSet in the nginx-deployment HPA indicate?
A:       To find out what the event ScalingReplicaSet means in the HPA, run the command below:
            kubectl events hpa nginx-deployment | grep -i "ScalingReplicaSet"
              HPA increasing tghe numnber of pods

F:   What is the cause of the FailedGetResourceMetric event in the nginx-deployment HPA?
A:          To find out what the event FailedGetResourceMetric means in the HPA, run the command below:
            kubectl events hpa nginx-deployment | grep -i "FailedGetResourceMetric"


___________________________________________________________________
      Resize the pods 

__________________________________________________________________
VPA (Vertical pod Autoscaler)
for vertically scalling always use EDIT command 
      -  it Monitors the metrics server 
      -  it automatically increases and decreases the resources assigned to the pod in a deployment 
      -  Balance the workload

We also see the three multiple Components deployed:
      1)   Admission controller:
            -  the Admission Controller intervenes the pod Creation process and uses the recommendation from the Recommender again.
            -  To then mutate the pod spec to apply the recommended CPU and memory values at startup.
            -  And this ensure that the newly created pod starts with the correct resources requests.

  
      2)   VPA Recommender: (Collect information ) 
            -  is responsible for continuously monitoring resources usage and collect history 
               and live usage data for pods and provides recomendation on optimal CPU and Memory values.
           -  It does not modify the Pod directly.
           -  It only suggest changes.


      3)  Updater Serivice:(Monitors  or get the information from the Recomender compares to the actual pods and if the pod beyond the threshhold it kill the pod (it depends on the polocy)  )
          - Detect pods that are running with sub optimal resources and
          -  Evicts them when an update is needed.
          -  It gets the information from the Recommender and monitors the pod.
          -  If the pod needs to be updated, It evicts(means terminates the pod) them

There are 4 modes 
The update modes are the most important part of the VerticalPodAutoscaler CRD because they determine how and when VPA applies its CPU/Memory recommendations.
    
        1. Off  (Only recommends. Does not change anything)
              VPA only generates recommendations but does not update pod resources.
              Use case:
                  -  You want to observe VPA’s suggestions before trusting it to make changes.
                  -  Good for testing or capacity planning.
    
        
        2. Initial :(Only changes on pod creation. Not later)
              -  VPA applies recommendations only at Pod creation time.
              -  Running pods are not updated.
              -  If a Pod is restarted for another reason (e.g., rollout, crash), it will get the updated recommended resources.
              Use case:
                -  You want to avoid disruptions to live workloads.
                -  Workloads that can tolerate static resource requests but should benefit from VPA tuning over time.


        3. Recreate  :(Evicts pods if usage goes beyond range)
              -  Like Auto, but more aggressive: VPA will always delete and recreate pods when updating resources (instead of relying on live-updates or waiting).
              -  Ensures Pods are restarted with the new resources immediately.
              Use case:
                  -  Workloads that cannot tolerate in-place updates but must always run with the latest recommendations.
                  -  Batch jobs or stateless apps where restarts are cheap.

        4. Auto
              -  VPA actively updates running Pods with new recommendations.
              -  If recommendations change, VPA may evict pods to restart them with new CPU/memory requests.
              -  Evictions respect Pod disruption budgets (PDBs).
              Use case:
                  -  Production workloads where continuous optimization is needed.
                  -  You’re okay with occasional evictions.
        Features  of VPA HPA (HOPw to choose should i increase a VPA or HPA)

⚖️ Choosing the Right Mode
        
        Off      → Safe start, observe recommendations only.
        Initial  → Low-disruption, “set once” policy, great for stable apps.
        Auto     → Dynamic optimization, good for microservices or fluctuating workloads.
        Recreate → Strong consistency, but higher disruption; good for short-lived or stateless apps.

___________________________________________
      LAB How to install VPA
What is VPA CRDs — these are part of Kubernetes Vertical Pod Autoscaler (VPA).
SEE Lab 135/136

___________________________________________

F:   Let us explore the environment first. How many nodes do you see in the cluster?
     Including the controlplane and worker nodes.
A:      kubectl get nodes 

F:   How many applications/deployments do you see hosted on the cluster?
     Check the number of deployments in the default namespace.
A         kubectl get deployments 

F:    Which nodes are the applications hosted on?
A:        kubectl get pods -o wide
Explain it in Detail
is a Kubernetes CLI (kubectl) command that lists all the running Pods in the current namespace, but with extra details compared to the default kubectl get pods.
Breakdown:
kubectl → Command-line tool for interacting with the Kubernetes cluster.
get pods → Lists all the pods in the current namespace.
-o wide → Expands the output to show additional information beyond the default columns.

F:   We need to take node01 out for maintenance.
     Empty the node of all applications and mark it unschedulable.
A:       kubectl drain node01
            - This command gives error because some pods DaemonSet (networking proxy, monotring) 
              find it all the pods aumatically thats why u are not able to delete the pods
              you have ignor these DaemonSet pods
          kubectl drain node01 --ignore-daemonsets

F:   What nodes are the apps on now?
A:       kubectl get pods -o wide

F:   The maintenance tasks have been completed. Configure the node node01 to be schedulable again.
A:      kubect uncordon nodes01

F:   How many pods are on node01?
A:       kubectl get pods 

F:   Why are there no pods on node01?
A:       Running the uncordon command on a node will not automatically schedule pods on the node.
         When new pods are created, they will be placed on node01.

F:   Why are the pods placed on the controlplane node?
     Check the controlplane node details.
A:        k describe node controlplane 
                  OR
          kubectl describe node controlplane | grep -i  taint

F:  
A:        kubectl drain node01 --ignore-daemonsets

F:  Why did the drain command fail on node01? It worked the first time! 
A:        k get pods 

F: What is the name of the POD hosted on node01 that is not part of a replicaset?
A:       k get pods -o wide 

F: 
A:       kubectl drain node01 --ignore-daemonset --force

F:    hr-app is a critical app and we do not want it to be removed and we do not want to schedule any more pods on node01.
      Mark node01 as unschedulable so that no new pods are scheduled on this node.
      Make sure that hr-app is not affected.
explain I donot wnat that any pod will be added on the node01
A:        Kubectl get nodes
          kubectl cordon node01
          kubectl get pods -0 wide
_______________________________________________
1)  kubectl cordon <node>
    -  Marks the node as unschedulable.
    -  No new Pods will be scheduled on it.
    -  Existing Pods keep running.
👉 Think: “Stop putting new Pods here, but don’t touch the old ones.”

2)  kubectl drain <node>
🔹 Drain
    -  Also marks the Node as unschedulable (same as cordon).
    -  Additionally, it evicts all existing Pods from that Node (except DaemonSet and static Pods).
    -  Those Pods are then recreated on other Nodes (if part of a ReplicaSet/Deployment).
The Node becomes completely empty.

3)   kubectl uncordon <node>
        -  Marks the node as schedulable again.
        -  New Pods can now be placed on it.
👉 Think: “Bring this node back to the pool.”

In short:
    cordon = stop new Pods
    drain = stop new Pods + move old Pods away
    uncordon = allow new Pods again
____________________________________________
Kubernetes manages software Releases

What we know about the API versions in kubernetes 
When we install the a kubernetes cluster with specific version of kubernetes 
We can see that when we run the command
    kubectl get nodes 

        Version is v1.33.0 in three parts
V1 :   first is the major version followed by the minor version and 
33 :   Minor version  (realease every few months with new features and functionalities)
0  :   patch version   (realease more often  with critical bugs fixes)

The download package, when extracted, has all the control plane componenets are on the same version 
eg : 
    kube-APIserver      :  v1-xx.y
    controller-manager  :  v1-xx.y
    kube-scheduler      :  v1-xx.y
    kubelet             :  v1-xx.y
    kube-proxy          :  v1-xx.y
    kubectl             :  v1-xx.y

There are two componenets that do not have the same version 
    etcd Cluster        :  V3.2.18
    coreDNS             :   V1.1.3

________________________________________________

Cluste upgrade process

F:  Is is complusary for all of these to have the same version 
A:  - NO the componenets can be at different release version.
    - the kubeapi is the primary component in the control plane and that is the component that lal other components talk to 
      none of the other components should ever be at a version higher than the kube-apiserver. X version 
    - The conroller-manager and scheduler can be at one version lower  X-1
    - kubelet  and  kube-proxy  can be 2 version lower    X-2 
    - kubectl  has a one version higher than the API server
Note: so it allow us to do the live upgade and  We can upgrade componenets by componenet if required


F: When should u upgrade?
A:   At any time the kubernetes supports only upto the recent three minor versions
eg     first release  : v1.10
       senond release : v1.11
       third relase   : v1.12
       fourth realse  : v1.13
now the first realease is not be suported

NOTE: BEFORE THE RELEASE OF V1.13 WOULD BE A GOOD TIME TO UPGRADE YOUR CLUSTER TO THE NEXT RELEASE

F: How we upgrade should we upgrade v1.10 to direkt v1.13
A: No. the best way to upgrade is one be one v1.10 to v1.11 to v1.12 denn v1.13... and sooo on 

You can deploy ur cluster in two ways with the help of kubeadm  and cluster from strach

    1) if u deploy ur cluster using tool like kubeadm then the tool can help you plan and upgrade the cluster
    2) u can deploy ur cluster from scratch then u manually ugrade the different components of the cluster yourself

1) we have cluster with master and worker Node running in production, hosting nodes, services users.
    the nodes and components are at version 1.10

________________________________
F: Upgrading a cluster involves two major steps.
    - First upgade your master node 
    -. Second upgrade ur worker Nodes
UPGRADE MASTER NODE    
1)  While the  master node is upgrading the control plane components, such as the API server, scheduler and controll-managers go down briefely.
        -  when the master cluster going down does not mean your worker nodes and application on the cluster are impacted.
        -  All workloads hosted on the worker Node continue to serve users as noemal.
        -  since the master is down, all management functions are down.
        -  you cannot access the cluster using kubctl or others kubctl-API
        -  you cannot deploy new applications or delete or modify existing ones.
        - If any pod will be fail a new pod won#t be automatically created.
        -   But as long as the nodes and pods  are up, your application should be up and users will not be impacted.
        -   Once the upgrade is complete and the cluster is back up, it fuctions normally and the master Node version is v1.11 and
        -   worker nodes at version 1.10


UPGRADE THE WORKER NODES
    There are Three types of strategie 
        1) downtime strategy  (Upgrade  all of them at once)
             - then your pods are down, and users are no longer able to access the applications
             - Once the upgrade is complete the nodes are back up, new pods are scheduled and user can resume access.
        2) upgrade one node at a time:
             -   we upgrade the first node, where the workersloads move to the second and thirs node and the user are serves from there 
             -   Once the first nodes is upgraded and backup.
             -   we update the second node, where the workloads moves to the first and the third nodes and so.......

        3) 3 Strategy would be able to new nodes with newer software version to the cluster.
             -  This is convienent when we are in Cloud environment where u easily add the new nodes and decommission/deleted old one.
             -  Move the workload to new node and remove the old node


  Kubeadm---- upgarde
    kubeadm has an upgrade command that helps the cluster to upgrade.
F: Whih command gives u a lot of information about the cluster version and list all the components and its version 
   and you must manually upgrade the kubelet version on each node.
Â:      kubeadm upgrade plan 

F: HOw to do this upgrade 
    - upgrade the kubeadm tool itself to version 1.12
          apt-get upgrade -y kubeadm=1.12.0-00
    -  then upgrade the cluster componenets using the command
          kubeadm upgrade apply v1.12.0
    -  it display the version of the API server not the version of API itself it display the old version
          kubectl get nodes
    - upgrade the kubeletes on the master node
NOTE : depending on ur setup u may or u may not have kubeletes running on your master node
          apt-get upgrade -y kubelet=1.12.0-00
    - once the package is updated then restart the kubelet
          systemctl restart kubelet
    - it display that the master nodes has been upgraded 10 1.12
          kubectl get nodes
_____________-_____
    - Now upgrade the worker Nodes. 
    - First moves the workloads from the first worker node to other nodes
    - this commandsafely terminated all the pods from a node and reschedules them on the other nodes.
    - it also cordons the nodes and marks it unschedulable(no new pods are scheduled/add on it)
          kubectl drain node-1
    -  upgrade the kubeadm and kubelet packages on the worker node as we did on the master node
          apt-get upgrade -y kubeadm=1.12.0-00
    - upgrade the kubdeadm tool
          apt-get upgrade -y kubelet = 1.12.0-00
    -  upgrade the node configuration for the new kubelet version
          kubeadm upgrade node config --kubelet-version v1.12.0
    - restart the kubelet service 
          systemctl restart kubelet
NOTE: now the node is up with the new software version 
NOTE: When we drain the nodes means we mark it as a unscheduleable  node. 
NOTE: so we need to unmark it by running the command
NOTE: it is not necessary that the pods come back to this nodes.It is only marked as scheduled
NOTE: only when the pods are delete from the other node or when new pods are scheduled do they really come back to this first node
            kubectl uncoden node-1


__________________________________________
Documentation 
kubernetes Documentation / tasks / administer a cluster / Administration with kubecdm /upgrading kubeadm cluster

______________________________________________________

              Security is  very Important topic 

-  KubeApi is the center of all operations within Kubernetes.
-  We intract with it through the kube control utility or by accessing the API directly
-  and through that you can perform almost any operation on the cluster

NOTE : So this is the first line of defense 
-   Controlling access to the API server itself

We need to make two types of decisions

1) Who can access the cluster ?
A:  Who can access the API server is defined by the authentification mechanisums.
There are different ways that you can authenticate to the API server starting with 
    -  User IDs  and Password 
    -  stored in static file - Username and Tokens
    -  certificates
    -  External Authentification providers -LDAP
    -  Service Accounts 

Once they gain the access to the cluster What can they do?
2) What can they do?
A:  is defined by authorization mechanisums. It is implemented using roll based acces 
control
    where the single/ group of users have an specific permissions
  There are different ways to give the 
    -  RBAC Authorization 
    -  ABAC Autherization 
    -  Node Authorization 
    -   Webhook Mode etc......
Nun All the communications with 
    API server 
    kube Controll Manager 
    Kube Scheduler
    Kube Proxy
    Kubelet
    ETCD Cluster is secured using TLS encryption 
    We learn later how to setup the certificates between the varionus components?
  
F:  What about the communication between applications within the cluster?
A:  By default, all the Pods can access all other pods  within the cluster.
    We can restrect then using network policioes How it will we do it we will see it
    later.
______________________________________________

Authentication 

-  Kubernetes cluster consists of multiple nodes, physical or virtual and various 
   componenets that works together.
-  We have the users like the administrators that access the cluster.
-  the developer thata access to test or deploy applications.
-  We have end users who access the applicationsdeployed on the cluster
-  We have third party who access the cluster for integration purposes
Now we learnHow we secure our cluster which have the communication between internal
components through authentication and authorization mechanisms.

There are different user that may be accessing the cluster.
1) Admin
2) Developers
3) Applicatoin End Users Security of end Users, who access the application
   deployed on the cluster is managed  by th applications themselves, internally.
4) Bots


There are two types of uesers
1) hummens/Users (Administrators and Developers)
2) Robots ( such as processes or services or applications that require access to the cluster
Kuberbets does not manage the accunts nativaly. It relies on the externak resources 
like a file withusers details or certificates or identity(LDAP) to manges these users


1) Users Administrators and Developers
  -  All the user access managed by the API server wheather u r accessing the cluster through  
  -  Kubectl (Admins see the Figure on Page 2)  tool or the API (Developers) directly.
  -  All of these request go through the kube API server.
  -  The Kube API server authenticates the request (1)
  -  before Processing it(2)

F:  How doesthe KUBE API server authenticates?
A:  There are different authentication mechanisums that can be configure(see figure on page 3)
    1)  Static Password File 
    2)  Static tocken File 
    3)  Certificates 
    4)  Identity

Authentication  Mechanisums- Basics
1)  Static Password File:Simplest form of Authentication
    -  You can create a list of users and their passwords in a .csv file and use that as
        the source for user Information.
    -  The file has three colums, password, usernmae and Uer ID
    -  Pass the file name as an option to the Kube API server 

Kube-api Server Configuration
    -  --basic-auth-file=user-details.csv
    -  if we set the cluster using the Kubeadm tool then u must modify the Kube APi server
       pod Defination file.
    -  The Kubeadm tool will automatically resteart the kube API server once you update this file 

Authenticate User 
     To authenticate using the basic credentials while accessing the API server, specify 
     the user and password in a curl command like this:
    eg. curl -v -k https://master-node-ip:6443/api/v1/pods -u "user1:password123"

2)  Static tocken File :
    this is also have a .csvfile. in theis file we have an token authentication file to the kube API server.
    eg: --tocken-auth-file=user-tocken-details.csv
F: HOw to request the tocken 
    curl -v -k https://master-node-ip:6443/api/v1/pods --header"Authorization: bearer KpjCDNJ/LKCKIDNFKDkh1c4Uc4B"


_________________________________________________
              TLS Introductions (for more information see 159)
What is public and 
Certificats: is used to guarantee trust betwen two parties duaring a transaction 
eg: when a user tries to access a web server TLS certificates ensure that the communication 
between the user and the server is encrypted and server is who it says it is.

Symmetric Encryption: When you have a single key to encrypt  the data is known as Symetric Encryption

Asymmetric Encryption: When we use the private and public key/public lock to encrypt  the data is known as Symetric Encryption
key :  is always private
Lock: is always public
if u lock ur data u will be able  to open only with the associated key
u must not share the key with other but the lock can be shared with others(public)
NOTE in this user name and passwords are too risky thats why u have decided to use the public and private lkey pair
we can do this by running the SSH command 

eg ssh-keygeb
It creates two file private key and public lock 
eg car +/.ssh/authorized_keys
  to see the keys
eg ssh -i id_rsa user1@server1

F: if have more than one server in ur cluster u cand creatre many public locks but 
    every is lock is opend with a single privte key
F  what when the others user need to access the server
    they can also do the same thing they also create private key and public lock

nun we have aproblem with this we have to send the username and password (private key and public key )
through the network this is very dangerous 

NOte: we have an problem with Symmetiric encryption was that the key used to encrypt data had to send to the server over the network along with the encrypted data,
and so there is a rick of the hacker getting the key to decrypt the data.

F: What how when we could somehow get the key to the server safely?
A: Once the key is safely made available to the server, the server and client can safely continue communication
with each other using symmetric encryption.

NOTE  symmetric encryption was that the key used to encrypt data had to be sent to the server
over the network along with the encrypted data, and so there is a risk of the hacker getting the key  
to decrypt the data.

F: What if we could somehow get the key to the sever safely?
A: once the key safely made available to the aserver the server and the client can safely continue communication with each other using symmetric encryption 
to sequerly trasfer the symmetric key from the client to the server, we use asymmetric encryption 
generate a public and private key pair on the server.
SSH key gen command was used earlier, to create a pair of keys for SSH Purposes.
USE Opsen SSL Command:
openssl genrsa -out my-bank.key 1024
openssl rsa -in my-bank.key -pubot > mybank.pem

When the user first accesses the web server using STTPS,
he gets the public key from the server
If the hacker get all the things. He will also get the public key
nun
The user in fact the users brower then encrypt the symettric key using the public key provided by the server.
The symmetric key is now secure.
send it to the server and the hacker also gets the copy
server uses the private key to decrypt the message
and retrieve the symmetry key from it.
but the hacker does not have the private key to decrypot and retrieve the symmetric key fom the message.
The hacker have only public key and not able to decrypt the messages 
With asymmetric encryption, we have successfully transfered the symmetric keys from user to the server
and we secure the all the future communication  with symmetric encryption
NOTE :there is a only way to hack is he creates it own website same as the ur 


F: What is certificate:
  -  it looks like and original certificate but in digital format
  -  information about: 
  -  to whome the certifictae is issued 
  -  the public key and location of the server
  -  the cerificate and the website matches its information 
  -  anybody can create a such type of certificate the Hacker can also do so
F: How to known that the certificate is legal or illigel 
  -  Who signed and issue the certificate 
  -  When the certificate gehort zu die dann , u will be sign it.
  -  If the certificate gehort to hacker he will sign it.
F: How the browser that website is liggal or illigial
  -  All the browers are builtin certificate validation mechanism
  -  Where in the browser text the certificate received from the server 
  -  and validates it and make sure it is legimated.
  - If it is a fake website it warns you NOT SECURE 

__________________________________
There are three types of certificates
1)  Server Certificates
2)  Client certificates
3)  Root certificates

1) Server Certificates: Configure on the servers.
2) Root Certificates: configure on the CA servers.
3) Client Certificates: configure on the clients. 


F: How we will create a legimated certificate that the browser will trust?

CA (Certificates Authority)
-  Certificates Authority signed the certificates they gives ur website authority.
-  eg Symantec, DigiCert, Comodo,Global Sign etc.....
-  we geenerate a certificate signing a request or CSR using the key you generate earilier
-  and the domain name of ur website 
-  singn it using SSL command
eg. openssl req -new -key my-bank.key -out my-bank-csr -subj "/C=US/ST=CA/O=MYOrg,Inc./CN=mydomain.
-  This command generates the MY-Bank CSR file 
-  Which is the certificate signing request
-  that should be sebt to the CA for Signing
-  They verifies your details and sign the certificate and send it back
-  Now the browser will trust ur website.
-   It will done by the Browser automatically.
F: If the hacker will tried to get his certificate signed in the same way
he will failsto get the certificatte
F: How the browser knows that the certificates is illigibale?
A: the certificate was signed by the Symantec

F: How the browser know that the Symantec is a valid CA and signed by the valid Symantec not by the someone Fake Symantec?
A: the CAs themselves also have a public and private key Pairs
-  the  CAs use their own private keys to sign the certificates.
-  the public keys of all the CAs are buit in to the browers.
-  the browers checks automatically wheather the website is sign by the orinal CAs or fake CAs
NOTE It did no check the internal email applications for that we can host our own CAs

F: Why we have to encrypt the messages?
A: to encrypt the messages, we use asymetric encryption with the parir of public and private keys
-  admin users a pir of Keys to secure SSH connectivity to the servers.
- server uses pair of keys to secure a STPS traffic
-  for this the server sends signing request sto the CA
- the CA uses its private keys to sign the CSR.
- All users have a copy of the CAs public keys.
-  the signed certificate send back to the server 
-  the sever configures the webapplication with the signed certificates
- Whenevre a user want to acces the web application 
- Ther server first send the certificate with its public key.
-  the user or the user browser reads the certificates and uses the CAs public keys
- to validate and retrieve the servers public keys
- it then generate a symmetric keys for all the communication in the future 
- the symetric key is encrypted using the servers public keys
- and send back to the server 
- the server uses the private key to decrypt the msg
- and retrive the symmetric key
- the adminstrator generates a key pair for securing SSH and the web server generates a key pair for securing the website with STTPS
- The certificate sauthorities generates its own set of keypait to signed it
- The end user only generates a single symmetric key.
- now he can uses his username and password to authenticates to the webserver 

F: Nun HOw the sever knwn that who are u are u hacker or the right person
A: the server can send a request a certificates from the client 
-  the client must generates a pair of keys
-  and a signed from the valid  CAs
- The clinet then sends the certificates to the server 
- now think u have never generate the client certifictes to access ur website.
- Because TLS client certificates are not generally implemet on web servers
-  Its all implemet by the host 
- A normal user do not have to mange the certificates manually 

NOTE: THIS WHOLE I NFRASTRUCTURE; INCLUDING THE CA THE SERVERS THE PEOPLES AND THE PROCESS
OF GENERATING; DISTRIBUTING AND MAINTAINING DIGITAL CERTIFICATES IS KNOWN AS
PUBLICKEY INFRASTRUCTURE    OR    PKI

there is no public lock and private Keys
-  you can encrypt data with any one of them only decrypt data with other 
- you cannot encrypt data with one and decrypt with the same.

F: We must be careful what you encrypt your data with.
A. if u encrypt your data with your private key then
- anyone will be able to decrypt and read your messages with public key
- public key certificates extension is .crt / .pem
- Private key with extension  .key  /  -key.pem
- private kay extension and the name of the certificates haveing the word key in it either
- when they did no have the word key then it is usally a public key or certificates 

____________________________________________________________________________
SOME THING NEW WORD or definations to learn

F: What is kubeadm File?
A: A kubeadm definition file (also called a kubeadm configuration file) is a YAML file used with kubeadm to control how a Kubernetes cluster is initialized or joined.

F:  What is the kube-apiserver?
-  The kube-apiserver is the front door of Kubernetes.
-  Every request (from kubectl, controllers, or nodes) goes through the API server.
-  It talks to etcd (the database) and exposes the Kubernetes API.

F: kube-apiserver Definition File
-  The definition file of the kube-apiserver is simply the Pod manifest that defines how the API server runs.
-  When you install Kubernetes with kubeadm, the control-plane components (API server, scheduler, controller-manager) are run as static Pods.
-  Their Pod definition YAML files are placed in:
        /etc/kubernetes/manifests/
Example:
        /etc/kubernetes/manifests/kube-apiserver.yaml
        /etc/kubernetes/manifests/kube-controller-manager.yaml
        /etc/kubernetes/manifests/kube-scheduler.yaml
        /etc/kubernetes/manifests/etcd.yaml

✅ In short:
-  The kube-apiserver definition file is the Pod manifest YAML stored in /etc/kubernetes/manifests/kube-apiserver.yaml.
-  It tells the kubelet how to run the API server container, including ports, certs, and etcd connection.

_______________________________________ 

🚀 What is Cluster Bootstrap?
   -  Cluster bootstrap means the very first setup of a Kubernetes cluster so that it can start working.
Think of it like:
   -  You buy a new phone → you need to set it up first (WiFi, accounts, PIN).
   -  Same with Kubernetes → before running apps, the cluster itself must be bootstrapped.
🔹 Steps in Cluster Bootstrap (simplified)
   -  Initialize the control plane (the "brain"):
      -  API Server
      -  Scheduler
      -  Controller Manager
      -  etcd (database)
Set up kubelet on each node (the "worker agent").
      -  Generate certificates & kubeconfigs so all parts can talk securely.
      -  Join worker nodes to the control plane.
      -  Install networking (CNI) so pods can talk to each other.
🔹 Tools for Bootstrap
      -  kubeadm → the standard tool for bootstrapping a cluster.
      -  Others: managed services (EKS, GKE, AKS) hide this process, but under the hood bootstrap still happens.
✅ In one line:
      -  Cluster bootstrap = the initial process of bringing up a new Kubernetes cluster, setting up its control plane, workers, and networking so it’s ready to run applications.


_____________________________________________________________________________________
TLS certificates 

There are three types of certificates
1)  Server Certificates
2)  Client certificates
3)  Root certificates

1) Server Certificates: Configure on the servers.
2) Root Certificates: configure on the CA servers.
3) Client Certificates: configure on the clients. 

NOTE: 
Certificates with public keys are named .crt und .pem extension 
eg
Server certificates with server.crt and server.pem
Clinet certificates are client.crt und server.pem

Private keys are names with .key and -key.pem
private keys have the word word key in them.
When it doesnot have the word key then it is usually public key / certificate 
eg :
server.key OR server.-key.PEM

F: How these conseptes sre related o the kubernetes
The kunbernets cluster consistes a set of master and worker nodes and 
communication between thgeses nodes need to be secure and must ne encrypted
All interaction between all services and their clients  are need to be secure.

eg A kubernets adminstrator need to conect with kubernetescluster  through the kubectl utility
or accesssing the kubernetes API  directly must establic TLS(transport layer security)
all the componenets in the kubernetes cluster also need to be secure 

There are two primary requirement to checkt who they are 
see the notes 160. Bild

_______________________

HOw to generate the TLS Certificates(transport layer security)
There are three type of tools 
1) EASYRSA
2) OPENSSL
3) CFSSL  and many more.....
Now we use OPENSSL tool to generate the certificates.


F: What is OpenSSL?
A: OpenSSL is an open-source command line tool that is commonly used to generate 
-  private keys, create CSRs, install your SSL/TLS certificate, 
-  and identify certificate information.

please see the notes

____________________________________
LAB to views Certificates 

F: Identify the certificate file used for the Kube-api server
A:   cat /etc/kubernetes/manifests/kube-apiserver.yaml

F: Identify the certificate file ussed to authenticate kube-apiserver as a client to ETCD Server.
A:   cat /etc/kubernetes/manifests/kube-apiserver.yaml

F: Identify the key used to authenticate kubeapi-server to the kubelet server 
A:   cat /etc/kubernetes/manifests/kube-apiserver.yaml

F: Identify the ETCD Server Certificate uses to host ETCD server 
A:   cat /etc/kubernetes/manifests/etcd.yaml

F: Identify the ETCD Server CA Root Certificate used to server ETCD Server 
   ETCD can have its own CA. So this may be a different CA certificat that the one used by kube-api server .
A:   cat /etc/kubernetes/manifests/etcd.yaml

F: What is the Common Name(CN) configured on the Kube Api Server Certificate?
A:   Openssl x509 -in /etc/kubernetes/manifests/kube-apiserver.crt -text -noout

F:  What is the name of the CA who issued the kube API Server Certificate?
A:   Openssl x509 -in /etc/kubernetes/manifests/kube-apiserver.crt -text -noout

F:  Which of the below alternate names is not configured on the Kube API Server Certificate?
A:     Openssl x509 -in /etc/kubernetes/manifests/kube-apiserver.crt -text -noout

F:   What is the Common Name(CN) configured on the ETCD Server certificate?
A:     now u have to go into the server file 
       Openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text -noout

F:  How long, from the issued date, is the Kube-API Server Certificate valid for?
A:     Openssl x509 -in /etc/kubernetes/pki/etcd/server.crt -text -noout

F:  How long, from the issued date, is the Root CA Certificate valid for?
A:     Openssl x509 -in /etc/kubernetes/pki/ca.crt  -text -noout

F: Kubectl suddenly stops responding to your commands. Check it out Someone recently modified the 
    /etc/kubenetes/mainfests/etcd.yaml file
    You are asked to inestingate and fix the issue. Once you fix the issue wait for sometime for kubectl to respond. 
    Check the logs of the ETCD container.
A:      kubectl get pods ( connection to the server is refused. we have to go one step down ) 
        docker ps -a | grep  kubeapi-server (look the logs for the actuall server)
        docker logs 843d43ecff24 name (in log 2379 error this is for the etcd
    Now look for the ETCD server
        docker ps -a | grep ETCD
    Now look at the logs of the etcd
        docker logs podname
    now in the lasst line he write that server certificate for etcd is not found. so u have to see the list of the files in the Etcd
        ls /etc/kubernetes/pki/etcd
        O/P there is nor server-certificate file 
    now go into the etcd.yaml file 
        cat etc/kubernetes/manifests/etcd.yaml | grep server-certificate.crt 
        o/P now u saw the path of the file where it is saved. it is not the right place 
    now go in the particular file path and edit its file name
        vi etc/kubernetes/manifests/etcd.yaml
        change the file name(give some time )
    now new pod will be created and powered
        docker ps -a | grep etcd
    now ckeck the logs of the new container 
        docker log podname
         no error will be there 
    now run again the kubectl command
        kubectl get pods
        kubectl get nodes

F: The kube-api server stopped again! Check it out Inspect the kube-api server 
   log and identify the root cause and fix the issue.

Run docker ps -a command tio identify the kube api server container. 
Run docker logs container-id command to view the logs.
A:    kubectl get nodes 
      kubectl get pods 
      O/P will be error in the last line 
now look the logs 
      docker ps -a | grep kube-apiserver
      o/ see first pods name and copy it 
Noe see the logs 
      docker logs podname(first pod name kube api)
      docker ps -a | grep etcd
      docker logs Podname(etcd)
      o/p rejected connection 
now look at the certificate where it is cpnfigured
      cat /etc/kubernetes/manifests/kube-apiserver.yaml 
                  or
      cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep "\-\-etcd"
      o/p now our file is not save on that path, thats why there is a connection problem
now right path 
      ls /etc/kubernetes/pki/etcd (check the file where it is )
now go into the yaml file there u have to change it 
      vi /etc/kubernetes/manifests/kube-apiserver.yaml
       docker ps -a | grep etcd
       docker logs Podname(etcd)

F: How to see the files or directories 
A: ls /etc/kubernetes/pki/etcd....


________________________________________________________
        certificate Workflow API chapter 166

-  I m a administrator and have a setup a CA server and 
-  a bunch of certificates for various components 
-  we then started the services using the right certificates and all r working 
-  i m th eonly adminstrator and have uwn admin certificates and keys.

NUN A new person comes in a team 
- She needs access to the cluster 
- We need to get her a pair of certificates and keypair for her to access the cluster 
- she creates her own private key, generate a certificates signing requestt and send to me.
- i take the certificate signing request to my CA server, gets signed by the CA server 
- using private key and root certificate
- and then certicate back to her 
- now she has a key to access the server.
- The certificates have a validity period it ends after a period of time 
- Every time it expires we follow the same process of generating a new CSR and getting the sign CA server

F: What is a CA Server and where it is generated?
A:  The CA is really just a pair of keys and certificate files we have generated
-  the person have the authority can sign any certificate for the kubernetes environment.
-  they can create a manay user as they want.

CA server where you store the certificate key files stored.(only on that server)
- When u want to sign a certificate you can only do it by logging into that server.
- we placed the certificates  on the kubernetes master node itself.
- so the master node is also known as CA server 
- Kubeadm do the same thing it creates a CA Pair of files and store that on the master nodes itself.
- till now we did it always manually but now u have to do it Automatically because ur teams grow.

Kubernets has a build in Certificate API that can do this for you 
- with Certicate API you send a certificate signing request 
- Directly to kubernetes through an API call.
- he received a certoificate and did not sign it and did not logged in on the masternodes.
- he creates a kubernetes Api Obeject Known as CertificateSigningRequest.
- Once the  certuficate object is created all certificate signing request can be seen by the administrator of the cluster.
- The request can be Review and Approved requests easily using the kubectl command 
-  certificate will be extracted and shared with the user.




______________________
KUBE CONFIG

-  By default, the kube config  is stored under the users home directory.
-  if  u store the kubeconfig there then there is no need to specify the path explicitly.
-  Kubeconfig file is ina specific format.

The Config files has three sections
1) Clusters
2) Contexts
3) Users

1) Clusters:
    -  Cluster sre various cluster that you need access to.
    -  you have multiple clusters for development environment 
       or testing environment or prod or for different organisation
       or on different cloud providersetc....
    -  All those go there.
2) Users:
    -  are the users account with which you have access to these cluster.
    -  eg. Admin, Dev User, Prod User etc...
    -  These user have different privilliges on different cluster.

3) Contexts: 
    -  Context merge these together.
    -  Contexts define which user account will be used to access which cluster.
eg  -  you could create a context named admin at production that will used the admin account to access a production cluster.
    -  Admin@Production 
    -  Dev@Google
NOTE: u are not creating any new users or configure any kind of users access
      or authorization in the cluster with this process
Important: we are using excisting users and existing priviligies and 
       defining what user your going to use to access what cluster.
    -  u do not have to specify the user certificates and server address in each and every kubectl command you run




F: Each cluster may be configured with multiple namespaces within it 
   Can u configure a context to switch to a particular namespace?  
A: yes, the context section in a kubeconfig file can take additional field called namespace
    here u can specify a particular namespace
NOTE : u have seen the paths to certificates file mentioned in kubeconfig like this ca.crt sdmin.crt/ admin.key
        it is better use full path like /etc/kubetnetes/pki/ca.crt OR /etc/kubetnetes/pki/user/admin.crt 
                      OR ANOTHER WAY 
    under cluster: certificate-authority-data:  u can copy the encoded key form of the certificate by using cat ca.crt | base64



F: HOw u will do that by using the kube ctrl command 
______________________________________
F: Where is the default kubeconfig file located in the current environment?
   Find the current home directory by looking at the HOME environment variable.
A:    ECHO $HOME 
      pwd
      ls -a
      ls .kube
      ls .kube/config
      cat .kube//config

F: How many clusters are defined in the default kubeconfig file?
A:    see F1 (oben)

F: How many users are defined in the default kubeconfig file ?
A:    see F1 (oben)

F: HOw many context are defined in the default config file 
A:     see F1 (oben)

F: What is the user configured in the current context?
A:      see F1 (oben)
        cat .kube//config

F: What is the name of the cluster configured in the default kubeconfig file?
A:      see F1 (oben)
        cat .kube//config

F: A new kubeconfig file named my-kube-config is created. It is placed in the /root directory.
   How many clusters are defined in that kubeconfig file?
A:     cat /root/my-kube-config

F: How many contexts are configured in the my-kube-config file?
A:     cat /root/my-kube-config

F: What user is configured in the research context?
A:      cat /root/my-kube-config        

F: What is the name of the client-certificate file configured for the aws-user?
A:   cat /root/my-kube-config

F: What is the current context set to in the my-kube-config file?
A:   cat /root/my-kube-config

F: I would like to use the dev-user to access test-cluster-1. Set the current context to the right one so I can do that.
   Once the right context is identified, use the kubectl config use-context command.
A:      cat /root/my-kube-config
        kubectl config use-context research --kubeconfig /root/my-kube-config

F: We don't want to specify the kubeconfig file option on each kubectl command.
   Set the my-kube-config file as the default kubeconfig file and make it persistent 
   across all sessions without overwriting the existing ~/.kube/config. 
  Ensure any configuration changes persist across reboots and new shell sessions.
Note: Don't forget to source the configuration file to take effect in the existing session. Example:
A:     la -a 
       mv /root/my-kube-config /root/.kube/config
       cat /root/.kube/config
            or
        ls -l /root/.kube/config
        If it exists, the file is successfully moved.
        You can also check the old location:
F: With the current-context set to research, we are trying to access the cluster. However something seems to be wrong. Identify and fix the issue.
   Try running the kubectl get pods command and look for the error. All users certificates are stored at /etc/kubernetes/pki/users.
A:     kubectl get nodes 
       kubectl get pods 
        cat /root/.kube/config (view the contents of the file)
        
§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§
§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§
            Different commands defination
ECHO $HOME: 
    it prints the path of your home directory

ls -a 
    ls → lists files and directories in the current folder.
    -a → shows all files, including hidden ones

PWD
    pwd = print working directory
    It shows the full path of the folder you are currently in.

ls  .kube/
    ls → lists files/folders.
    .kube/ → a hidden directory (because of the dot .) usually in your home folder. 
    It stores Kubernetes configuration files.
👉 So, this command shows the contents of the .kube directory (e.g., config, cache files, credentials).

.ls .kube/config
    .kube/config → typical Kubernetes config file in your home folder.
    ./kube/config → a file named config inside a kube folder in your current working directory.

Cat 
    cat → prints the contents of a file.
    .kube//config → path to the Kubernetes configuration file. The double slash // doesn’t matter in Linux paths — it’s treated like a single /.
    So this is the same as .kube/config.
👉 Running it will display the contents of your Kubernetes config file, which usually contains:
    clusters (Kubernetes API servers)
    users/credentials (auth info)
    contexts (which cluster + user to use)
    the current-context (default cluster connection)
⚠️ Be careful: this file can include sensitive info (like tokens or certificates).

VIM :
    Vim (Vi IMproved) is a highly configurable, keyboard-based text editor.
    It is an improved version of the older Vi editor, commonly used on Linux/Unix systems
👉 In simple words: Vim = a fast, powerful text editor for programmers and system admins.
§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§
§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§§
__________________________________________________
API Groups 

F:  What is a API kunbernetes?
A:  It is the interface through which you talk to the Kubernetes cluster.
    You send commands (like create, update, delete pods) to the API server.
    The API server then makes the cluster do what you asked.
👉 Think of it as the control panel of the cluster.

F: How to see the Master node version?
A:   Curl https://kube-master:6443/version

F:  How to see the list of pods on the master Node?
A:    Curl https://kube-master:6443/api/V!/pods

NOw we lern about the Version  and the API 

Kubernetes API: 
    - Api is grouped into multiple such groups based on their purpose,
    - such as, APIsone for health, matrics and logsetc....
Kubernetes Version API: 
    -  is viewing the version of the cluster 
THe Matrics and Health 
    - are used to monitor the health of the cluster.
The API logs:
    - are integrating with third-party logging application 
this video 

API responsible for th cluster funcationality
The APIS are diveided into two parts 
  !) the Core Group 
      - Where all core funcationally exists, 
      - such as namespace,pods,rc,events,endpoints,nodes,bindings PV,PVC
      - configmaps, secrets,services etc...
            see ausdruck
  2) the named Group
      -  are more organised
          see ausdruck
F: we can also see the listed of named API throught command
A:     curl http://localhost:6443 -k

F: and how i display all the supported resources groups 
A:    curl http://localhost:6443/apis -k | grep "name"

F: How i quickly note on accessing the cluster API like that.
A:      curl http://localhost:6443 -k
___________________________________
WHY Authorization 
Admin:
As admin we can peroform any tsk adding pos, nodes  deleting pods and nodes cluster , make chanes everywhere etc.....
but soon we have another things we can do such as other administration developers, testers or other applications 

Developers:
so we create an account for them to access the cluster by creating username 
passwords or tokens or signed  TL certificates or service accounts etc...
but we donot want that all of them have the same levelof acces  as us 
we do not the develeopes have acces to modify the cluster configuration 
like adding or deleteing nodes or the storage or netwoking configuration 
we can allow them to view but not modify but 
they could have access to deploying the application 

Service accounts (users)
We only want to apply the external applications 

NOTE:
-  When we share our cluster between different organisations
-  or team s, by logically partitioning it using name space,
-  we want to restrict access to the users to their namespaces alone.
- That is what authorization can help you within the clusters.

There are different types of Authorization Mechanisms
1) Nodes
2) ABAC
3) RBAC
4) Webhook

1) Node: 
-  We know that the kubi API Server is accessed by Users, for management purpose
-  as well as kubeletes on Nodes within the cluster for managemment process.
-  The kubelet accesses the API server
   to read information about services and points,
   nodes, and pods.
Node Authorizer:
   The kubelet also reports to the Kube API Server with information about the node,
   such as its status. These requests are handled by a special authorizer
   known as the Node Authorizer.
-  we discussed that the kubelets should be part of the system nodes group
   and have a name prefixed with system node.
-  So any request coming from a user with the name system node

2) ABAC: Attribute-based authorization
-  is where you associate a user or a group of users with a set of permissions.
-  In this case, we say the dev user can view, create and delete pods.
-  You do this by creating a policy file with a set of policies defined in 
   Json format this way you pass this file into the API server.
-  we create a policy definition file for each user or group in this file.
-  Now, every time you need to add or make a change in the security,
-  you must edit this policy file manually and restart the Kube API Server.

3) RBAC: Roll Based Access Control
-  we define a role, in this case for developers We create a role with the set
   of permissions required for developers then we associate all the developers to that role.
-  we create a role for security users with the right set of permissions required for them then associate the user to that role.
-  whenever a change needs to be made to the user's access we simply modify the role
   and the changes will be done on all developers immediately.

Webhook:
-  A webhook in Kubernetes is a way for the API server to call an external service 
  (HTTP callback) whenever certain events happen.

Authorization Mode:
    1) Always Allow : allows all requests without performing any authorization checks.
    2) Always Deny: Always Deny, denies all requests.
These modes are set using the Authorization Mode Option on the Kube API Server.
if you don't specify this option, it is set to Always Allow by default.
_______________________________________________
RBAC: pod-> cluster(how to intract with the cluster)

Role :  Defines permissions within a namespace (e.g., read/create pods).
RoleBinding : Grants a Role to a user/ServiceAccount in a namespace.
              it specifiy which service account do what 
First step
To create a roll
      kubectl create -f developer-role.yaml
to give the permissions 
      go into the yaml file 
      write rules: apiGroups:[]
                    resources:[pods]
                    verbs:[list],[get]......
Second step 
link the user to roll
    now we create an another object roll binding 
    kubectl create -f devuser-developer-binding.yaml  
Check Access :
How to check the access to a particular resources in the cluster 
      kubectl auth can-i create deployment 
      kubectl auth can-i delete nodes 
Now you get a task from ur chef to get the permission to anybody, you gave it and now to have check  eheather the permission is done or not 
       kubectl auth can-i create deployment  --as dev-user
      kubectl auth can-i delete nodes --as dev-user
you can also specify the namespace in the command to check wheather the user have the permission to do something uin the namespace
        kubectl auth can-i create deployment  --as dev-user --namespace test
________________________________________________
LAB  RBAC
F: Inspect the environment and identify the authorization modes configured on the cluster.
   Check the kube-apiserver settings.
A:     cat  /etc/kubernetes/manifests/kube-apiserver.yaml
                    OR
      ps -aux |grep authorization

F: How many roles exist in the default namespace?
A:     kubectl get roles

F: How many roles exist in all namespaces together?
A:        kubectl get roles -A 
          kubectl get role -A --no-headers | wc 
          kubectl get role -A --no-headers |wc -l (display only line number)

F: What are the resources the kube-proxy role in the kube-system namespace is given access to?
A:     kubectl ger roles -A
       kubectl describe role kube-proxy -n kube-system

F: What actions can the kube-proxy role perform on configmaps?
A:     see under verb (get)

F: Which of the following statements are true?

F:  Which account is the kube-proxy role assigned to it?
A:     kubectl get rolebindings -n kube-system
       kubectl describe rolebindings kube-proxy -n kube-system


A:      k config view 
        k get pods --as dev-user

F: Create the necessary roles and role bindings required for the dev-user to create, list and delete pods in the default namespace.
   Use the given spec:
A:       kubectl create role developer --resource=pods --verb=list,create,delete
         kubectl describe role developer
now create a rolebinding 
          kubectl create rolebinding dev-user-binding --role=developer --user=dev-user
         k describe rolebinding dev-user-binding 

F:  A set of new roles and role-bindings are created in the blue namespace for the dev-user.
    However, the dev-user is unable to get details of the dark-blue-app pod in the blue namespace.
    Investigate and fix the issue.
    We have created the required roles and rolebindings, but something seems to be wrong.
A:      kubectl --as dev-user get pod dark-blue-app -n blue
        kubectl get role -n blue
        kubectl get rolebinding  -n blue
        kubectl describe role developer -n blue 
        o/p u will see that the name of the resources is blue-app in the file but we wanr dark-blue-app
        kubectl edit role developer -n blue (change the name of the resources
        kubectl describe role developer -n blue (to check wheather the name is changed or not)
        k -as dev-user get pods dark-blue-app -n blue

F:  Add a new rule in the existing role developer to grant the dev-user permissions to create deployments in the blue namespace.
    Remember to add api group "apps".
A:       kubectl --as dev-user create deployment nginx --image=nginx -n blue
          kubectl edit role developer -n blue 


____________________________________________________________
Cluster Roles 
