Kubernetes  is a Open-Source system for Automating the deployment, scaling and management of containerized application.
fEATURES 
  - Conatiner Orchestration:
      Manages Conatines  accross a cluster of maschine.
  - Self-healing:
      Resatrts failed containers, repolace them., Kills unresponsive ones.
  - Sacling:
      Automatically increase/descrease resources based on load
  - Load Balancing :
      Distrub straffic to keep the system responsive.
  - Service Discovery: 
      Makes it easy for services to find and talk each other.
  - Configuration Management: 
      Store and  manages secret and application settings.
_____________________________________________________________________________________

CORE CONCEPT:
  1) Container: 
      Container is the basic unit of software in Kubernetes, packaging applications with their necessary code, libraries, and dependencies.
  2) POD:
      The smalles  deployment unit;
      wraps one or more containers.
  3) Node (server):
      A Single maschine (Physical or virtual) in the cluster.
  4) Cluster:
      A group of nodes managede by Kubernetes.
  5) Deployment:
      Tells Kubernetes how to create and manage Pods.
  6) Service:
      Exposes Pod to the network and handels traffic routing.
_____________________________________________________________________________________
Typical Use Case:
- Bild your app in Dockers container. Then you use Kubernetes to:

1 Deploy the containers across fleet to server.
2 Ensure they stay running.
3 Scale them up during traffic spikes.
4 Roll out new versions without downtime.
_____________________________________________________________________________________
KUBERNETS HAS TWO MAIN PARTS 
  1) MASTER(CONTROL PLANE)
  2) WORKER NODES
_____________________________________________________________________________________

COMPONENETS OF MASTER:

- API Server:
    It Provide the interface (Command line Interface) also known as qubecontrol.
- Schedular:
    Assign node to newly created Pods. ((Pod is a small Unit where the container run it. We have multiple nodes and like to run a new container.
    Which node will be assigned this work has been done by the schedular.)
- ETCD:
    Key value, store having all cluster Data.
    we have all the cluster, Different Nodes, Applications , containers and Pods that store the information ETCD
-  Control Manager
    control Manager is responsible for managing the state of the cluster (When something is damage or kaputt then Control manager repair )
_____________________________________________________________________________________
WORKER NODES

- KUBLET: 
    Agent,make sure that containers are running in Pods.
    (It checks wheather the conatianers are working Properly in the Pods yes or NO)

- POD: 
    the Containers are running in the Pod.
    (Checks that all the conatainers are working Properly  with in the POD)
    A single Instance of a running process in a cluster. It can rum one or more conatiners and share the same resources.

- Kube-Proxy:
    Maintain the Network rules for communication with POds.
    eg. we have a cluster, we have inside Network, we have Outside Network, There are many Pods running in the NOde(to run/Maintain all the Things we use Kube-Proxy. 

- Container-runtime:
    A tool is responsible for running containers er. Dockers 
    
_____________________________________________________________________________________
Difference between Docker and the ConatinerD



_____________________________________________________________________________________
ETCD:


_____________________________________________________________________________________
NOTE: if we not move a one pod from one node to another node.
      we can only delete the pod and create a new pod on anothe node where u want 


F: How to get the help for the commands 
    kubectl run --help
F: On which image used to create the new Pods?
    kubectl describe pod newpodname 
    kubectl  get pods-o wide 
F: How many containers are in the pod 
    kubectl describe pod 
F: Why do you think the computer agenty in pod webapp is in error

F: What does the READY column in the output of the kubectl get Pods commans indicate 
    1/2  Running container in PODS/Total container in POD 
F: 
F: How to create the Pods?
    kubectl run ngnix --image=ngnix

F:  How to delete the pod?
      kubectl delete pod webapp
      kubectl delete pods --all 
F: How to make the change in the pod?
      kubectl edit pod podname
F: How to Create a new Pod with the name of the rednis and with the image rednis123?
   Note: Use the POD-Defination YaAML file. And yes the image name is wrong
    kubectl run rednis --image=rednis123 --dry-run -o yaml
    kubectl run rednis --image=rednis123 --dry-run=client -o yaml
    kubectl run rednis --image=rednis123 --dry-run=client -o yaml > rednis.yaml
    cat rednis.yaml

--dry-run: It is only used to check th command wheather it is right or wrong and this is the 
  clint side command. The request will not go to the server 


Note: Use the POD-Defination YaAML file. And yes the image name is wrong
a) how to create a yaml file 
    kubectl create -f rednis.yaml
b) see weather thepod s file is created or not 
    kubectl get pods


F: How to change the image on this pod to rednis. Once done, the pod should be in a running state?
    cat redis.yaml
    vi redis.yaml
    kubectl applay -f rednis..yaml
    kubectl get pods

F: How to show the labels 
    kubectl describe pod rednis
    kubectl describe pod --show-labels 

--dry-run: It is only used to check th command wheather it is right or wrong and this is the 
  clint side command. The request will not go to the server 

 

____________________________________________________________________

A ReplicationController is a legacy Kubernetes controller that ensures a specified number of pod replicas are running at any given time.

--Function of replicaController
    Automatically replaces failed pods
    Ensures the desired number of pods are always running  
    Used in earlier versions of Kubernetes

-REPlication Controller:(what and Why we need this)
    - Replication controller helps us to run multiple instances of a single pod in the kubernetes cluster,
      thus providing high availability.
    - Replication controller can help by automatically bringing up a new pod when existing one fails
      thus the replication controller ensure that the specified numbers of pods are running all the times
      even it 1 or 100

NOTE: we can also use the replication controller when we use the one pod not only multiple pods 
There are two similar terms(same perpouse but not same)
1) Repelation Controller: is the older technology that is replaced by Replica Set 

2) Replica Set: is the new Recomended way to set up replication.

- How to create a Replication Controller defination file 
    rc-defination.yml
- As we any kubernets defination file we have 4 section
  1)  API version 
  2)  Kind
  3)  Metadata 
  4)  Specification  but written as spec NOTE we cn tested the two defination file together 

1) API version: is specific to what we are creating 
2) Kind : As we know oit is Replacation Controller.
3) Meta Data: Add Name, Labels, app type and assign values to them 
4) spec. What is inside the object we are creating.
NOTE: we can copy from the pod -defination file the data into/under the spec template  
____________________________________________________________________
                          LAB 2 Replication controller

F: How to create the Replication  Controller
    kubectl create -f rc-defination.yml 
F: How to see the Replication Controller list 
    kubectl get replicationcontroler 
F: How many Pods are created by the replication Controller
    kubectl get pods
____________________________________________________________________
REPLICA_SET
    A ReplicaSet is the newer version of ReplicationController that 
    also ensures a specified number of pod replicas are running continuously, 
    
    but with more powerful and flexible label selectors.

there is only one differencence between replication controller and Replica set 
- Replica Set requires a selector defination the selector section helps the Replica Set 
  identify what pods fall under it.

Festures of the Replica Set 
    Same purpose as RC: maintain a stable number of pods
    Supports set-based label selectors
    Commonly used as part of a Deployment

- In Replica set there are three sections 
    1) Template 
    2) replicas 
    3) Selector 

_____________________________________________________________________


____________________________________________________________________
                            LAB Replica Set 

F: How to create the Replica Set
    kubectl create -f replicaset-defination.yml 
F: How to see the Replic set list 
    kubectl get replicaset 
F: How to create a Pod in the replicaSet?
    kubectl describe replicaset
F: How to delete the Replicaset Pod
    kubectl delete replicaset myapp-replioaset
    (Also delete all Underlying pod)
F: How to replace and update the Replica set
    Kubectl replace -f relicaset-defination.yml
F: How to kube control scale command to scale the Replicaset 
    kubectl scale -replicas=6 -f replicaset-defination.yml
F: How to check the apiversion of replicaset 
    kubectl api-resources | grep replicaset
F: 
____________________________________________________________________

                        LABELS AND SELECTORS 

         
____________________________________________________________________
                    LAB  Lables and Selsctors  
F: How to create an Labels 
       kubectl describe pod nginx | less
F: How to attach the new Labels 
      kubectl label pod nginx env=testing 
F: How to overright the label 
      kubectl label --overwrite pod ngnix env=prod
F: How to delete the Label
      kubectl label pod ngnix env-
F: How to see/display the Labels 
      kubectl get pods --show-labels 


____________________________________________________________________
                    DEPLOYMENT

Deployment is used to manage the automatic life cycle of the pods. 
It create updates scaling and rollback of the Instances when any eroor occur
NOTE: Deployment create an Replicaset automaticall. When we run
      kubectl get replicaset
      You will see the replicaset file.


Features of the Deployment
1. Manages Replica Pods
    It makes sure that a specified number of Pods are always running.
2. Self Healing
    If a pod crashes or gets deleted, the Deployment will automatically create a new one to replace it.
3. Rolling Updates
      You can update your application image or configuration, and the Deployment will roll out changes gradually with zero downtime.
4. Rollback support 
      If something goes wrong during an update, you can easily rollback to a previous stable version.

_____________________________________________________________________
              How to create a Deployment file 
- The Deployment file is same like a Replicaset 
    only in Kind: DEPLOYMENT
- Contents of the File
    API Version:
    Metadata: 
      name 
      labels
    spec:
        template:
          metadata:
          
    replica: 
    selector:
_____________________________________________________________________
              LAB 3 deployment

F: How many pods are created
      kubectl get POD 
F: HOw many replica set are created 
      kubectl get rs
F: HOw many deployments are created 
      kubectl get deployments
F: How many deployments exists on the system. We just creaed a one Deployment 
      kubectl get deployments 
F: How many rs are created
      kubectl get rs
F: How many PODS are created 
      kubectl get Pod
F: Out of all the PODs how many are READY
      kubectl get deployments 
F: What is the image used to create the POD in the new deployments 
      kubectl describe pod frontend-deployment-7fd8cdb696-stmbx  
F: Create a new Deployment using the deployment-definition-1.yaml file located at /root/.
A:    pwd (to go to the root)
      ls (see the list of the deployments files)
      kubectl create -f deployment-defination-1.yaml (to create a yaml file on the root)
      vi deployment-defination-1.yaml ( to make the changes on the yaml file)
      kubectl create -f deployment-definition-1.yaml (to create a .yaml file on the root)

 
F: Create a new Deployment with the bwlow attributes using your own deployment defination file 
    NAME : http-frontend:
    replica: 3 
    image:httpd:2.4-alpine
A:  kubectl create  deployment --help ( to get the help of the deployments commands)
    kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replica=3

NOTE: If u forget to create a replica then use 
      kubectl scale deployment httpd-frontend --replicas=3


_____________________________________________________________________


            Kubernetes services
Kubernetes services enable the communication betwen the various components and with in the outside the application 
it helps us to connect applications together
-- typically using a stable DNS name and IP address.


Types of services

  1) NodePort
  2) ClusterIP
  3) LoadBalancer 

Curl: Curl is used to talk with the AIP server (u can send or get the data from the networl
curl is a command-line tool for transferring data to or from a server using protocols like HTTP, HTTPS, etc.

 1) Why we need Services?

    - Pods are temporary â€” they can die, restart, or be replaced.
    - Each Pod gets a unique IP, which may change.
    - A Service provides a stable way to reach Pods, regardless of changes in the underlying Pod IPs.

2) Types of Services:

    - ClusterIP (default): Exposes the Service on an internal IP in the cluster. Accessible only within the cluster.
    - NodePort: Exposes the Service on a static port on each Nodeâ€™s IP. Allows external traffic to access the Service via <NodeIP>:<NodePort>.
    - LoadBalancer: Uses a cloud provider's load balancer to expose the Service externally.
    - ExternalName: Maps a Service to a DNS name (e.g., external database).

3) How Services Work:

    -  A Service uses labels to find the set of Pods it targets.
    -  For example, if all backend Pods have a label app=backend, the Service will route traffic to all matching Pods.

4) DNS Names in Kubernetes:

    - Services are automatically assigned a DNS name.
    - For example, a Service named db-service in the marketing namespace can be accessed at:
          db-service.marketing.svc.cluster.local
5) Ports:

    - Services define a port to expose and optionally a targetPort (the port on the Pod).
    - Example: A Redis Service might expose port 6379.

NOTE: If an application wants to connect to a Redis database, it doesnâ€™t need to know the IP of the Redis Pod.
      It just connects to redis-service:6379, and the Service routes traffic to the correct Pod(s).

________________________________________________________________
                LAB 4 Services

F: How many Services exist on the system?
   In the current(default) namespace
A:   Kubectl get service
     kubectl get svc

F: What is the type of the default kubernetes service?
      kubectl get svc

F: What is the targetport configured on the kubernetes service?
      kubectl describe svc kubernetes

F: How many labels are configured on the kubernetes service?
     kubectl describe svc kubernetes

F: How many Endpoints are attached on the kubernetes service?
    kubectl describe svc kubernetes

F: How many Deployments exist on the system now?
   In the current(default) namespace
    kubectl get deploy

F: What is the image used to create the pods in the deployment?
      kubectl describe deploy simple-webapp-deployment


Create a new service to access the web application using the service-definition-1.yaml file.
Name: webapp-service
Type:       NodePort
targetPort: 8080
port:       8080
nodePort:   30080
selector:
 name:      simple-webapp

A:  ls
    cat service-defination-1.yaml
    vi  service-defination-1.yaml'(update the file) 
    kubectl create -f service-defination-1.yaml
    
   _____________________________________________________________________
What is a Namespace in Kubernetes?
    A namespace is a way to divide a Kubernetes cluster into multiple virtual environments,(separate, logical parts)
    like creating separate compartments for different teams, projects, or applications.

F: Why use Namespaces?
   - To separate environments (e.g., dev, test, prod)
   - To manage multiple teams or projects in one cluster
   - To apply resource limits and access control per group

1) Isolation:(à¤µà¤¿à¤­à¤¾à¤œà¤¨ à¤”à¤° à¤…à¤²à¤—à¤¾à¤µ)
    - Namespaces provide logical isolation between groups of resources (like Pods, Services, Deployments).
    - For example, two teams can each have a web-app running in separate namespaces (team-a and team-b) without interfering with each other.

2) Resource Scoping:
    - Most Kubernetes resources (like Pods, Services, ConfigMaps) live within a namespace.
    - Some resources (like Nodes, PersistentVolumes) are not namespaced.

3) Access Control:
    - Role-Based Access Control (RBAC) rules can be applied per namespace, giving fine-grained permissions.

4) Resource Quotas:
    - Namespaces can have quotas (CPU, memory, number of objects) to prevent one team from using up all cluster resources.

5) Default Namespace:
    - If you donâ€™t specify a namespace, Kubernetes uses the default namespace.
______________________________________________________________________
              LAB 5 Namespaces 

F: How many Namespaces exist on the system?
      kubectl get namespaces 
      kubectl get ns

F: How many pods exist in the research namespace?
      kubectl get pods --namespace=research
      kubectl get pods --n=research

F: Create a POD in the finance namepsace. Use the spec given below
      kubectl run redis --image=redis -n=finance
    

F: To see wheather the pod is created in the finance
      kubectl get pod -n=finance

F: Which namespace has the blue POD in it?
      kubectl get ns 
      kubectl get pods  --all-namespaces (this is used to display the namespaces AND all the pods
      kubectl get pods -A

F: What DNS Name should the Blue application use to access the database db-service in its own namespace-marketing
   You can try it in the web application UUUI. USe port 6379
      kubectl get pods -n=marketing
      kubectl get svc -n=marketing

F: What DNS Name should the Blue application use to access the database 'db-service' in the devnamespace
   You can try it in the web application UUUI. USe port 6379
      kubectl get svc -n=dev
      

__________________________________________________________

Imperative und Declarative 



__________________________________________________________
                LAB 6 (Impariative Commands)
F: Deploy a pod named nginx-pod using the nginx:alpine image.
      kubectl run nginx-pod --image=nginx:alpine

F: Deploy a redis pod using the redis:alpine image with the labels set to tier=db.
      kubectl run redis --image=redis:alpine --labels="tier=db"

F: Create a service named redis-service to expose the existing redis pod within the cluster on port 6379.
      kubectl expose pod redis --port 6379 --name redis-service
      kubectl get svc

F: Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas.
       kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3
       kubectl get deploy

F: Create a new pod called custom-nginx using the nginx image and run it on container port 8080.
       kubectl run custom-nginx --image=nginx --port=8080

F: Create a new namespace called dev-ns.
        kubectl create namespace dev-ns

F: Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2 replicas.
        kubectl create deployment redis-deploy --image=redis --replicas=2 -n  dev-ns
        kubectl get deployment -n dev-ns

F: Create a pod named httpd using the image httpd:alpine in the default namespace.
Then, create a service of type ClusterIP with the same name (httpd) that exposes the pod on port 80.
      kubectl run httpd --image=http:alpine --port 80 --expose=true 
__________________________________________________________________

Chapter 2 Schudeling 

LAB 7

NOTE When u want to schedule the port means(u delete the pod from one Node to another)
      Youu habe to do 4 steps 
        kubectl get pods 
        Kubectl get pods -o wide(to see in which node is the pod)
        vi nginx.yaml(open the pod and change the node name Manually and save it)
        kubectl replace --force -f nginx.yaml(delete the pod pod from one node and create a new pod to another node)
                                             (this command delete the data from the pod )
                                             ( when we want that the data wilkl not be deleted we have to use  Persistent Volume (PV) )

F: A pod definition file nginx.yaml is given. Create a pod using the file.
   Only create the POD for now. We will inspect its status next.
      kubectl get pods
      ls (to see the files)
      cat nginx.yaml (display all the data from the file)
      kubectl create -f nginx.yaml (to create a pod though the file)
      kubectl get pods

F: What is the status of the created POD?
      kubectl get pods 

F: Why the pod is the pending state?
   Inspect the environment for various kubernetes control plane components.
A:    kubectl describe pod nginx
      kubectl get pods -n kube-system

F: Manually schedule the pod on node01.
   Delete and recreate the POD if necessary.
      ls
      vi nginx.yaml (make changes nodeName: node01)
      kubectl get pods 
      kubectl  get pods --watch (to watch the pods wheather it make any changes )
      kubectl replace --force -f n
F: Now schedule the same pos on the controlplane node
   Delete and recreae the pod if necessary
      kubectl get pods 
      vi nginx.yaml(make the changes) 
      kubectl replace --force -f nginx.yaml
      kubectl get pods
      kubectl get pods -o wide 

      _________________________________________________________________________________
                    Labels and selectors  LAB  7

F: We have deployed a number of PODs. They are labelled with tier, env and bu. How many PODs exist in the dev environment (env)?
   Use selectors to filter the output
        kubectl get pods 
        kubectl get pods --selector env=dev 
        kubectl get pods --selector env=dev | wc -l (it display number of lines with header)
        kubectl get pods --selector env=dev --no-headers| wc -l(it display the nummber of lines without header)
NOTE: first  label=podName 
             env=dev 

F: How many PODs are in the finance business unit (bu)?
        kubectl get pods --selector bu=finance --no-header | wc -l

F: How many objects are in the prod environment including PODs, ReplicaSets and any other objects?
        kubectl get all  --selector env=prod --no-header | wc -l
NOTE all is use to see all the obejects are created in the environment prod


F: Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
        kubectl get all --selector env=prod,bu=finance,tier=frontend

F: A ReplicaSet definition file is given replicaset-definition-1.yaml. Attempt to create the replicaset; you will encounter an issue with the file. Try to fix it.
  Once you fix the issue, create the replicaset from the definition file.
        ls
        cat replicaset-definition-1.yaml 
        kubectl create -f replicaset-definition-1.yaml 
        vi replicaset-definition-1.yaml (change the name of the tierName in the selector)
        kubectl create -f replicaset-definition-1.yaml
        kubectl get rs

___________________________________________________________________
tant and tolaration:

tant and tolerations areused to set the restrictions.
On which Node the pod can be scheduled

Tains and toleration doesnot tell the pod to go to a particuler Node,
instead tell the node to accept only pods with certain tolerations
NOTE: 
the tanits are set on Nodes.
the toleration are set on the Pods
Now the 

Syntax:   kubectl taint nodes name key=value:taint-effect
eg.       kubectl taint nodes node1 app=blue:Noschedule

1) taint-effect: 
    defines what would be happend to the pods, if they do not tolerates the taint there are three taint effects.
    - NoSchedule:
      Which means that the pod will not be scheduled on the node.
    - PreferNOSchedule:
      Which means the system willl try to avoid placing a pod on the nodes and that is no gauranted.
    - NoExectute:
      Which menas that the new pod will not be scheduled on the node and existing pods on the node. if any will be evicted
      means they do not tolerate the taints 

Toleration :
      Tolaration are added to pods. To add the toleration to pods first pull the defination file and change the tolerations: under spec:
      How to write in the file:
      toleration: 
      key:"app"
      operator: "Equal"
      value:"blue"
      effect: "NoSchedule"

F:   how to see wheather the tent are attached there 
     kubectl describe nodes worker01 | less 
__________________________________________________________________
      Lab 8 Taint and toleration 

 F: How many nodes exist on the system?
    Including the controlplane node.
 A:    kubectl get nodes

 F: Do any taints exist on node01 node?
 A:     kubectl describe node node01

F: Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule
A:      kubectl taint node node01 spray=mortein:NoSchedule
        kubectl describe node node01  (to check wheather the taints are created are not)

F:  Create a new pod with the nginx image and pod name as mosquito.
A:      kubectl run mosquito --image=nginx

F: What is the state of the POD?
A:      kubectl get pods

F:  Why do you think the pod is in a pending state?
A:      kubectl describe pod mosquito

F: Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.
   Image name: nginx
   Key: spray
   Value: mortein
   Effect: NoSchedule
   Status: Running

A:      kubectl run bee --image=nginx --dry-run=client -o yaml
        kubectl run bee --image=nginx --drs-run=client -o yaml > bee.yaml (recreate a file)
        vi bee.yaml
              tolerations:  
               - key: spray
               value: mortein
               effect: NoSchedule
               operator: Equal
        kubectl create -f bee.yaml

F: Observe that the bee pod has been scheduled on node node01 due to the toleration that has been configured for the pod.
        kubectl get pods 
        kubectl get pods --watch

F: Do you see any taints on controlplane node?
A:       kubectl describe pod controlplane

F: Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
A:       kubectl taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule-

F: What is the state of the pod mosquito now?
         kubectl get pods 

F: Which node is the POD mosquito on now?
          kubectl get pods -o wide


__________________________________________________________
Node Selector 
  A node selector is a simple way to tell a Pod which nodes it can run on based on labels.
      Nodes have labels (e.g., disktype=ssd).
      Node selector in a Pod specifies these labels.
      Kubernetes will schedule the Pod only on nodes matching those labels.

We can select the pods that the pod run only on the specific nodes: there are two ways to doe that
1) Using the node selector (Simple and eiser method)
    - open the pod defination yaml file 
    -  to limt this pod we add a new propertie as a     nodeSelector  under spec: section
    -  you have to write nodeSelector: Large (where the large comes from)
    -  you have to declare the large as a label and selector 
    -  to specify the labels on the Nodes 

Syntax:      Kubectl label nodes <node-name> <label-key> = <label-value>
eg:          lubectl label nodes node-1 size=Large

    - this is very simple example What is when we want to put the pod on the not small and not on the large Node
      In this case we have to use NODE AFFINITY
------------------------------------------------------
NODE AFFINITY:

Node Affinity is used to control which nodes a Pod can be scheduled on--- based on the labels assigned on the nodes.
This is the best to tell the Kubernetes Scheduler.
eg i want this pod run on the nodes that  matches the things(location , speicher, hardware.......)

ðŸ”¸ How is it Different from nodeSelector?

Feature	            nodeSelector	            nodeAffinity
Type	              Simple key-value match	  Advanced expressions supported
Flexibility	        Limited	                  More flexible (AND, OR, ranges)
Priority Support	  No	                      Supports soft rules (preferred)


ðŸ”¸ Types of Node Affinity
1) requiredDuringSchedulingIgnoredDuringExecution(Hard rule)
        Pod will not be scheduled if node doesn't match
        Once running, the rule is ignored

-------   DuringScheduling: is the state where a pod does not exist and created for the first time.

F: What happend what if the nodes  with matching labels are not available or forgot it ?
A: now i have to use the node Affinity. If i used 
    1) requiredDuringSchedulingIgnoredDuringExecution  (Hard rule)
        the Scheduler will mandate that the pod be placed on a node with the given affinity rules 
        if it cannot find one, the pod will not be scheduled.
NOTE: this type is ued when the placement of pod is crucial. if the maching nodes does not exists the pod will not be scheduled.


---- if the placement of Pod is less important than running the workload itself in the t case we used
    2)  preferredDuringSchedulingIgnoredDuringExecution   (Soft rule)
          where the matching node is not found. the Scheduler will not simply ignore node affinity rules and place the pod of any node.
NOTE: this is the way to tell the scheduler 

---------- DuringExecution:  is a state where the pod has been running and a chane is mede in the Environment that affects on the node affinity 
          Such as change in the label of the node.
eg. Some one remove the label, but we are not able to change the labels on the Node.

---- if you want to change any Pod that are running on Nodes 

        Kubernetes tries to schedule to a matching node, but can skip if none match

3)  requiredDuringSchedulingRequiredDuringExecution (not stable)
        Rule is enforced both at scheduling and runtime
___________________________________________________

____________________________________________________
LAB 9 
_____________________________________________________
Daemon Set 
  -  Daemon sets are like the replica sets
  -  It helps you to deploy multiple instances of pods.
  -  But it runs one copy of pod on each node in your cluster.
  -  Whenever the new  node is added to the cluster a replica of the pod is automatically added to that node.
  -  And when a node is removed then the pod is automatocally removed.
  -  It ensure that the one copy of the  pods is always present in all nodes in the cluster 

NOTE: runs one copy of pod---- What is in the pod --- What it do 
ðŸ” What is inside that Pod?  
      The Pod contains whatever you define in the DaemonSet YAML configuration.
      Typically, DaemonSets are used to run background or system-level tasks on every node.

ðŸ”§ Common examples of what's inside:
      -  Log collectors (e.g., Fluentd, Filebeat)
      -  Monitoring agents (e.g., Prometheus Node Exporter)
      -  Security agents or antivirus
      -  Network plugins or storage daemons
Good example is
  (Kube-proxy) for every workerNode 
  (Weave-net) for Networking Solution agent on each Node in a Cluster 
____________________________________________________________
      LAB 10
F:  How many DaemonSets are created in the cluster in all namespaces?
    Check all namespaces
A:   kubectl get Daemonsets -A

F: Which namespace is the kube-proxy Daemonset created in?
A:   kubectl get Daemonsets -A

F: Which of the below is a DaemonSet?
A:   kubectl get all --all-namespaces
     kubectl describe Daemonsets kube-proxy -n kube-system

F: On how many nodes are the pods scheduled by the DaemonSet kube-proxy?
A:   kubectl describe daemonset kube-proxy --namespace=kube-system

F: What is the image used by the POD deployed by the kube-flannel-ds DaemonSet?
A:   kubectl describe daemonset kube-flannel-ds --namespace=kube-flannel 

F: Deploy a DaemonSet for FluentD Logging.
   Use the given specifications.
   Name: elasticsearch
   Namespace: kube-system
   Image: registry.k8s.io/fluentd-elasticsearch:1.20
A: - kubectl create deployment elasticsearch -n kube-system --image=k8s.gcr.io/fluentd-elasticsearch:1.20 --dry-run=client -o yaml 
   - kubectl create deployment elasticsearch -n kube-system --image=k8s.gcr.io/fluentd-elasticsearch:1.20 -o yaml > fluent.yaml
        MAKE CHANDES IN THE FILE #
        kind: Daemonset 
        delete the replica/delete line 13(strategy {})/ lat line status:{}
    -  kubectl create -f fluentd.yaml
    -  kubectl get ds -n kube-system



    
______________________________________________
  Static pod (Chapter 76)

Static pods are directly managed by the kubectl on a specific node, rather than the kubernetes API server/scheduler
All the master Nodes are the Components of the static pod 
- Kube/API-server
- Kube Scheduler
- Kube-controller-manager
- ETCD
- Cloud Controll Manager(Optional)

F: What happend when all the above Components  in the cluster  are not there. How kubelet(Worker Node) create the Pods USW.....
A:  -  The Kubectl can manage the Node Independently
    -  we Installec the Kubelet and docker(Containers) Installed 
    -  There is no kubernetes cluster and all the Components of the Master Nodes
    -  To create a Pod we need the detail of the Pod in a Pod defination file.
    -  Now we have to configure the Kubelet to read Directory on the server 
    -  Place the defination file in htis directorsy
    -  It checks and read the file and create the Pods on the host.
    - If the application crasch the kubectl restart it.
    -  If changes are made the kubelet recreate the pod 
    - If remove the file, the pod will be deleted 
    - THESE PODS ARE KNOWN AS STATIC PODS
NOTE : we cannot create the pods with the help of relicas, deployments usw----

IMPortant 
    1) View and Config theis option
    2) use Methods to set the cluster 
        -  Check the pod-manifest-path in the kubelet services file 
        -  When not find look at the config file 
        -  Within the Configfile find StaticPodPath
        -  We Cannot use the Kubectl command because it is used with the Api-server 
        -  


_________

LAB 11
F: How many static pods exist in this cluster in all namespaces?
A: kubectl get pods -A
      To know that these are the static PODs
      The Pod name is extened/append with the node name.
F: How to see the static pod in detail 
    kubectl get pod coredns-77456fff-brwnd -n kube-system -o yaml 

F: Which of the below components is NOT deployed as a static pod?
A:    kubectl get pods -A

F: Which of the below components is NOT deployed as a static POD?
A:   kubectl get pods -A
 
F: On which nodes are the static pods created currently?
A: kubectl get pods -A

F: How to see where is the config file is srtored (I think it is falsch )
AAAA: kubectl get configmap -n kube-system kubeadm-config -o yaml

__________________________________

Priority


_______
LAB 12
______
Multiple schedular 

We use more than one Schedular in a Cluster.
Every Schedular uses a seprate Configuration file and each file having its own Schedular name(my-schedular-2.service)
F: How its work if u deploy the schedular as a pod?
A:  -  We create a pod defination file and specifiy the kubeconfig property
    -  Under Sspec: Conatiner:  Command: write the path of the config file.
    -  that has the authentication information to connect to the kubernetes API server.
    -  create a each Scheduler as  yaml file and write the (LleaderElection) 
    -  leader erection is used when u have multiple copies of the scheduler running on the different master node.
    -  If multiple copies of the same Scheduler are running on different nodes, only one can be activ at a time 
       and thats why the leaderElection help who will lead the scheduling activities.
Note:  If the scheduler was not configureed correctl, then the pod will continue to remain in a 
       pending state(look Pods logs under Kubectl describe command)
       an if good is it will in the running state.


F: How to look that which schedular is picked up 
    kubectl get events -o wide(Source)
F: If we have the running issues how to see the logs ?
    kubectl logs my-custom-schedular --name-space=kube-system
_______________________________________________________________________________________
          LAB 13
F: What is the name of the POD that deploys the default kubernetes scheduler in this environment?
A:    Kubectl get pos -A

F: What is the image used to deploy the kubernetes scheduler?
   Inspect the kubernetes scheduler pod and identify the image
A:    kubectl describe pod kube-schedular-controlplane -n kube-system


F: We have already created the ServiceAccount and ClusterRoleBinding that our custom scheduler will make use of.
   Checkout the following Kubernetes objects:
      ServiceAccount: my-scheduler (kube-system namespace)
      ClusterRoleBinding: my-scheduler-as-kube-scheduler
      ClusterRoleBinding: my-scheduler-as-volume-scheduler
Run the command: kubectl get serviceaccount -n kube-system and kubectl get clusterrolebinding
A:   kubectl get sa my-scheduler -n kube-system    ( to check the service account)

F: Please create a ConfigMap that the new scheduler will utilize, implementing the concept of ConfigMap as a volume.
   A ConfigMap definition file named my-scheduler-configmap.yaml has
   been provided at the /root/ path. This file will be used to create a
   ConfigMap with the name my-scheduler-config, utilizing the content
   from the file located at /root/my-scheduler-config.yaml.
A:     kubectl create configmap my-scheduler-configmap --from-file=/root/my-scheduler-config.yaml -n kube-system
       kubectl get configmap my-scheduler-configmap -n kube-system


F: Deploy an additional scheduler to the cluster following the given specification.
    Utilize the manifest file located at /root/my-scheduler.yaml. Ensure that you are using the same image as that of the default Kubernetes scheduler.  
    To verify the image used by the default Kubernetes scheduler, execute the following command:
    kubectl describe pod kube-scheduler-controlplane --namespace=kube-system
Note : Deploying the new scheduler may take a few seconds to reach a running state.
A.     kubectl get pods -A
       kubectl describe pods my-scheduler-controlplane -n kube-system | grep Image  (to see only imagename)(copy the image name)
       vi my-scheduler.yaml (change the image name)
       kubectl create -f my-scheduler.yaml( to create the pod as yaml file) 
        kubectl get pods  -n kube-system( to see the yaml file wheather the file is created or not) 

F:   Please modify the provided Pod manifest file located at /root/nginx-pod.yaml 
     to specify that the Pod should be scheduled by your custom scheduler, which is named my-scheduler.
     After updating, create the Pod in the default namespace and verify it is scheduled by your custom scheduler.
Note : The pod may take a few seconds to reach a running state.

A: ls
   vi nginx-pod.yaml( add the schedular name under spec:)
   kubectl create -f nginx-pod.yaml
  (u have to create the command because the yaml file is always saved in the local and pods are on the cluster thats why u have to upadate the cluste through this command)
    kubectl get pods (check wheather the pods is running or not) 
_________________________________________
      Admission Controllers      (Securing kubernetes)

-  We are running the commands from the command lines.
-  Kubectl utility to perform various kind of Operations on our kubernetes cluster.
1) we send a request to create a pod ---> the request goes to API server ---> then the pod is created ---> ths information is saved in the Etcd database
2)  when request hit the API server ---> goes through an anthentication process ---> create a pod 
    (authentication process is responsible identify the user who send the request and making sure the user is valid 
3) kubectl ---> Authentication ---> Authorization ---> create a pod 
    request goes to Authorization process check if the user has permission performed that operation this is done throught th e role-based access controls.
F: What is role-based control:
    we can place different kind of restrictions such as allow or deny with particular role like create list or delete depÃ¼loyments or services.
F: Why we use the Admission Controllers
    - Security
    - Policy Enforcement
    - Automatic Modification
    - Consistency
4) kubectl ---> Authentication ---> Authorization ---> Admission Controllers ---> create pod 

-  Admission controllers has additional operations before the pod is created 
      -- Always pullimages (when the pod is created it will pull the images)

      -- DefaultStorageClass ( observe the creation of the PVC automatically add default storage class to them)

      -- EventRateLimit

      --NAmespaceExists

    -- NamespaceAutoProvision (create automatically the namespace if it doesnot exists)

eg..  We craete a pod with name nginx and image name is also nginx and in the namespace blue 
      the namespace will not be find in the it will be rejected 
       kubectl ---> Authentication ---> Authorization ---> Admission Controllers checks wheather the namspace is find when yes it created a pod otherwise it will be rejected


F: How to see the enable Admission Controller bydefault 
    kube-apiserver-h | grep enable-admission-plugins

_________________
LAB  14

F:  Which admission controller is not enabled by default?
A:    kubectl get pods -n kube-system
      kubectl exec -it kube-apiserver-controlplane -n kube-system --kube-apiserver -h | grep 'enable-admission-plugins'

F: Which admission controller is enabled in this cluster which is normally disabled?
      vi /etc/kubernetes/manifests/kube-apiserver.yaml
      grep enable-admission-plugins /etc/kubernetes/manifests/kube-apiserver.yaml

F: Create an nginx pod in the blue namespace. Please note that the blue namespace does not currently exist. Do not create the blue namespace at this time.
   Execute the following command to deploy a pod using the nginx image within the blue namespace:

F: 
      vi /etc/kubernetes/manifests/kube-apiserver.yaml 
      kubectl get ns

F:  Delete the existing PVC named myclaim:
      kubectl delete pvc myclaim

F: Reapply the same manifest:
      kubectl apply -f myclaim.yaml
F: Check the status of the PVC:
    kubectl get pvc myclaim

F:   Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins.
A:   ps -ef | grep kube-apiserver | grep admission-plugins
______________________________________________
Validating Controller (how we can controll our Admission Controller)
example Namespace:
It can  help valdate if a namespace already exists, and reject the request when it doesnot exists

There are two type of bildin Admission Controller 
1) Mutating Admission Controller 
      are those that can change the request 
      generally, mutating admission Controller are invoked first followed by valadating admission Controllers.if it was run th eother way then it will always be rejected.

2) Valaditing Admission Controller
      are those that can validate the request and allow or deny it and 
      there may be Admission controller that can both that can mutate a request as well as Validate the request

We can also create our own Admission Controller. 
there are two special external admission Controller availabe

1) MutatingAdmissionWebhook
2) ValidatingAdmissionWebhook

F: What is a webhook
    it is a service on the server to create our own code own logic for Admission Controll
    we can configure the webhook to point andit hosted ob in the kubernetes cluster or outside our server
    When i send the request it always hit the webhook ---> make admission webhook server ---> by passing Admission Review object ---> json format
    it the request is allowed it will be TRUE and if it rejected it will be FAULT
F: How do we set this up 
A:   depoly the webhook server with own logic and configure the webhook on the kubernetes by creating the webhook configuration 

-
____________________________


____________________________

Application Logs
Various loggin mechansims in Kubernetes
we create a pod with the same Docker image using the pod defination file 

F: How to loggin Docker?
    docker run kodekloud/event-simulator

F: If i run the docker container in the background in a detechmode using -d option, I would see the logs,
    docker -d kodekloud/event - simulator 

F: If i wolud like see the log in the 
      docker logs -f ecf   (-f display the live log trail)

NOTE: we create a pod with the same Docker image using the pod defination file once the pod is running, we can view the logs 
    -  kubectl create -f event-simulation.yaml (create a podd)

    -  kubectl logs -f event-sim ( see the log files)
       

If the multiple conatiners in the pod you must specify the name of the conatiner to see the log file 
    -    kubectl logs -f event-simulator-pod  event-simulator 
          - event-simulator-pod: this is the name of the pod 
          - event-simulator: this is the name of the first container in the pod 

________________________________________
LAB 

F:  To see the logs?
A:      kubectl logs webapp-1

F:   We have deployed a new POD - webapp-2 - hosting an application. Inspect it. Wait for it to start.
A:      kubectl logs webapp-2 event-simulator ( it there are two container are running on the pod then write the name of the container to see the logs)

_______________________________________________
Rolling Updates and Rollbacks in a Deployment

F:  What is rollout and versioning in Deployment?
A:    -  When u first create a deployment, It triggers a rollout
      -  A new Rollout create a new deployment revision, (revision 1)
      -  In future the container is updated to a new one, a new rollout is triggered and 
      -  new deployment revision is ceated  (_Revision 2) 

______________________________________________
LAB Rolling Update 

F:  We have deployed a simple web application. Inspect the PODs and the Services
    Wait for the application to fully deploy and view the application using the link called Webapp Portal above your terminal.
A:        kubectl get pods 
          kubectl get deploy 

F:  Run the script named curl-test.sh to send multiple requests to test the web application. Take a note of the output.
    Execute the script at /root/curl-test.sh.
A:         ./root/curl-test.sh

F: Inspect the deployment and identify the nummber of the Posds deployed by it 
A:         kubectl get deploy 

F:   What container image is used to deploy the applications?
A:         kubectl describe deploy  frontend 

F:   Inspect the deployment and identify the current strategy
A:         kubectl describe deploy frontend 

F:   If you were to update the application now what would happen?
A:         pods are upgrading few at a time
            (by default is that first pod willl be down and than upgrade it . then secon will down and then  upgrade it..

F:   Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2
     Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

A:   1 way
         kubectl describe deploy frontend (first see the container name)
         kubectl aet image deploy frontend simple-webapp=kodekloud/webapp-color:v2
      2 Way
          kubectl edit deploy frontend (and change the image Name)

F:    Run the script curl-test.sh again. Notice the requests now hit both the old and newer versions (If checked immediately). However none of them fail.
      Execute the script at /root/curl-test.sh.
A:         ./curl-test.sh
            bash /root/curl-test.sh ( that means the application is updated)

F:    Up to how many PODs can be down for upgrade at a time
      Consider the current strategy settings and number of PODs - 4

          
F:     Change the deployment strategy to Recreate
      Delete and re-create the deployment if necessary. Only update the strategy type for the existing deployment.
A:         kubectl edit deploy frontend 
          and change the thing what they need (delkete the after strategie 3 lines bis type)

F:     Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v3
       Do not delete and re-create the deployment. Only set the new image name for the existing deployment.
A:             kubectl set image deploy frontend simple-webapp=kodekloud/webapp-color:v3

F:     Run the script curl-test.sh again. Notice the failures. Wait for the new application to be ready. Notice that the requests now do not hit both the versions
      Execute the script at /root/curl-test.sh.
A:               ./curl-test.sh
_____________________________________________________________
Commands and Arguments in Dockers
BYDEFAULT it sleep s for 5 seconds

To run a docker containers 
      - docker run ubuntu( to run the instance and image immediadely)
      - docker PS (list to see the running containers)
      -  docker ps -a (list all the conatiners including stopped containers)

F: What we run the conatiners 
      - Containers have a specific task/ process 
      -  to run a host an instance of a web server 
      -   application server or a database 
      -  or simpoly to carry out some computation or analysis.
      - when it completed its wtask it exists 
Note   the Container only lives as long as the process inside it is alive,
      if the website inside the conatiner is stopped or crashes, the container exits 

F: We can also do it automattically that when th container run the sleep command will run/invoke  automaticallly 
     -  We have to save in the startup.
     -  TRNTRYPOINT is the command thet is run at startup 
     -  CMD  is the default parameter passed to the command 
     -  the entry point instruction is a command instruction thet you can specify the programm that will run when the containers starts
     -  ARGS option in the pod defination file we override the cms instruction in the docker file 
    
____________________________________________________________
  Commands and Arguments in Kubernetes 

      -  CMD  is the default parameter passed to the command 
      -  the entry point instruction is a command instruction thet you can specify the programm that will run when the containers starts
      -  ARGS option in the pod defination file we override the cms instruction in the docker file 
F: What you do if u would like to override the Entry point 
      -  under spec: --> containers: ---> name: --> Image : 
      -  command: [ "sleep2.0" ]   ----------    Entrypoint
      -  args: [ "10" ]            ----------    CMD
- You have to create apod yaml file so that u can change the Manual sleep time 
-  
______________________________________________________
        LAb 

F:  How many PODs exist on the system?
    In the current(default) namespace
A:        kubectl get pods 

F:  What is the command used to run the pod ubuntu-sleeper?
A:        kubectl describe pod ubuntu-sleeper
          cat ubuntu-sleeper (look at under container)

F:   
A:        cat ubuntu-sleeper-2.yaml
          vi  ubuntu-sleeper-2.yaml (make the changes under spec write command: [ "sleep","5000" ]
          kubectl create -f  ubuntu-sleeper-2.yaml
         kubectl describe pod ubuntu-sleeper-2

F:   Create a pod using the file named ubuntu-sleeper-3.yaml. There is something wrong with it. Try to fix it!
     Note: Only make the necessary changes. Do not modify the name.
A:         cat ubuntu-sleeper-3.yaml
           vi ubuntu-sleeper-3.yaml (make the changes "1200")
           kubectl create -f ubuntu-sleeper-3.yaml
           kubectl describe pod ubuntu-sleeper-3 (only to check wheather all the things are right )

F:  Update pod ubuntu-sleeper-3 to sleep for 2000 seconds.
    Note: Only make the necessary changes. Do not modify the name of the pod. Delete and recreate the pod if necessary.
A:         kubectl edit pod ubuntu-sleeper-3 (change the data but it gives error u copy the path)
           cat  /tmp/kubectl-edit-180931990.yaml (write the path) it make the changes
          kubectl replace --force -f /tmp/kubectl-edit-180931990.yaml (now receate the pod)

F: Inspect the file Dockerfile given at /root/webapp-color directory. What command is run at container startup?
A:         cat /root/webapp-color/Dockerfile

F:   Inspect the file Dockerfile2 given at /root/webapp-color directory. What command is run at container startup
A:          cat /root/webapp-color/Dockerfile2

F:   Inspect the two files under directory webapp-color-2. What command is run at container startup?
     Assume the image was created from the Dockerfile in this directory.
A:         ls web-color-2
             cat web-color-2/Docketfile          (First we create an image and then we use this in the yaml file)
             cat web-color-2/webapp-color-pod.yaml
             (The command will alwyas override the Entrypoint)
              Entrypoint Overwrite the ---> command
              CMD  overwrite the  args


F:  Inspect the two files under directory webapp-color-3. What command is run at container startup?
    Assume the image was created from the Dockerfile in this directory.
A:           ls web-color-3
             cat web-color-3/Docketfile          (First we create an image and then we use this in the yaml file)
             cat web-color-3/webapp-color-pod.yaml
             (The command will alwyas override the Entrypoint)

F:  Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green.
A:          kubectl run webapp-green --image=kodekloud/webapp-color --dry-run=client -o yaml
            kubectl run webapp-green  --image=kodekloud/webapp-color --  --color green
            kubectl describe pod webapp-green
_________________________________________________
ENV Variables in Kubernetes 

To set an variable Environment variable, use ENV property.
    -  env: is an Arry and every property starts - dash indicating an item in the array .
    -  each item hase a name and a value of properties 
    -   the naem is the ENV variable (avilable in a container) and value is its value 

There are three ways to setting the Environment variables 
    -  plain key Value     
          eg  env: 
                -   name: APP_COLOR
                    value: pink
    -  ConfigMap
          eg env: 
                -   name: APP_COLOR
                    valueFrom: 
                        configMapKeyRef:
    -  Secrets
          eg  env: 
                -   name: APP_COLOR
                    valueFrom: 
                        secretKeyRef:

__________________________________________
create ConfigMaps:

F: How to Configur the data in kubernetes? 
   when u have a lot of pod definations file it will be difficult to manage the environment data stored within the queries file

  In Kubernetes, a ConfigMap is an object used to store configuration data as key-value pairs.
  In Configmaps we can change the settings without rebuilding the conatiner image.
     
  There are two phases in Configmap
      - create the config map
      - inject them into pod 
  
  There are two ways to create an 
      1)  kubectl create configmap\  (Imperative)
              sitConfigname --from-literals=APP_COLOR=blue
                            --from-literals=PASSWORD=&Harekam0003
  NOTE:  It will be complicated when u have too many configuration thats why we can also use the file 
         When u want to use the file u have to specify the path 

      2)  kubectl create -f configmap.yaml  (Declarative)
            we have to create a yaml file 
                    kind: configdata 
                    metadata:
                        name: app-config
                    data: 
                        APP_COLOR: blue
                        PASSWORD: &Harekam0003

F: How to view the Config map
        kubectl get configmap

F: How to see in the detail?
        kubectl describe configmaps
F: NOw how to combine/inject pod and the configmap file?
   add new property in the pod yaml file
        envFrom:
          - configMapRef:
              name:
        
        
F: 

______________________________________
F:  What is the environment variable name set on the container in the pod?
        kubectl describe pod webapp-color
F:  What is the value set on the environment variable APP_COLOR on the container in the pod?
        kubectl describe pod webapp-color (under Environment : pink
F:  Update the environment variable on the POD to display a green background.
    Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.
A:     kubectl edit pod  webapp-color ( used to edit the pod and give the error)
       cat /tmp/kubectl-edit-1695555210.yaml (now make the changes but it did not save it)
       kubectl replace --force -f /tmp/kubectl-edit-1695555210.yaml (fÃ¼r changes we have to create the replace and delete the pod)
F:  How many config maps are there?
A:     kubectl get ms

F:  Identify the database host from the config map db-config
A:     kubectl describe cm db-config 

f:  Create a new ConfigMap for the webapp-color POD. Use the spec given below.
A:     kubectl create configmap webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard

F:   Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap.
    Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.
A:         kubectl edit pod webapp-color 

        kubectl replace --force -f /tmp/kubectl-edit-2160797237.yaml

______________________________________
            Secrets
In conig maps stores configuration data in plain text format.its ok for host name and user name but it is not good for the password 
Secrets: 
      -  are used to store the sensative information like password and keys. 
      -  It stores the data in entcoded form 
      -  First, create the secret and 
      -  second, inject it into pod 

There are two ways to create a secrets 
      1) The imperative way, Without using secret defination file.
      2) the Declarative way, by using a declarative file.

1)  The imperative way, First way Without using secret defination file.
          you can directy specify the key value pair in the commands line itself
          eg. kubectzl create secret generic  <secret name> --from-literal=<key>=<value>

Second way with using secret defination file where the path is stored 
          eg. kubectzl create secret generic  <secret name> --from-file=<path-to-file>

2) the Declarative way, by using a declarative file.
        kind: secret
        data:
            DB_HOst: mysql
            DB_User: root
            DB_PAssword: paswrd
Note in secret we have to store the in encoded form not in plain text
When we enter the data in the secret file we have to enter in the encoded form

F: How can we encode the data from plain text?
     On Linux just run the 
      echo -n 'mysql' | base64
      o/p    bXlzcWw=
      echo -n 'root' | base64
      o/p    cm9vdA==
      echo -n 'paswrd' | base64
      o/p   cGFzd3JK
 
F: how to view secrets?
      kubectl get secrets 
F: How to see/view the more information?
      kubectl describe secrets
      It shows the attributs in the secret but hides the value themselves
F: How to views the values now u can see the encoded values ?
      kubectl get secret app-secret -o yaml

NOTE now u have to create a two files one is for po-defination.yaml and second is secret-data.yaml 

NOW step two 
F: HOw to inject N ENVIRONMENT VARIABLE and anew property to the container 
1) we can inject a single environment 
2) the whole secretes as a file in a volume
spec:
  containers  
    envFrom:
      -  secretRef:
            name: there is the name under metadata from secret-data.yaml

F: If u want to mount the secret as a volume in pod Secrets in pods as Volumes
A: we have to create a file with each attribute as a secret of the contents 
        ls /opt/app-secret-volumes
        cat /opt/app-secret-volumes/DB_Password
if there are three attributes in our secret that means three files are created.

Important on Secrets:
    - Secrets are not Encrypted. Only encoded.
          -    Do not check-in secret objects to SCM along with code.
    - Secrets are not encrypted in ETCD
    - 
    
_________________
      LAB
F:  How many Secrets exist on the system?
    In the current(default) namespace.
A:       kubectl get secrets

F: How many secrets (data keys) are defined in the dashboard-token secret?
A:     kubectl describe secret default-token-cr4sr

F: What is the type of the dashboard-token secret?
A:    kubectl describe secret default-token-cr4sr

F: Which of the following is not a secret data defined in dashboard-token secret?
A:      kubectl describe secret default-token-cr4sr

F:  The reason the application is failed is because we have not created the secrets yet. Create a new secret named db-secret with the data given below.
    You may follow any one of the methods discussed in lecture to create the secret.
A:        kubectl create secret generic db-secret  --from-literal=DB_Host=sql01 --from-literal=DB_User=root 
          --from-literal=DB_Password=password123



_______________________________________________


Multi container Pods 
    means to run more than one conatianers in a pod simuntencely 

MULTICONTAINER AND ITS  DESIGN PATTERN 
There are different patteren for Multi-container pods 
    1)  Co-located Containers 
    2)  Init Containers
    3)  Sidecar Container 

  1) Co-located Containers: 
        -  these are used, when two services are depnded on each other.
        -  This is the original form of mullticontainers means that two Containers are running in a pod.
        -  the conatiners are running till the pod lifecycle. theses are used  when two services are depended to each other 
        -  
   2)  Init Containers:
        -   are used when the initilization steps to be performed when a pod starts before the mian application itself.
        -   Init Conatiners starts and ends its job and then the main application starts  

   3)  Sidecar Container: 
        -  A small helper container that runs alongside the main application inside the same pod
        -  It is set up like the init Container 
        -  The side car starts first, 
        -  does its job,
        -  instead of ending, It continues to run throughtout the lifecycle of the pod and ends after the main app ends.
        -  The main application starts after the sidecar starts.
        -  Both containers start, stop, and share the same network and storage, working closely together.
NOTE: Sidecar = helper container running alongside the main app, like a log shipper.

The First and the last containes are same what is the difference between them.
      -  
_____________________________________________________
                                  INIT LAB

F: Identify the pod that has an initContainer configured
A:       kubectl  describe pods

F: What is the image used by the initContainer on the blue pod?
A:    kubectl  describe pod 
      kubectl describe pod green
      kubectl describe pods blue  | grep Image

F:  What is the state of the initContainer on pod blue?
      kubectl describe pod  blue

F:   Why is the initContainer terminated? What is the reason?
       kubectl describe pod blue 

F:   We just created a new pod named purple. How many initContainers does it have?
        kubectl describe  pod purple 

F:   What is the status of the purple POD?
        kubectl describe pod purple 

F:  Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds
    Delete and re-create the pod if necessary. But make sure no other configurations change.
A:      kubectl esit pod red 
        kubectl replace --force -f  /tmp/kubectl-edit-847706811.yaml

F:  Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds
    Delete and re-create the pod if necessary. But make sure no other configurations change.
A:      kubectl get pod orange 
        kubectl describe pod orange 
        kubectl logs orange -c  init-myservice
Explain it: kubectl â†’ The Kubernetes command-line tool.
        logs â†’ The command to view output (stdout/stderr) from a container.
        orange â†’ The name of the pod you want to inspect.
        -c init-myservice â†’ Specifies which container inside the pod you want logs from. This is necessary because pods can have multiple containers (including init containers).
        In this case, init-myservice sounds like an init container, which runs before the main app containers in the pod, often to set up configs, load data, or check dependencies.
        If you donâ€™t include -c ..., kubectl will default to the first container in the podâ€”but for init containers, you must specify them explicitly.

  ______________________________________________________________
Auto Scaling in CKA
 
Scaling means: (thji si sthe example of the of version when the have our own data center or servers)
    When we used the physical server with a predefined  CPU memory capacity and what happend we load increases.
    First we need to scale up the server 
    so we took down the application and added more  CPU or more memeory resources to it and then powered it back up.

Vertical scaling :
    When we added more resources(more cpu and memory resources  etc... ) on our exisiting application is known as vertical scalling
Horizontal scaling:
    running more instances or more server to your server is known as horizontal scaling.


________________________________________________
There are two types of Scaling in Kuberenets:
      1) Scaling Workloads: you can scale the work loads by adding and removing containers or pods onto the cluster
      2) scaling cluster Infa:that adding nad removing more servers ot Infrastructure to our cluster

    1) Scaling Workloads: you can scale the work loads by adding and removing containers or pods onto the cluster
          There are two types of scaling 
            - Horizontal scaling: means adding more pods
            - Verticaling: increasing the resources allocated to existing pods          

    2) scaling cluster Infa:that adding nad removing more servers ot Infrastructure to our cluster.
            There rae two types of scaling 
            -  Horizontal scaling: the adding more nodes to the cluster.
            -  vertical scaling: increasing the resources on exiciting nodes in the cluster

 There are two ways of scaling (How we do the scaling)
      1) Manual way for Scaling Cluster Ifra;
            Manual approach of horizontally scalling cluster infra would be to manually provision new nodes.(Vartically scaliing is not be prefered)
            eg.  kubeadm join command    (to add the new nodes in the cluster.
      1) Manual way of Scaling Workloads
             Also two ways of Scaling(HOw we do the scaling)
            - Horizontal scaling eg. 
                kubectl scale command is used in the workload to scale up and down the nummber of Pods.
            - Vertical scaling: eg. 
                Kubectl edit command to go into that deployment or set the replica set and change the limits aand resources on the pods 
      
2) Automated way:
    2) Cluster Autoscaler:

    2) Horizontal Pod Autoscaler (HPA)
        
    2) Vertical Pod Autoscaler(VPA)



 ______________________________________________________________
Scaling a workload the Manual way

NOTE: In manual way we have to sit on my computer and continuously monitor resources usage and if there is a sudden traffic spike and want to break or add something
      i m not able to react fast thats why we used (HPA)  HORIZONTAL POD AUTOSSCALER
        kubectl top pod my-app-pod command:
                is used to see the resources consumption of the pod, if u have to do it maually.NOTE( you must have the metrics server running on the cluster)
        kubectl scale deployment my-app --replicas=3 : 
                is used to add  the additional pods, so this is the manual way to scale a worklooad(nun i have to sit on my computer and continuously monitor resources usage.

Automatacally run the HPA
1)   Horizontal pod Autoscaler(HPA):
          -  Observe metrics: HPA continuousaly monitors the matrics as we did manually with the top command.
          -  Add Pods: It can Automatacally ioncreases and decreases the pod in a deployment statefull set or replica set based on the CPU memory.
          -  Balances tresholds: If the memory or memory usage goes oo high, HPA creates more pods to handel that and if no needs it removes the extra pods 
          -  Tracks multiple metrics: this balance the thresholds and it can also track the multiple different types of metrics which we refer to in a few minutes.
      
Imperative way
          eg Kubectl autoscale deployment amy-app \ --cpu-percent-50 --min=1 --max=10
                It creates a horizontal Pod Autoscaler  for this deployment first reads the limits on the pod  500 millicore, second it can use the monitor mmeory bis zu 50%
                thirs, It can modify the numbers of the replica min1 and max 10
          eg. Kubectl get hpa
                to view/list the status of the current hpa.
          eg kubectl delete hpa my-app
                to delete the hpa

Declarative way:( we can create an hpa With the API version:)
        apiVersion :autoscaling/V2
        kind: HorizontalPosAutoscaler

Metrics server :
      A Metrics Server in Kubernetes is a lightweight service that collects resource usage data (like CPU and memory) from nodes and pods.
      It provides this data through the Kubernetes API, so tools like the Horizontal Pod Autoscaler (HPA) or the kubectl top command can use it.

ðŸ‘‰ In short:
Metrics Server = resource usage collector for scaling and monitoring in Kubernetes.
____________________________________________________________
        LAB Manual Scaling 

F:  Create a Deployment
    Using the /root/deployment.yml manifest file provided , create a Kubernetes deployment for the Flask application.
    Discovery
        Use kubectl get deployments to observe the deployment status.
        Use kubectl get pods to see the running pods.
A: We use the apply command, if the the deployment is already written in yaml file it just appy on the cluster,
    if it is not written in tha yaml nfile it will create a deployment and applay on the cluster 
        kubectl apply -f /root/deployment.yml
        kubectl get pods 
        kubectl get deployments 

F:  What is the primary purpose of the kubectl scale command?
A:   to scale the number of replica and deployments 

F:  Can the kubectl scale command be used to scale down a statefulset in Kubernetes?
A:      The kubectl scale command can be used to scale both deployments and statefulsets.
        When scaling a statefulset, Kubernetes ensures that the state and order of the pods are maintained, 
        unlike in deployments where pods can be created and destroyed in any order.

F:   How to see that how many replicas are created?
A:       kubectl get deployments 

F:   If you scale a deployment using kubectl scale to a higher number of replicas, 
     but the cluster has insufficient resources to accommodate all new replicas, what will happen?
A:      When you scale a deployment to a higher number of replicas than the cluster can support due to resource constraints,
        Kubernetes will create as many replicas as possible within the available resources.
        The remaining replicas will be in a pending state until sufficient resources are freed up or added to the cluster. 
        This behavior allows Kubernetes to manage resources dynamically while maintaining the desired state as closely as possible.



F:   Create a Deployment
     Using the /root/deployment.yml manifest file provided , create a Kubernetes deployment for the nginx application.
     Click on Skooner to access the monitoring tool and view the resources in the Kubernetes cluster.
     Token for the Skooner can be found in /root/skooner-sa-token.txt
     Is the nginx deployment running?
A:       To deploy nginx application, run the below command.
          kubectl apply -f /root/deployment.yml

        To view the deployment and pods creation, use the below commands.
          kubectl get deploy or kubectl get pods


f: We have a manifest file to create autoscaling for the Nginx deployment located at /root/autoscale.yml. Review the manifest file and identify the current replicas and desired replicas?

A. Current replicas= 7
    Desired replicas= 3

B. Current replicas= 3
    Desired replicas= 7

C. Current replicas= 7
    Desired replicas= 1

D. Current replicas= 0
    Desired replicas= 0

A: kubectl get hpa  (D)

F: Create an autoscaler for the nginx-deployment with a maximum of 3 replicas and a CPU utilization target of 80%.
A:   kubectl autoscale deployment nginx-deployment --max=3 --cpu-percent=80
            OR
      kubectl apply -f /root/autoscale.yml

F:  What is the primary purpose of the Horizontal Pod Autoscaler (HPA) in Kubernetes?


F:  What component in a Kubernetes cluster is responsible for providing metrics to the HPA?
A:     metrics server

F:  What is the current replica count of nginx-deployment after deploying the autoscaler?
A:       kubectl get deploy nginx-deployment

F:   What is the status of HPA target?
A:       kubectl get hpa

F:   The HPA status shows /80 for the CPU target. what could be a possible reason?
A:       If the status of the Horizontal Pod Autoscaler (HPA) target is <UNKNOWN>/80,
         it typically means that the HPA is unable to retrieve the current metrics for the specified target. 
         Run kubectl describe hpa nginx-deployment 
          to find more details.

F:  Since the HPA was failing due to the resource field missing in the nginx-deployment,
    the resource field has been updated in /root/deployment.yml. Update the nginx-deployment using this manifest.
    Watch the changes made to the nginx-deployment by the HPA after upgrading by using the kubectl get hpa --watch command.
A:      To update the nginx-deployment, run the below command:
              kubectl apply -f /root/deployment.yml
        To watch the changes made to the nginx-deployment by the HPA, use the below command:
              kubectl get hpa --watch

F:   What does the event ScalingReplicaSet in the nginx-deployment HPA indicate?
A:       To find out what the event ScalingReplicaSet means in the HPA, run the command below:
            kubectl events hpa nginx-deployment | grep -i "ScalingReplicaSet"
              HPA increasing tghe numnber of pods

F:   What is the cause of the FailedGetResourceMetric event in the nginx-deployment HPA?
A:          To find out what the event FailedGetResourceMetric means in the HPA, run the command below:
            kubectl events hpa nginx-deployment | grep -i "FailedGetResourceMetric"


___________________________________________________________________
      Resize the pods 

__________________________________________________________________
VPA (Vertical pod Autoscaler)
for vertically scalling always use EDIT command 
      -  it Monitors the metrics server 
      -  it automatically increases and decreases the resources assigned to the pod in a deployment 
      -  Balance the workload

We also see the three multiple Components deployed:
      1)   Admission controller:
            -  the Admission Controller intervenes the pod Creation process and uses the recommendation from the Recommender again.
            -  To then mutate the pod spec to apply the recommended CPU and memory values at startup.
            -  And this ensure that the newly created pod starts with the correct resources requests.

  
      2)   VPA Recommender: (Collect information ) 
            -  is responsible for continuously monitoring resources usage and collect history 
               and live usage data for pods and provides recomendation on optimal CPU and Memory values.
           -  It does not modify the Pod directly.
           -  It only suggest changes.


      3)  Updater Serivice:(Monitors  or get the information from the Recomender compares to the actual pods and if the pod beyond the threshhold it kill the pod (it depends on the polocy)  )
          - Detect pods that are running with sub optimal resources and
          -  Evicts them when an update is needed.
          -  It gets the information from the Recommender and monitors the pod.
          -  If the pod needs to be updated, It evicts(means terminates the pod) them

There are 4 modes 
The update modes are the most important part of the VerticalPodAutoscaler CRD because they determine how and when VPA applies its CPU/Memory recommendations.
    
        1. Off  (Only recommends. Does not change anything)
              VPA only generates recommendations but does not update pod resources.
              Use case:
                  -  You want to observe VPAâ€™s suggestions before trusting it to make changes.
                  -  Good for testing or capacity planning.
    
        
        2. Initial :(Only changes on pod creation. Not later)
              -  VPA applies recommendations only at Pod creation time.
              -  Running pods are not updated.
              -  If a Pod is restarted for another reason (e.g., rollout, crash), it will get the updated recommended resources.
              Use case:
                -  You want to avoid disruptions to live workloads.
                -  Workloads that can tolerate static resource requests but should benefit from VPA tuning over time.


        3. Recreate  :(Evicts pods if usage goes beyond range)
              -  Like Auto, but more aggressive: VPA will always delete and recreate pods when updating resources (instead of relying on live-updates or waiting).
              -  Ensures Pods are restarted with the new resources immediately.
              Use case:
                  -  Workloads that cannot tolerate in-place updates but must always run with the latest recommendations.
                  -  Batch jobs or stateless apps where restarts are cheap.

        4. Auto
              -  VPA actively updates running Pods with new recommendations.
              -  If recommendations change, VPA may evict pods to restart them with new CPU/memory requests.
              -  Evictions respect Pod disruption budgets (PDBs).
              Use case:
                  -  Production workloads where continuous optimization is needed.
                  -  Youâ€™re okay with occasional evictions.
        Features  of VPA HPA (HOPw to choose should i increase a VPA or HPA)

âš–ï¸ Choosing the Right Mode
        
        Off      â†’ Safe start, observe recommendations only.
        Initial  â†’ Low-disruption, â€œset onceâ€ policy, great for stable apps.
        Auto     â†’ Dynamic optimization, good for microservices or fluctuating workloads.
        Recreate â†’ Strong consistency, but higher disruption; good for short-lived or stateless apps.

___________________________________________
      LAB How to install VPA
What is VPA CRDs â€” these are part of Kubernetes Vertical Pod Autoscaler (VPA).
SEE Lab 135/136

___________________________________________

F:   Let us explore the environment first. How many nodes do you see in the cluster?
     Including the controlplane and worker nodes.
A:      kubectl get nodes 

F:   How many applications/deployments do you see hosted on the cluster?
     Check the number of deployments in the default namespace.
A         kubectl get deployments 

F:    Which nodes are the applications hosted on?
A:        kubectl get pods -o wide
Explain it in Detail
is a Kubernetes CLI (kubectl) command that lists all the running Pods in the current namespace, but with extra details compared to the default kubectl get pods.
Breakdown:
kubectl â†’ Command-line tool for interacting with the Kubernetes cluster.
get pods â†’ Lists all the pods in the current namespace.
-o wide â†’ Expands the output to show additional information beyond the default columns.

F:   We need to take node01 out for maintenance.
     Empty the node of all applications and mark it unschedulable.
A:       kubectl drain node01
            - This command gives error because some pods DaemonSet (networking proxy, monotring) 
              find it all the pods aumatically thats why u are not able to delete the pods
              you have ignor these DaemonSet pods
          kubectl drain node01 --ignore-daemonsets

F:   What nodes are the apps on now?
A:       kubectl get pods -o wide

F:   The maintenance tasks have been completed. Configure the node node01 to be schedulable again.
A:      kubect uncordon nodes01

F:   How many pods are on node01?
A:       kubectl get pods 

F:   Why are there no pods on node01?
A:       Running the uncordon command on a node will not automatically schedule pods on the node.
         When new pods are created, they will be placed on node01.

F:   Why are the pods placed on the controlplane node?
     Check the controlplane node details.
A:        k describe node controlplane 
                  OR
          kubectl describe node controlplane | grep -i  taint

F:  
A:        kubectl drain node01 --ignore-daemonsets

F:  Why did the drain command fail on node01? It worked the first time! 
A:        k get pods 

F: What is the name of the POD hosted on node01 that is not part of a replicaset?
A:       k get pods -o wide 

F: 
A:       kubectl drain node01 --ignore-daemonset --force

F:    hr-app is a critical app and we do not want it to be removed and we do not want to schedule any more pods on node01.
      Mark node01 as unschedulable so that no new pods are scheduled on this node.
      Make sure that hr-app is not affected.
explain I donot wnat that any pod will be added on the node01
A:        Kubectl get nodes
          kubectl cordon node01
          kubectl get pods -0 wide
_______________________________________________
1)  kubectl cordon <node>
    -  Marks the node as unschedulable.
    -  No new Pods will be scheduled on it.
    -  Existing Pods keep running.
ðŸ‘‰ Think: â€œStop putting new Pods here, but donâ€™t touch the old ones.â€

2)  kubectl drain <node>
      -  First cordons the node.
      -  Then evicts (removes) all workload Pods (except DaemonSets & static Pods).
      -  The evicted Pods get rescheduled on other healthy nodes.
ðŸ‘‰ Think: â€œEmpty this node completely so I can maintain or remove it.â€

3)   kubectl uncordon <node>
        -  Marks the node as schedulable again.
        -  New Pods can now be placed on it.
ðŸ‘‰ Think: â€œBring this node back to the pool.â€

In short:
    cordon = stop new Pods
    drain = stop new Pods + move old Pods away
    uncordon = allow new Pods again
____________________________________________
Kubernetes manages software Releases

What we know about the API versions in kubernetes 
When we install the a kubernetes cluster with specific version of kubernetes 
We can see that when we run the command
    kubectl get nodes 

        Version is v1.33.0 in three parts
V1 :   first is the major version followed by the minor version and 
33 :   Minor version  (realease every few months with new features and functionalities)
0  :   patch version   (realease more often  with critical bugs fixes)

The download package, when extracted, has all the control plane componenets are on the same version 
eg : 
    kube-APIserver      :  v1-xx.y
    controller-manager  :  v1-xx.y
    kube-scheduler      :  v1-xx.y
    kubelet             :  v1-xx.y
    kube-proxy          :  v1-xx.y
    kubectl             :  v1-xx.y

There are two componenets that do not have the same version 
    etcd Cluster        :  V3.2.18
    coreDNS             :   V1.1.3

________________________________________________

Cluste upgrade process

F:  Is is complusary for all of these to have the same version 
A:  - NO the componenets can be at different release version.
    - the kubeapi is the primary component in the control plane and that is the component that lal other components talk to 
      none of the other components should ever be at a version higher than the kube-apiserver. X version 
    - The conroller-manager and scheduler can be at one version lower  X-1
    - kubelet  and  kube-proxy  can be 2 version lower    X-2 
    - kubectl  has a one version higher than the API server
Note: so it allow us to do the live upgade and  We can upgrade componenets by componenet if required


F: When should u upgrade?
A:   At any time the kubernetes supports only upto the recent three minor versions
eg     first release  : v1.10
       senond release : v1.11
       third relase   : v1.12
       fourth realse  : v1.13
now the first realease is not be suported

NOTE: BEFORE THE RELEASE OF V1.13 WOULD BE A GOOD TIME TO UPGRADE YOUR CLUSTER TO THE NEXT RELEASE

F: How we upgrade should we upgrade v1.10 to direkt v1.13
A: No. the best way to upgrade is one be one v1.10 to v1.11 to v1.12 denn v1.13... and sooo on 

You can deploy ur cluster in two ways with the help of kubeadm  and cluster from strach

    1) if u deploy ur cluster using tool like kubeadm then the tool can help you plan and upgrade the cluster
    2) u can deploy ur cluster from scratch then u manually ugrade the different components of the cluster yourself

1) we have cluster with master and worker Node running in production, hosting nodes, services users.
    the nodes and components are at version 1.10

________________________________
F: Upgrading a cluster involves two major steps.
    - First upgade your master node 
    -. Second upgrade ur worker Nodes
UPGRADE MASTER NODE    
1)  While the  master node is upgrading the control plane components, such as the API server, scheduler and controll-managers go down briefely.
        -  when the master cluster going down does not mean your worker nodes and application on the cluster are impacted.
        -  All workloads hosted on the worker Node continue to serve users as noemal.
        -  since the master is down, all management functions are down.
        -  you cannot access the cluster using kubctl or others kubctl-API
        -  you cannot deploy new applications or delete or modify existing ones.
        - If any pod will be fail a new pod won#t be automatically created.
        -   But as long as the nodes and pods  are up, your application should be up and users will not be impacted.
        -   Once the upgrade is complete and the cluster is back up, it fuctions normally and the master Node version is v1.11 and
        -   worker nodes at version 1.10


UPGRADE THE WORKER NODES
    There are Three types of strategie 
        1) downtime strategy  (Upgrade  all of them at once)
             - then your pods are down, and users are no longer able to access the applications
             - Once the upgrade is complete the nodes are back up, new pods are scheduled and user can resume access.
        2) upgrade one node at a time:
             -   we upgrade the first node, where the workersloads move to the second and thirs node and the user are serves from there 
             -   Once the first nodes is upgraded and backup.
             -   we update the second node, where the workloads moves to the first and the third nodes and so.......

        3) 3 Strategy would be able to new nodes with newer software version to the cluster.
             -  This is convienent when we are in Cloud environment where u easily add the new nodes and decommission/deleted old one.
             -  Move the workload to new node and remove the old node


  Kubeadm---- upgarde
    kubeadm has an upgrade command that helps the cluster to upgrade.
F: Whih command gives u a lot of information about the cluster version and list all the components and its version 
   and you must manually upgrade the kubelet version on each node.
Ã‚:      kubeadm upgrade plan 

F: HOw to do this upgrade 
    - upgrade the kubeadm tool itself to version 1.12
          apt-get upgrade -y kubeadm=1.12.0-00
    -  then upgrade the cluster componenets using the command
          kubeadm upgrade apply v1.12.0
    -  it display the version of the API server not the version of API itself it display the old version
          kubectl get nodes
    - upgrade the kubeletes on the master node
NOTE : depending on ur setup u may or u may not have kubeletes running on your master node
          apt-get upgrade -y kubelet=1.12.0-00
    - once the package is updated then restart the kubelet
          systemctl restart kubelet
    - it display that the master nodes has been upgraded 10 1.12
          kubectl get nodes
_____________-_____
    - Now upgrade the worker Nodes. 
    - First moves the workloads from the first worker node to other nodes
    - this commandsafely terminated all the pods from a node and reschedules them on the other nodes.
    - it also cordons the nodes and marks it unschedulable(no new pods are scheduled/add on it)
          kubectl drain node-1
    -  upgrade the kubeadm and kubelet packages on the worker node as we did on the master node
          apt-get upgrade -y kubeadm=1.12.0-00
    - upgrade the kubdeadm tool
          apt-get upgrade -y kubelet = 1.12.0-00
    -  upgrade the node configuration for the new kubelet version
          kubeadm upgrade node config --kubelet-version v1.12.0
    - restart the kubelet service 
          systemctl restart kubelet
NOTE: now the node is up with the new software version 
NOTE: When we drain the nodes means we mark it as a unscheduleable  node. 
NOTE: so we need to unmark it by running the command
NOTE: it is not necessary that the pods come back to this nodes.It is only marked as scheduled
NOTE: only when the pods are delete from the other node or when new pods are scheduled do they really come back to this first node
            kubectl uncoden node-1





