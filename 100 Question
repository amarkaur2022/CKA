Allegemein Information:

Network Policy      : 12   	(Lab )
deployment          : 1,8,32  	(Lab 35)
PODS          	    : 2,7,9,14,25,26   (Lab 27)
Static pods 	    : 2     	(Lab 78)
Namespace     	    : 6  	(LAab 43)
Replica	      	    : 4,    	(Lab 31)
Cluster             : 3,15  	(Lab 96)
Cluster,role,binding: 3,11,15,47  (Lab 175/179)
cluster upgrade	    :	 	(Lab 146)
Ingress             : 22,	(Lab 249)
Egress 		    : 45,	(Lab
Labels              : 5, 23     (Lab 60)
images              : 1, 24
images security	    :        	(Lab 186)
Scheduled           : 271,   (Lab 57)
multipleSchduler    : 		(Lab 83)
pv                  : 18,28,	(Lab 209)
non-PV		    : 37        (lab 209)
Storageclasses			(Lab 214)
taints & tolaration : 10 29,31  (Lab 63)
logs                : 21	(Lab 99)
Nodes               : 13,20     (Lab 67)
Memory (pods)       : 19
secrets             : 		(Lab 116)
secrestes & Mount pods: 1
service             : 16
service role & rolebinding cluster:11
Network policy      :12,	(Lab 193)
Daemonset           :      	(Lab 75)
Rolling update&rollback	:   	(Lab 104)
Env Variables       :        	(Lab 113)
VPA		    :         	(Lab 135)
TLS certification   :		(Lab 164)
API certification   :           (Lab 167)
configmap 	    : 		(LAb 113/170)
Corden and drain a node  :  39  (Lab 140)
Troubleshotting(pod):

difficult 	     :	35	(Lab113)
Backup ETCD          : 30        (LAB
CronJob 		     : 		(LAb 51)














Basic commands:
---

--dry-run=client  <-- this will not create the resource, instead, it will tell us weather the command is correct or not

--dry-run=client -o yaml  <-- it will check if the command is correct and generate a .yaml file also

> kubectl replace --force -f abc.yaml         <-- this command will delete the existing pod and create a new with this file
Â                                              normally we need to write first "delete" then "apply" but this command do both
Â                                              of the things in one time.



> kubectl explain pods   or   replicase  or deployment
> kubectl describe pods
> kubectl create pods --help
> kubectl create secret generic --help
> kubectl explain secret 
> kubectl run --help
> k taint --help 

> kubectl replace --force -f abc.pod          <-- like this it will forcefully delete and recreate the pod

> kubectl replace --force -f /tmp/kubectl-edit-3434343.yaml     <-- like this we can forcefully delete and recreate the pod

################################################################

Scale:
-	The kubectl scale command is used to change
Â 	the number of replicas (pods) in a
Â 	deployment, replica set, or stateful set.
-	To increase or decrease the number of running
Â 	instances (replicas) of your application.

> kubectl scale deployment my-deployment --replicas= 5
> kubectl scale replicaset my-replicaset --replicas=3
> kubectl scale statefule my-stateful --replicas=4

> kubectl edit pod redis      (edits a resources live in cluster)
Â   - Directly opens the resource in an editor (like vi or nano) so you can change it live.
Â   - It fetches the current YAML definition from the cluster.
Â   - Opens it in your editor (vi by default).
Â   - When you save and exit, it automatically applies the changes.

> kubectl apply -f redis.yaml   (create Or Update)applies file-based config to cluster)
Â   - Apply a configuration from a YAML file to the cluster.
Â   - Reads the resource definition from a file (e.g. redis.yaml).
Â   - If the resource doesnâ€™t exist â†’ creates it.
Â   - If it exists â†’ updates only the changed fields.

> vi redis.yaml (it edit the file locally)
Â   -  vi is not a kubectl command â€” itâ€™s just a Linux text editor.
Â   -  You can use it to manually open a file and edit YAML before applying.
Â   -  make changes
Â   -  save and exit
Â   -  then run

> kubectl create -f redis.yaml  (if pod doesnot exists it created error if exists)
Â   - It creates a new resource in the Kubernetes cluster from a YAML file or command.
Â   - If the resource doesnâ€™t exist, it is created.
Â   - If it already exists, it will throw an error (it will not update or modify it).

> kubectl replace -f redis.yaml  (if pod exists it replaced if missing gives error)
Â   - If the resource exists â†’ itâ€™s deleted and recreated (with a new definition).
Â   - If the resource doesnâ€™t exist â†’ it throws an error.


################################################################
---
Important:
When a pod is created how to create yaml file to make the changes
---

> kubectl run redis --image=redis123 --dry-run=client -o yaml > redis.yaml
Â 		it create a new pod and with given image only for client not live in the cluster

> kubectl appy -f redis.yaml
Â 		it  create a pod on the cluster

if the pod already exists
---
> kubectl get pod mypod -o yaml > mypod.yaml




> kubectl get pods 
> kubectl get pod mypod-name -o yaml > mypod-name.yaml        <-- create a yaml file and 
if the pod does not exists


Pods :
---
> kubectl  run  mypod --image nginx -n my-namespace         <-- it will directly create a pod in "my-namespace"
> kubectl run mypod --image redis:alpine --labels="tier=db"
> kubectl run mypod --image redis:alpine --labels env=dec,ab=bc
> kubectl run my-pod --image nginx --port=8080
> kubectl run my-pod --image httpd:alpine --port 80 --expose true    <-- it create a "pod" and also a "Service" same time
> kubectl run my-pod --image busy-box --command sleep 3200 --dry-run=client -oyaml      
> k run mypod --image=ubuntu:18.4 --command -- sleep 1000            
> k run mypod --image=ubuntu:18.4 --dry-run=client --command -- sleep 1000              


> kubectl get pods 
> kubectl get pods --no-header | wc
> kubectl get pods --no-header | wc-1

> kubectl describe pods newpods-b5znk | grep Image      <-- it display only one line Image
> kubectl get pods -o wide				<-- 

> kubectl get pods -o yaml | grep image
> kubectl get pods -o wide              <-- it also shows that on which node this pod is placed
> kubectl get pods redis -o yaml | grep image    <-- it dsplay an image of the single pod


> kubectl run redis --image=redis123 --dry-run=client  -o yaml   
Â 		<-- it just create a file in yaml format only client side and display on 								    						     the screen

> kubectl run redis --image=redis123 --dry-run=client  -o yaml > redis.yaml   
Â 		<-- save the output in a redis.yaml file but only client side

> kubectl apply -f redis.yaml
Â 		<-- After that u have to create a yaml file o that the changes are save
Â 		    on the clust so it will be saved on the cluster

> cat redis.yaml
Â 		<-- display the Inhalt on the screen



> kubectl get pod -n my-ns --show-labels         <-- it will show the pods with their labels also
> kubectl describe pods 
> kubectl get pods --all-namespaces
> kubectl get pods -A
> kubectl get pods --selector app-labele=myvlaue          <-- like this we can search for pods that has a lable "app-labele=myvlaue"
> kubectl get pods --selector env=dev | wc -l             <-- the return the number of lines in the result, their is a HEADER in the result also
Â                                                              this is the reason we have to do -1 from the resutle, if result is 8, then 8-1= 7 is answer
> kubectl get pods --selector env=dev --no-headers | wc -l     <-- this will return a result with a number, in the result no HEADER is included 
Â                                                                  so as the last obove command we can directly get 7 as result.

> kubectl get all --selector env=prod --no-headers | wc -l   <-- return all the objects that has lable "env=prod"

> kubectl get all --selector  env=prod,tier=db,app=p-app 

> kubectl get pods --watch

> kubect set image   my-deployment my-container=nginx:1.9.1     <--like this we can set a new "image" in the Deployment 

> k api-resources                           <-- get all the resources 

> k api-resources --namespaced=true         <-- it will return a list of all the resources that we can create inside a "Namespace"

> k api-resources --namespaced=false        <-- it will return a list of all the resources that we can not create inside a "Namespace"
Â                                              those must be cluster scope

################################################################

Q: Wy we need the image in the deployments and why we update it?
-	images are where all the application(Software) code + libraries are packed
-	every where the software is installed have the same environments.
-	make it easier to rollout and rollback fast and reliable

Q: why we update the images in deployments?
-	to add new Features
-	fix the bugs or security issues.

Q: What is bugs or security issues?
-	bug is a when app(software) has a mistake or error like the button is not working  properly, or app crashing.
-	Security issues are when the app has  a weakness that hacker could steal the data.

Bug fix = making the app work correctly
Security fix = making the app safe
---

Q1. Update the Nginx image of the deployment NGINX to 1.19.8
Â 	kubectl get deployments -o wide        <-- to get the more information for the deployment

Â 	kubectl set image deployment/<DEPLOYMENT_NAME> <CONTAINER_NAME>=<NEW_IMAGE>:<TAG>		<-- syntax to update the deployments images
Â 	kubectl set image deployment frontend-deployment busybox-container=busybox888.1

Â 	kubectl run pro-redis â†’ creates a Pod named pro-redis
Â 		--image=redis:alpine â†’ uses the Redis (Alpine) Docker image
Â 		--dry-run=client â†’ doesnâ€™t actually create the Pod, just simulates it (no changes in the cluster)
Â 		-o yaml â†’ outputs the Pod definition in YAML format
Â 		> redis.yaml â†’ saves that YAML output into a file named redis.yaml						<-- to update the image of deployments

Â 	kubectl describe deployment nginx | grep Image							<-- to display only the Image of the deployments

Â 	kubectl rollout undo deployment nginx								<-- it go back to the old version

################################################################
Declaration
Q: What  are the static pods
Â 	Normal Pod:
Â 	Kubernetes API server à¤•à¥‡ à¤œà¤°à¤¿à¤ à¤šà¤²à¤¤à¤¾ à¤¹à¥ˆà¥¤
Â 	à¤†à¤ª kubectl apply à¤¯à¤¾ Deployment à¤¸à¥‡ à¤¬à¤¨à¤¾à¤¤à¥‡ à¤¹à¥‹à¥¤
Â 	Controller (Deployment, ReplicaSet) à¤‡à¤¸à¥‡ manage à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤
Â 	Static Pod:
Â 	Kubelet node à¤ªà¤° à¤¸à¥€à¤§à¥‡ YAML à¤«à¤¼à¤¾à¤‡à¤² à¤¦à¥‡à¤–à¤•à¤° à¤šà¤²à¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤
Â 	API server à¤¸à¥‡ à¤¨à¤¹à¥€à¤‚ à¤¬à¤¨à¤¤à¤¾à¥¤
Â 	Controller à¤‡à¤¸à¥‡ manage à¤¨à¤¹à¥€à¤‚ à¤•à¤°à¤¤à¤¾à¥¤
Â 	à¤…à¤—à¤° Pod crash à¤¹à¥‹ à¤—à¤¯à¤¾, kubelet à¤‡à¤¸à¥‡ automatically restart à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤
Â 	Mostly control-plane components à¤œà¥ˆà¤¸à¥‡ kube-apiserver, kube-scheduler à¤®à¥‡à¤‚ use à¤¹à¥‹à¤¤à¤¾ à¤¹à¥ˆà¥¤

What happend when u donot have the scheduler etcd and API server and also no Master NOde
How u will create the pods wothout Ai server .
NOw u can configure the Kubelet to read the pods defination files fron th directory /etc/kubernetes/manifests

______________________________
Static pods

F: Why we need to change the static pod Path
-	By default, the kubelet watches the folder /etc/kubernetes/manifests/.
-	Any YAML file placed here will automatically be started as a Static Pod by the kubelet.
Reasons:

1) Multiple clusters on the same node
Â 	If you are running two control-plane clusters on the same machine, you need separate static pod directories.
2) Cluster upgrade or migration
Â 	During upgrade/restore, you may want to place static pod manifests in a temporary or new path.

F: How to find the static pods in the pods list?
Â 
- 	Static pods are created and managed directly by the kubelet on each node
-	and Kubernetes appends the node name to the pod name.

F: Commands to find the static pods
Â 	kubectkl get pods -A --all-namespace    <-- see the sufix when the pod name is end with comtrolplane theses are the static pods
Â 	ls /etc/kubernetes/manifests/           <-- display the list of the static pods
---
Frage:
Q2. Change the static pod  path to /etc/kubernetes/manifests
-	first important to know----HOw to find the config file
-	it is not possible to delete the pod like that, after delte it will  reecreate it
-	we need to find the manifest file and then delete it
Â 
>   ps -aux | grep kubelet         
Â 		<-- It display to encrupt the kubelet ((((ps -aux: ps -aux â†’ shows all running processes on the system.))))
Â 		<--ps -aux shows all running processes on the system.
Â 		ps â†’ shows processes
Â 		a â†’ all usersâ€™ processes
Â 		u â†’ show user and detailed info
Â 		x â†’ include processes without a terminal

>   nano /var/lib/kubelet/config.yaml    <-- nano is a text editor used in Linux to open/ recreate the file


################################################################

Q4: Create a deployment with 2 replicas

Â 	kubectl create deployment httpd-deploy --image=httpd:2.4-alpine    <-- to create the deployment with image name

Â 	kubectl scale deployment httpd-deploy --replicas=2
Â 					  <-- We use kubectl scale to increase or decrease the number of Pods (replicas) in a Deployment, ReplicaSet,
Â         	      				or StatefulSet.
Â 	          				It changes the number of Pods in the Deployment httpd-deploy to 2

Â 	kubectl describe deployment httpd-deploy                          <-- to see the detail

Â 	kubectl get deploy -o wide
Â 		  							  <-- to display the more information in one line

Â 	kubectl get deployment httpd-deploy -o yaml  | grep image:
Â 							             	  <-- to display only the images
################################################################Declaration:

Labels with pod
labels : 	We use labels in Pod definitions to identify and group Pods.
Â 		To select Pods easily (using selectors).
Â 		To connect Pods with Services, Deployments, etc.
Â 		For organization and management (e.g. by app, env, version).

Frage:
Q5 Create a pod with label tier=redis

Â 	kubectl run messagepod --image=rednis:alpine -l tier=redis	<-- to create apod with image and labels

NOT if u forget to attach the labels

Â 	kubectl run messagepod --image=rednis:alpine

Â 	kubectl label pod messagepod tier=redis

Â 	kubectl get pods --show=labels    				<-- display all the labels

Â 	kubectl describe pod messagepod					<-- display the pod in detail

Â 	kubectl get pod messagepod  -o yaml | grep image:		<-- to display only the image on the screen
################################################################
Q6 Create a pod in the "tech" namespace

Â 	kubectl create namespace tech					<-- First create a namespace

Â 	kubectl run temp --image=redis:alpine 				<-- create pod with image

if u forget to create ns u add the pod in the default


Â 	kubectl run temp --image=redis:alpine 				<-- u create the pod in the default/falsch namespace

Â 	kubectl create namespace tech					<--	now create the right namespace if does not exists
Â 
Â 	kubectl get pod temp -o yaml > temp.yaml
Â 		kubectl get pod temp â†’ fetches the Pod named temp.
Â 		-o yaml â†’ outputs the Pod definition in YAML format.
Â 		> temp.yaml â†’ saves that YAML to a file named temp.yaml.
Â 		You can inspect, edit, or recreate the Pod using this YAML.

Â 	vi temp.yaml							<-- open the file make the changes (namespace)

Â 	kubectl apply -f temp.yaml					<-- Apply changes made to a resource without deleting it manually.

Â 	kubectl delete pod temp -n default				<-- delete the pod in  the falsche namespave

create the all in one line

Â 	kubectl run temp --image=redis:alpine -n tech

Check wheather it is running in the righht namespace

Â 	kubectl describe pod temp -n tech				<-- to see the detail in detail

Â 	kubectl get pods -A						<-- display all the pods in all the namespaces

################################################################
Q7: Create a pod  and expose it
Â    pod name
Â    image
Â    servicename
Â    port
Â    target-port:8080

Â 	kubectl run  connection --image=redis:alpine			<-- create apod

Â 	kubectl expose pod connection --name=connection-service --port=8080 --target-port=8080   <-- create a service with expose and connection means to
Â 
Â 
check
Â 	kubectl describe service/svc connection-service

Â 	kubectl get svc							<-- to check the services
Â 	kubectl get service
Â 	kubectl describe svc connection-service

################################################################
Rollout:
---

Â 	Note: Rolling-update is the default deployment strategy. if we do not define any strategy, then it select by-default "Rolling-update"
Â      	as a strategy.

Rolling-update Strategy:
---
Â 	In Rolling-update if we have 5 pods are already their, and we want to update them with new version, then while doing
Â 	deployment, it delete one pod and bring one new, in this way it goes.

Recreate Strategy:
---
Â 	In this case it first delete all the pods and after that it create all new pods at once, this strategy can have downtime
Â 	because at the time of first delete all the pods are deleted.

--record:
---
Â 	This helped track who changed what and why, especially when using kubectl rollout history.

---
Q8:  create a deployment with 3 replicas and upgrade using rolling update?

Â 	kubectl create deployment nginx-deploy --image=1.16 --replicas=3 --dry-run=client -o yaml > deploy.yaml
Â 
Â 	kubectl set image deployment nginx-deploy nginx=nginx:1.17 --record

Â 	kubectl get deployments -o wide

Â 	kubectl rollout history deployment/nginx-deploy

Â 	k rollout undo deployment nginx-deploy

OR

if i forget to create a replicas

Â 	kubectl create deployment nginx-deploy --image=1.16

Â 	kubectl scale deployment nginx-deploy --replicas=3
Â 
Â 			OR
Â 


Â 	kubectl create -f deploy.yaml  --record          <-- like this we can create a new Deployment and also recording it for the "k rollout" directly from first time.
Â                                          		Note: As at the creation time we put the flag "--record" now on every update of this deployment it will be automatically
Â                                              		saved versions for "k rollout" we do not need everytime to user this "--record" flag on any updation.
Â                                         		Note: if we do at the time of update a Deployment "--record" , if we do update after this one then we do not
Â                                               		need to put this "--record" flag again.



Â 	kubectl create -f abc.yaml       		<-- with this command we can create a deployment first time

Â 	kubectl apply -f abc.yaml         	        <-- As a deployment is already create, with this command it will update the Deployment
Â                                                            If their is no deployment then with this command we can also deploy it first time

Â 	kubectl set image   my-deployment my-container=nginx:1.9.1    <-- like this we can change the image of current running deployment
Â                                                                          my-deploment is the name of the Deployment
Â                                                                          my-container is the name of the Container


Â 	 k rollout status deployment/my-deploy      			<-- it will log the status of the deployment

Â 	 k rollout history deployment/my-deploy     			<-- it will print the history of the deployemnt

Â 	 kubectl rollout status my-deployment     			<-- with this we will get the status of the deplyoment

Â 	 kubectl rollout history my-deployment

Â 	 kubectl rollout undo my-deployment          			<-- with this we can rollback the changes to last deployed deployment.
Â 
Â 	 k rollout undo deployment my-deployment --to-revision=3        <-- like this we can rollback to any version

################################################################

Q9: Create a static pod and use the command "sleep 1000"

-	first we need to find the location of the static pod path and this will be find in the config file
-	we check the name of the node
-	go to the node
- 	we change the root user
-	check the static pod file in the config file

Â 
Â 	ps -aux | grep kubelet				<-- to find the config file (((JUst copy the path ))))
Â 
Â 	kubectl get nodes

Â 	minikube ssh					<-- minikube is the name of the node
Â 
Â 	sudo -i						<-- we can change the root user

Â 	cat /var/run/cri-dockered socks			<-- write the path of the file which we already copied after running the first step ps -aux-----

Â 	logout

Â 	kubectl run static-box --image=busybox --command sleep 1000 --dry-run=client -o yaml  > static.yaml
Â 
Â 	kubectl get nodes
Â 


################################################################
Taints:(Node) 		means(Only some Pods are allowed to run here)

Â 	A taint is a setting applied to a node that prevents/ask the Pods from being scheduled on it
Â 	only those pods are be scheduled who have a matching toleration.

Toleration:(pod)	means(it ask the pod to run on that node)
Â 	A toleration are applied on the pod, that tell the pod to run/scheduled on the matching taint(node).
Â 
Why use them?

Â 	To control which Pods run on which Nodes
Â 	(for example, database Pods only on powerful nodes)

Â 	To keep special workloads isolated (like system or GPU workloads)

---
Â 
Q 10: Taint a node to be unschedulable and test it b creating a pod on the node

Â 	kubectl get nodes			<-- to see how many nodes are created

Â 	kubectl describe nodes nodes01		<-- to see wheather the taint are exists on the node

Â 
Â 	Kubectl taint node nodename key=value:operator		<-- SYNTAX
Â 	Kubectl taint node node01 env_type=production:NoSchedule	<-- used to taint the node

check
Â 	kubectl describe node node01 | grep -i taint	<-- to display only taints( -i is used so that the taints can be written in any case.)
Â 
Â 	Kubectl describe node node01 | grep Taints

Now create a pod

Â 	kubectl run pod no-redis --image=redis:alpine	<-- to create a pod
Â 
Â 	kubectl get pods
Â 
Â 	kubectkl describe pods no-redis      <-- to display the pod on which node it is running on.

Â 	kubectl get pods no-redis -o wide    <-- to display the pod in detail in one line

Now create a second pod with tolerartion

Â 	kubectl run pods pro-redis --image=redis:alpine	--dry-run=client -o yaml | redis.yaml
Â 		kubectl run pro-redis â†’ creates a Pod named pro-redis
Â 		--image=redis:alpine  â†’ uses the Redis (Alpine) Docker image
Â 		--dry-run=client      â†’ doesnâ€™t actually create the Pod, just simulates it (no changes in the cluster)
Â 		-o yaml               â†’ outputs the Pod definition in YAML format
Â 		> redis.yaml          â†’ saves that YAML output into a file named redis.yaml
Â 
Â 	nano redis.yaml
Â 		namo  -> Opens a text file for editing.
Â 			 you write configuration files, scripts, YAML files (like redis.yaml), etc.
Â                         You can save changes, search, cut/copy/paste, and exit â€” all within the terminal.
Â 			 If the file exists, it opens it for editing.
Â 			 If it doesnâ€™t exist, nano creates a new file with that name
Â 		save it strg + o
Â 		exit it strng + x
copy the data from the documentation file
Â 	tolerations:
Â 	- key: "key1"
Â 	  operator: "Equal"
Â 	  value: "value1"
Â 	  effect: "NoSchedule"
save it strg + o
exit it strng + x


Â 	kubectl create -f redis.yaml
Â 		kubectl       â†’ the Kubernetes command-line tool.
Â    		create        â†’ tells Kubernetes to create a resource.
Â 		-f redis.yaml â†’ means â€œuse the configuration from the file redis.yaml.â€
check
Â 	kubectl describe pods pro-redis
Â 
Â 	kubectl describe pods pro-redis | grep -i node
Â 	kubectl describe pods pro-redis | grep  node

################################################################

the main reason to use  SVC for better security with the
combination of clusterrole and clusterrolebinding  we are able to decide what we can and we cannot do.
NOTE this is the first task when u want to secure the cluster.

--verb=list
Â 	When you create a Role or ClusterRole in Kubernetes, you specify what actions
Â       (verbs) the role is allowed to perform on certain resources.
Â 

---

Q11: create a service account, clusterroll and clusterrollbinding.
Â     Make it possible to list the persistent volumes.
Â     And createa apod with new service account
Steps to do:
Â 	create a service
Â 	create clusterrole with resources and verb
Â 	create clusterrolebinding & attached with cluserrole and serviceaccount
Â 	create a pod with yaml file
Â 	add the serviceaccount in pod yaml file under spec

practicle:

Â 	kubectl create serviceaccount nedserviceaccount 			<--create a service account

Â 	kubectl	create clusterrole pv-role --resources=persistentvolumes --verb=list	<--create a clusterrole

Â 		--resources=persistenvolumes: --> is the recource name when u want to see the list of resources just see the print out
Â 		     persistentvolumes are persistent storage resources in Kubernetes.
Â 		     A PersistentVolume (PV) is a cluster-wide storage resource that exists independently of any Pod or Namespace
Â 		     if the pod is deleted the data is in the PV is not lost
Â 	for More detail see word document PV/PVC
Â 		--verb=list means: verbs are actions you are allowed to perform on a resource.
Â 		    "Who can do what to which resource?" view types are:
Â 		     eg get/list/watch/create/update/patch/delete/deletecollection/exec/proxy.portforward/Impersonate.

Now we bind the cluster role
Â 	kubectl create clusterrolebinding pv-binding --clusterrole=pv-role --serviceaccount=default:nedserviceaccount


Â 	kubectl run pv-pod --images=redis --dry-run=client -o yaml > pod.yaml
Â 		<--Now we create the pod and save O/P to the yaml File

Â 	nano pod.yaml      <-- Edit the yaml file

Â 	under spec:
Â 	      serviceAccountName: nedserviceaccount    <-- save and close it strg + o and strg +x
Â 	kubectl create -f pod.yaml			<-- create a pod from the yaml file -f
Â 	kubectl describe pod pv-pod 			<-- check wheather the pod have the changes

################################################################
NetworkPolicy: is used to more secure the cluster

-	is a security rule in Kubernetes that controls how Pods communicate with each other and with external endpoints.
-	NetworkPolicy acts like a firewall for Pods â€” it decides who can talk to whom and on which ports.vvvv
By default, all Pods can talk to all other Pods, across all namespaces.
-	 No restrictions â†’ every Pod can connect to any other Pod or service.
Once you apply a NetworkPolicy, that changes.
Â 	 When you apply a NetworkPolicy to a Pod, all traffic not explicitly allowed is blocked.

Key Components of a NetworkPolicy
Â 	Field		Purpose
Â 	apiVersion	Always networking.k8s.io/v1
Â 	kind		Always NetworkPolicy
Â 	metadata	Name and namespace of the policy
Â 	podSelector	Selects which Pods the policy applies to (by label)
Â 	policyTypes	Defines if the policy controls Ingress, Egress, or both
Â 	ingress		Defines which inbound traffic is allowed
Â 	egress		Defines which outbound traffic is allowed

Letâ€™s say you have two types of Pods:
Â 	app: backend
Â 	role: frontend
You want only frontend Pods to access the backend Pods on port 80.
Allow Internet Access (Egress Policy)

_______________________________

Q12: create a NetworkPolicy that allows all the pods in   the "tech-deploy" namespace to have communication only on a single post
NOTE: it secure more our cluster.Network policy are apply only within a single namespace.
Step to do:
Â 	adding a label to the namespace
Â 	open documentation for example(service network) copy it
Â 	createown yaml file pate it
Â 	to select all namespaces that have the label app: tech-deploy(under igress)

Â 	kubectl label namespace tech-deploy app=tech-deploy
Â 	nano mp.yaml
Â 		apiVersion: networking.k8s.io/v1
Â 		kind: NetworkPolicy
Â 		metadata:
Â 		  name: tech-policy
Â 		  namespace: tech-deploy
Â 		spec:
Â 		  podSelector: {}
Â 		  policyTypes:
Â 		    - Igress
Â 		  igress:
Â 		    - from
Â 			-namespaceSelector:
Â 		    matchLabels:
Â 		       app: tech-deploy
Â 	          ports:
Â 	          - protocol: TCP
Â        	    port: 80
Â 	kubectl create -f mp.yaml     <-- create a networkpolicy with -f
Â 	kubectl describe networkpolicy tech-policy -n tech-deploy     <-- to see the networkpolicy in the namespace

################################################################
In this we look at the JASON path


Q13: List all the internal IP#s of all the nodes in the cluster and save it to /doc/ip_nodes.txt
-    we can copy the Imperative command from """kubectl cheat sheet"""

Â 		kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}' > /doc/IP_nodes.txt
Â 		change ExternameIP to InternalIP
Â 
wenn u donot have a directory just ceate a new one
Â 		mkdir -p/doc  <-- create a new directory in Unix
Â 		cd /doc	      <-- we can change the directory name
Â 		ls            <-- it display the list of the files
Â 
Â 		vi /doc/IP_nodes.txt
################################################################
Sleep (3600) command means:
Â 	 it means the container will not exit immediately after starting â€”
Â 	 instead, it will stay running (idle) for 3600 seconds (1 hour).
NOTE:	 sleep doesnâ€™t mean the Pod â€œgoes to sleepâ€ â€”
Â 	 it means the Pod stays alive but idle, doing nothing.
Â 	 that gives u the time to test,debug or inspect the pod while its running

Normally, a Pod stops when its main process finishes.
Â 	But if you use sleep, thereâ€™s no work being done â€” it just â€œwaitsâ€.
Â 	While itâ€™s waiting, the Pod remains in Running state.
Â 	.

eg. kubectl run test-pod --image=busybox -- sleep 3600
Â 	Creates a Pod named test-pod
Â 	Runs the sleep 3600 command (wait for 1 hour)
Â 	Keeps the Pod alive so you can test inside it
---

Q14: Create a multipod with two containers and Add the command "sleep 3600" to container 2

Â 	kubectl run mega --image=busybox --command sleep 3600 --dry-run=client -o yaml > multi.yaml

Â 	nano multi.yaml				<-- make the changes
Â 		- name:
Â 		  image:	save and exit

Â 	k create -f multi.yaml			<-- create a pod

Â 	k get pods -o wide

Â 	k get pods multi -o wide
################################################################

Q15 A new worker has joined your team. create a new user and grant him access to the cluster.
Â    He should have the permission to create, list,get,update and delete pods in the tech namespace
Steps:
Â 	we will do this through roll and rolebinding
Â 	createa namespace
Â 	check documentation
Â 	certificating signing request
Â 	create privatekey & certificate key
Â        apply it
Â 	approved
Â 	create role and rolebinding with imperative or declarative commands
Â 	check the role and rolebindings are created or not
Â 	checks wheather u can delete the podsor not

>	kubectl create namespace tech
>	kubectl create ns tech
Â 
>	openssl genrsa -out jan.key 3072
Â 		<-- tocreate a prive key and certificate signing request

>	openssl req -new -key jan.key -out jan.csr 

>	nano csr.yaml
Â 		<-- cop the context from the documentation file and change the name only
Â 		    save it and exit
> 	cat jan.csr | base64 | tr -d "\\\\\\\\n"
Â 		<-- copy the text

> 	nano csr.yaml
Â 		<-- delete the text in the request: and paste the new text in the request: field
Â 		    save it and exit

> 	kubectl apply -f csr.yaml 	
Â 		<-- we issued the certificate
> 	kubectl get csr

> 	kubectl certificate approve jan 

> 	kubectl get crs 
Â 		<<- u will see the certificate will be issued/approved


NOTE till now we just create a user and after that we give the permissions

> 	kubectl create role jan-role --verb=create,list,get,update,delete --resource=pods -n tech

>	kubectl create rolebinding jan-rolebinding --role=jan-role --user=jan -n tech

>	Kubectl get role -n tech   
Â 		<-- to check the role wheater it is created in the  tech namespace

> 	kubectl get rolebinding  -n tech
Â 		<-- to check rolebinding wheather the role bindings is created in the tech namespace

>	kubectl get rolebinding.rbac.authorization.k8s.io -n tech

> 	kubectl auth can-i delete pods -n tech --as jan
Â 		<-- to check wheather the pods can be deleted or not

################################################################
Q:16 Create a servicefrom the green pod and run DNS lookup to check the service and write to file /doc/lookup.txt
-service name: green-service
-port:80
Steps:
Â 	- check wheather the pods is created or not
Â 	- create a service with expose command
Â 	- create nslookup
Â 
> 	kubetcl get pods 
> 	kubectl run green --image=nginx
> 	kubectl expose pod green --name=green-service --port=80
Â 		<-- to create a service
>	kubectl run nslookup --image=busybox:1.28 --command sleep 3600
>	kubectl exec -it nslookup -- nslookup green-service 
Â 		<-- kubectl exec allows you to run a command inside a running Pod.
Â 		    This is useful for debugging, inspecting, or testing things inside the container.
Â                <-- -i â†’ interactive (keeps stdin open)
Â 		    -t â†’ allocates a pseudo-TTY
Â 		    -it allows you to interact with the Pod as if youâ€™re inside its shell.
Â 		green-service â†’ the service name you want to check.
Â 		Kubernetes automatically creates DNS entries for Services.
Â 		This command will return the ClusterIP of green-service, confirming the service is accessible.
Â 	After -- â†’ kubectl stops interpreting anything as its own flags and treats it as the command to run inside the Pod:
Â 		nslookup green-service â†’ this is run inside the container, not by kubectl itself

################################################################
Q:17. Create a secret and mount it to the pod "yellow"?
-  Secret name: yellow-secret
-  Secret content: password=kube1234

>	kubectl run yellow --image=nignx
>


################################################################
PV: (Persistent Volume)  PVC (Persistent Volume Claim)
-     A Persistent Volume (PV) is a piece of storage (like a disk, NFS share, or cloud volume) that is managed by the cluster
Â      and keeps data even if a Pod is deleted or restarted.
-     A Persistent Volume (PV) is a way for Pods to store data that doesnâ€™t disappear when they stop or restart.

Pod	<-- Runs containers â€” data is lost if it restarts
PV  	<-- Permanent storage in the cluster
PVC 	<-- A request by a Pod for storage

Q18:   List all the PV stored by capacity and write to the file /doc/pervol.txt

>	kubectl get pv

> 	check cheatsheet

>	kubectl get pv --sort-by=.spec.capacity.storage
Â 			<-- list pv stored by the capacity

> 	kubectl get pv --sort-by=.spec.capacity.storage > /doc/pervol.txt
Â 			<-- save the list in the file

>	cat /doc/pervol.txt
Â 			<-- check wheather  the list is saved in the file
################################################################

Q: 19 From the pod level environment =process, find all the pods running high cpu worksloads and write the name of which is consuming the most CPU to the file /doc/cpu.txt



################################################################

Q: 20 JSON Path to get all the node names and stored them in the file /doc/nameofnodes.txt
steps :
Â 	go to cheat sheet
Â 	copy the path under externalIP of NODE
Â 	check the file wheather the file is there if not create it
Â 	make the changes and sae he data in the file
Â 
> 	kubectl get nodes -o jsonpath='{.items\\\\\\\[\\\\\\\*].metadata.name}' > /nameofnodes.txt

> 	vi nameofnodes.txt
Â 
################################################################
Q21: Show thr logs ro he container and save it to /doc/nginx.log
Â     - Pod name:direct-pod
Â     - Container:nginx
Â     - Namespace: dev-net
steps:

>	kubectl get pods -n dev-net -o wide
Â 		<-- to check wheather the pod is running in the namespace

>	kubectl logs direct-pod -c nginx -n dev-net
Â 		<-- to check the logs of the pods

> 	kubectl logs direct-pod -c nginx -n dev-net > /doc/nginx.log
Â 		<-- save thelogs in the file

> 	nano /doc/nginx.log
Â 		<-- check the contents of the log file

################################################################
Ingress:
-   Ingres in Kubernetes means incoming traffic to
Â    your cluster.
-   Itâ€™s how external users (for example, web
Â    browsers) access your services inside the
Â    cluster.
-   Usually, you define an Ingress resource to
Â    manage HTTP and HTTPS routes.
-   The Ingress Controller (like NGINX or Traefik)
Â    listens for these rules and routes traffic to
Â    the correct Service inside your cluster.
Egress
-   Egress means outgoing traffic from your cluster
Â    to the outside world.
-   eg. when a pod in your cluster connects to an
Â    external API, downloads updates, or sends data
Â    to a database outside the  cluster.
-   You can control egress traffic using Network
Â    Policies, firewalls, or egress gateways
Â    (like in Istio).


Ingress and Egress rules are not applied only on the namespace itself, but they work within a namespace and control traffic to/from the Pods in that namespace.

âœ… What this really means

In Kubernetes NetworkPolicies have two types of rules:

Ingress â†’ what traffic is allowed into Pods

Egress â†’ what traffic is allowed out from Pods

ğŸš© Important points

NetworkPolicies are created inside a namespace

They only affect Pods in that same namespace

They do NOT block traffic at the entire namespace boundary, only at the Pod level




Q22. create a new ingress resources and expose service "hello" on path/hello by using service port 5678
steps:
-  create a namespace if it is not created
-  create a ingress yaml file
-  copy the data from the ingress recources from documentation
-  save it
-  k create -f ingress.yaml  to save thze data
-  double check it wheather it is created ina namespace or not

> kubectl create namespace host-dev

> nano ingress.yaml
Â 	apiVersion: networking.k8s.io/v1
Â 	kind: Ingress
Â 	metadata:
Â 	  name: ingress-connect
Â 	  namespace: host-dev
Â 	spec:
Â 	  rules:
Â 	  - http:
Â 	      paths:
Â 	      - path: /hello
Â 	        pathType: Prefix
Â 	        backend:
Â 	          service:
Â 	            name: hello
Â 	            port:
Â 	              number: 5678

>  create ingress -f ingress.yaml

>  kubectl get ingress -n host-dev
Â 	<-- double check it

################################################################
Â 
Q 23: Overwrite the labels of the den-nginx pod with the value env=true.
steps:
-  create a pod with image and labels
-  change the labels names
-  check whether it is changed or not

> kubectl run dev-nginx --image=nginx -l env=green

> k describe pod dev-nginx


> kubectl label pod dev-nginx env=true --overwrite
Â 		OR
> kubectl label pod/dev-nginx env=true --overwrite



> kubectl get pod dev-nginx --show-labels
Â 		OR
> kubectl describe pod dev-nginx
Â 		OR
> kubectl describe pod dev-nginx | grep Labels
Â 		OR
>kubectl describe pod dev-nginx | grep -i labels
Â 	<-- to see the labels whether it is changed or not


################################################################
Q24: Upgrade the image in the deployment "green" to
Â     "busybox:1.28", check the history and roll back
steps:
-   create the deployment
-   check the image name
-   change the image name of he eployment
-   check weather the image name is changed or not
-   rollout the image
-   check again it will be rollback or not

> kubectl describe deployment green | grep -i image

> kubectl create deployment green --image=busybox 

> kubectl describe deployment green | grep -i image

> kubectl set image deployment green busybox=busybox:1.28

> kubectl describe deployment green | grep -i image

> kubectl rollout undo deployment green

> kubectl describe deployment green | grep -i image



################################################################
Q25: Find out how which pods are available with the
Â     label env=green in the cluster and write them
Â     to the file /doc/podsavailable.txt
steps:
-   check how many pods are created
-   write the pods in the text file
-   check the file

> kubectl get pods -l env=green
Â           OR
> kubectl get pods --selector env=green --no-headers | wc -l

> kubectl get pods -l env=green > /doc/podsavailable.txt

> cat /doc/podsavailable.txt

################################################################
Q 26:  The pod "red is failing. Find out why and
Â       fix the issue.

steps:
-  first check the pods (see crashloopBackoff)
-  go inside the pods (describe)
-  create/open the pod in yaml file/format
-  correct it https://uklabs.kodekloud.com/topic/practice-test-labels-and-selectors-2/
Now
-  u have to first delete the pods
-  create  a pod again(while u are not allow to make the changes while the pod is running)

Practicle:
---

> kubectl get pods red -o wide

> kubectl describe pod red
Â 	<-- to see which type of error message

> kubectl get pod red -o yaml > red.yaml

> kubectl delete pod red 

> nano red.yaml
Â       <-- correct it
> kubectl apply -f  red.yaml


> kubectl get pod red -o wide 
Â 	<-- it is in running state
Â 
################################################################
Q 27: create a pod that will only be scheduled on a
Â      node with a specified label?
Â      Pod name: blue
Â      Image : nginx
Â 
stepsToDo:
- first create a lable on the node
- create a pod with yaml file
- open the yaml file and make the changes.
- create a pod again

> kubectl get nodes

>  kubectl label nodes controlplane disk=green 

> kubectl run blue --image=nginx --dry-run=client -o yaml > blue.yaml
> nano blue-pod.yaml

spec:
Â   nodeSelector:
Â      disk=green
>  kubectl create  -f blue-pod.yaml

> kubectl get pod blue -o wide

> kubectl describe  node controlplane 

Â 
################################################################
Q 28:  Create pod which uses a persistent volume for storage
- pod yellow
- image: busybox
- pv name
- pv claim
- pv size

StepsToDo
- fist create a pv
- persisten volume claim(pv)
- create a pod with volumes

Â 
>   kubectl get pv 
Â 
>   vi yelow-pv-volume.yaml
Â 	-- to create a pv just copy the data from the documentation

>   kubectl create -f yellow-pv-volume.yaml
Â 	<-- create a pvc als yaml

>   kubectl get pv

>   vi pvc.yaml
Â 	<-- create a persistent volume claim and copy from the documentation

>   kubectl create -f pvc.yaml


>   kubectl run yellow-pod --image=busybox --dry-run=client -o yaml > yellow-pod.yaml
Â 	<-- create a pod with yaml file copy the text from Documentation und add mounts and volume
Â 
apiVersion: v1
kind: Pod
metadata:
Â  labels:
Â    run: yellow
Â  name: yellow
spec:
Â  volumes:
Â    - name: pv-storage
Â      persistentVolumeClaim:
Â        claimName: pvc-yellow
Â  containers:
Â  - image: busybox
Â    name: yellow
Â    resources: {}
Â    volumeMounts:
Â      - mountPath: /data
Â        name: pv-storage
Â  dnsPolicy: ClusterFirst
Â  restartPolicy: Always
status: {}

>   kubectl create -f yellow-pod.yaml

################################################################
Q30:











################################################################
Q31: (Labe 63)
Schedul a pod on the node "minikube-m2" by using tolerations
- Pod name: blue-pod
- image:nginx

Setps to do:
- first check the taint on the node
- create a pod with dry-run a yaml file
- copy the data from the documentation
- update the pod through create -f command
- check the pod  on which node it is running through toleration

> kubectl describe node node01 | grep -i taint

> kubectl run blue-pod --image=nginx  --dry-run=client -o yaml > blue-pod.yaml

> vi blue-pod.yaml

> kubectl create -f blue-pod.yaml

> kubectl get pod blue-pod -o wide

################################################################
HPA: Lists all Horizontal Pod Autoscalers (HPA) in the current namespace.
Â 	kubectl get hpa
Â 		<-- display how many min amd max deployments are created and how many replicas are created automatically
Â 
Q32:  (LAB 35)
Apply autoscaling to the "green-deployment" with a
minimum of 5 and maximum of 10 replicas and target CPU of 75%

SetpsToDo:
- firts check the deployment if it is not created then create a new one
- apply the needs
- check it eheather it is all there or not

>   kubectl get deloyments

>   kubectl create deployment green-deployment --image=nginx

>   kubectl autoscale deployment gree-deployment --min=5 --max=10 --cpu=75%

>   kubectl get hpa

>   kubectl get pods 

################################################################
to create a directory:

mkdir: Stands for "make directory".
-p: Stands for "parents".
Â    This option ensures that all necessary parent directories in the path are created automatically if they don't already exist.

/doc: This is the absolute path and the name of the directory to be created.


Q 33: LAB(67)
Check how many nodes are in ready state and write it to the file /doc/readynodes.txt

StepsToDo:
NOTE: you have to create a directory there is no need to create a file. it will be automatically created when u run the command
-   First create a directory
-   check the nodes wheather it is on ready state
-   list only the ready nodes through grep command
-   save it in the directory and in dile

>   mkdir -p /doc

>   kubectl get nodes

>   kubectl describe nodes | grep ready 

>   kubectl describe nodes | grep ready > /doc/readynodes.txt


################################################################
Q34:  (difficult)
kubectl exec -it grey --sh -c 'echo $dev' dev10
Â 	<-- This command executes a shell command inside a running Kubernetes pod named grey.
Â        kubectl exec: Executes a command inside a container.
Â       -it: Combines -i (interactive) and -t (terminal), which are necessary for interacting with the shell session.
Â        grey: The name of the target pod where the command will run.
Â        --sh -c '...': Starts a shell (sh) inside the pod to run the specific command provided (-c 'echo $dev').
Â        'echo $dev': The actual command being run: it prints the value of the environment variable dev, as it is defined inside that specific pod.


Create a pod and set the environment variable "dev=dev10"


################################################################

How to create a config map
Confi maps are great way to store data and make it easier to organizer your data and edit when necassery

What is CpnfigMap
Â 	A ConfigMap is a Kubernetes API object used to store data in key-value pairs.
Â        Pods can consume this data as environment variables, command-line arguments, or mounted configuration files.

kubectl create configmap my-config --from-literal=APP_MODE=production
Â 	<-- createa configMap from a literl key-value pair

kubectl create configmap my-config --from-file=config.json
Â 	<-- create a configMap from a file

kubectl create configmap my-config --from-file=./config-dir/
Â 	<-- create a ConfigMap from  directory

Q 35:   (lab 113)
Â Create a configmap and add it to the pod
StepsToDo:
-    Checks the pods in default and all the pods in every namespace
-    create a cofigmap
-    describe the Config map
-    displa all the pods on the efault namespace
-    the pod is created and num create a yaml file
-    make the changes with envFrom:
-    delete the old pod and create a new pod
-    check wheater all are right or not

> kubectl get pods -A

> kubectl create configmap data-config --from-literal=user=root --from-literal=password=pass1234

> kubectl get configmap

> kubectl describe configmap data-config

> kubectl get pods

> kubectl get pod -o yaml > web-color-config.yaml 	

> kubectl exec -it webapp-color -- env
Â     <-- It opens an interactive session in the webapp-color pod and shows all environment variables.
Â     exec â†’ run a command inside a running Kubernetes pod
Â      -it â†’ interactive (-i) with a terminal (-t)
Â     webapp-color â†’ the name of the pod
Â     -- â†’ separates kubectl options from the command
Â     env â†’ prints all environment variables inside the container

NOTE: it display the information of the Container running inside the pod

################################################################
Q36
List all the events stored by the timestamp and write the result to file /doc/events.log
StepsToDo:
-   first open the cheatsheet
-   copy the command and paste it on the lab
-   save the data in the given text file

>   kubectl get events --sort-by=.metadata.creationTimestamp 

>   kubectl get events --sort-by=.metadata.creationTimestamp > /doc/events.txt

>   cat /doc/events.txt
################################################################

A non-persistent volume is storage that exists only as long as the Pod is running.
If the Pod is deleted or recreated â†’ the data is lost.

Q37 Create a pod with a non-persistent volume
StepsToDo:

- copy the data from emptyDir:{} Documentation
- create a yaml file open it and make the changes
- create a yaml file
- and check wheater it is created or not

> kubectl get pods

>  nano nonper-pod.yaml
Â 
apiVersion: v1
kind: Pod
metadata:
Â  name: nonper-pod
spec:
Â  containers:
Â  - image: redis
Â    name: redis
Â    volumeMounts:
Â    - mountPath: /data//per-redis
Â      name: data-volume
Â  volumes:
Â  - name: data-volume
Â    emptyDir: {}

>  kubectl create -f nonper-pod.yaml 

>  kubectl get podnonper-dod -owide

> kubectl descibe po noner-pod 


################################################################
In this my node is not in working state and nun how to check the



Q 38:

The node "miniku" is in "NotReady" state. Investigate and bing the nde bac to ready state
StepsToDo:
- Firts check the satate of the node
- describe the njode
- ssh in the node
- we change o rot
- check the kubelet (it is aktiv)
- lets start the kubelet again
- check the state again  and it in running state


################################################################

Cordon a node
Â     To cordon a node means to mark it as unschedulable.
Â     New pods will NOT be placed on that node, but existing pods
Â     stay running.

Drain a node
Â     To drain a node means to evict (move) all running pods from
Â     that node.
Â     Kubernetes reschedules the pods onto other nodes.
Â     Used when you want to maintain, update, or shut down a node
Â     safely.


Cordon and drain a node
We did it because the node is unavailable and reschedule all
Â   the pods.
The main reason is because we have to upgrade the node
We cannot upgrade the nodes when the pods are running on it

Q: 39:

StepsToDo:
- first On which pods are nodes are running on
- how many nodes are available
- cordon a node so that we are not able to Schedule any pods
Â  anymore
- Drain the nodes so that we schedule all the pods on the another
Â  nodes
- check wheather all the pods are running on another node

> kubectl get pods -o wide

> kubectl get nodes

> kubectl cordon mininkube

> kubectl drain minikube --ignore-daemonsets --force

> kubectl get pods -o wide

> kubectl get nodes

################################################################


Q 40:
Â Create a pod that echo go to tech-educator.com and then exists.
Â The pod should be deleted automatically when it is completed?


> kubectl run tech-pod --image=busybox  -it --rm --restart=Never 
Â                 -- /bin/sh -c 'echo go to the tech-educator.com'
Â 
Â        -it --- Runs the container in interactive mode with a TTY
Â                (like opening a terminal).
Â 	--rm ---Delete the pod automatically when it finishes.
Â        --restart=Never ---- Runs it as a one-time pod, not a
Â                             Deployment or ReplicaSet.
Â        -- /bin/sh --- this starts the shell inside the container
Â        - c ------Run the command that comes next as a string.â€

################################################################

Q41: Annotate the existing pod "yellow" and use the value
Â     "name=yellow-pod"

>  kubectl get pods 
Â 	if it is not created just create it

>  kubectl run pod yellow --image=nginx

>  kubectl annotation pod yellow name=yellow-pod
Â 	to write the annotation for more information

> kubectl describe pod yellow
Â 	OR
> kubectl describe pod yellow | grep -i annotation 
################################################################
Q42 Get a list of all the pod s in all the namespaces  and write
Â    it to the file/doc/podsnamespace.txt

> kubectl get pods 

> kubectl get pods --all-namespaces

> kubectl get pods --all-namespaces > /doc/podsnamespaces.txt
################################################################
Q43:


>   kubectl describe configmap config-green

>   kubectl describe confimap config-green -o wide > 
Â    config-green.yaml

>   vi config-green.yaml

>   kubectl replace -f config-green.yaml

>   kubectrl describe configmap config-green 

>   kubectl get pods 

>   kubectl describe pod green 

>   kubectl  exec - it green -- env
Â 	<-- hier u see that the password is not be changed

>   kubectl get pod green -o yaml | kubectl replace --force -f-
Â 	<-- the pod will be deleted and it replace the pod in
Â            one command explain it

Â       --- kubectl get pod green -o yaml
Â            <-- Gets the Pod named green and outputs its full
Â                 YAML.
Â       --  |
Â            <-- Sends that YAML to the next command.
Â       --  kubectl replace --force -f -
Â            <-- Deletes and recreates the pod using the YAML
Â                from the first command.
Â            (--force makes it do a delete + create instead of an
Â               update. -f - means read from stdin
################################################################
Q44: you just create the pod "blue" bit it is not scheduling on
Â      the node. Troubleshoot and fix the issue

StepsToDo:
-  First check the status of the pod
-  check that may be he type error
-  before we do anything first check the lables of the nnodes
- kubectl get nodes




> kubectl get pod blue -o wide

> kubectl describe pod blue

> kubectl describe node minikube | grep Labels 

> kubcetl get pod blue -o wide yaml > bluenoschedule.yaml

> vi blueschedule.yaml
Â 	<<--make the changes

> kubectl replace --force -f bluenoschedule.yaml

> kubectl get pods blue -o wide 

> kubectl describe pod blue 

################################################################

Q 45: Create a network policy and allow the traffic from the
Â      "green " pod to the finance-service and the data service
- policy name : internalpolicy
- policy type : egress
- Label pod: role=post
- Egress Allow: finance
- Finance Port: 8080
- Egress Allow: data
- Data Port: 5432

SetpsToDo:
NOTE he did not copy the thng he wrote it

################################################################
Q46:Create a pod and set "SET_TIME" + sleep 3600 secons
-podname : grey
- image: busybox

SetpsToDo:
- create a pod with sleep yaml file
- copy the data from the kubernets documentation
-

>   kubectl run grey --image=busybox --command sleep 3600 
Â    --dry-run=client > greypod.yaml

>   nano greypod.yaml   
Â      <--- paste the data from dokumentaion (SYS\_TIME)
Â       securityContext:
Â         capabilities:
Â           add:
Â            - SYS_TIME


>  kubectl create -f greypod.yaml

>  kubectl get greypod.yaml

>  kubectl get pod grey -o wide
Â 	<-- to see wheater the pod is running or not

>  kubectl get pod grey -o yaml    

################################################################
Q 47:  (Lab 175/
Create a clusterroll and clusterrolebinding which "get "watch"
and list access to the pod
- Clusterrole name: cluster-administrator
- Clusterrolebinding name: Clusterrolebinding-administrator
- serviceaccount: admin-sa


StepsToDo:
-  Create a cluster role
- create role binding
- check whether it is created or not

>  kubectl create  clusterrole cluster-administrator 
Â   --verb=get,watch,list --resource=pods

> kubectl create clusterrolebinding clusterbindingadministrator 
Â  --clusterrole=cluster-administrator
Â  --serviceaccount=default:admin.sa

> kubectl auth can-i list pods --as 
Â  system:srviceaccount:default:amin-sa

Â   2. auth can-i
Â 	Asks the API server â€œCan this identity perform this
Â        action?â€
Â 	It checks RBAC permissions.
Â 
Â   3. list pods
Â 	The action (list) and the resource (pods).
Â 	This asks: Is listing pods allowed?

Â   4. --as system:serviceaccount:default:admin-sa
Â 	Runs the check as if you were this identity.
Â 	system:serviceaccount: = prefix for all service accounts
Â 	default = namespace of the service account
Â 	admin-sa = name of the service account

Â   5. What it actually checks
Â 	It evaluates the RBAC rules of that service account.
Â 	It does not switch namespace unless you specify
Â        --namespace.

Â   6. Possible output
Â 	yes â†’ the service account can list pods
Â 	no â†’ it cannot
Â 	Add --verbose for explanation.
Â 
Â   7. Common pitfalls
Â 	Spelling must be correct: serviceaccount, not
Â        srviceaccount or amin-sa.
Â 	Impersonation must be allowed on the cluster for --as
Â        to work.
Â 	If checking in a specific namespace, add --namespace <ns>.

################################################################
Q 48:
Get the ipAddress of the blue podand write tit to the
Â file/doc/ip.txt

StepsToDo:
First check the pods and labels
dispay the ip address

>   Kubectl get pods 

>   kubetl decribe pod blue | grep labels

>   kubectl get pods -l run=blue -A -o jsonpath=
Â    '{range.items\[\*]}{@.status.odIP}{""}{"\\n}{end}'
Â 		<--  to get the ip address

> Â kubectl get pods -l run=blue -A -o jsonpath=
Â   '{range.items\[\*]}{@.status.odIP}{""}{"\\n}{end}'
Â   > '/doc/ip.txt
Â 	<-- it is used to save the ip address in the file


################################################################
Q 49: find out the version of the cluster and write it to the
Â       file  /doc/versioncluster.txt

> kubectl get nodes 
Â 	<-- it display the version of the cluster

> kubectl get nodes > /doc/versioncluster.txt
Â        <-- save the data in the file

################################################################
Satefulset:
Â    A StatefulSet is a Kubernetes controller that runs stateful
Â    applications â€” apps that need:
Â    - fixed pod names
Â    - fixed network addresses
Â    - their own persistent storage
Unlike Deployments, StatefulSet pods are not identical. Each pod
has its own identity (like app-0, app-1, app-2) and keeps it even
after restarting.

Mountpath:
Â  In Kubernetes, mountPath is the path inside a container where
Â  a volume is mounted.
Â  mountPath is the folder inside a container where Kubernetes
Â  puts a volume.

Why do  we need a volume?
Â  A volume gives a container a place to store files that do NOT disappear when the container restarts.
Â  - Containers normally lose their data when they stop.
Â  - Volumes solve this by keeping the data safe.

Q 50:  Change the mountpath of the nginx container in the "online " staeful to 2/usr/share/nginx/updateed-html

SetpsToDo:
Â firts check the steful

> kubectl get statefulset -o wide

> kubectl get statefulset online -o yaml > stateful.yaml

> nano stateful.yaml
Â 	<-- just change the path in th mountpath field

> kubectl replace -f stateful.yaml

>

> kubectl get stateful 
Â 	<-- to check the stateful

> kubectl describe stateful online 


################################################################

A Cron Job is an automatic(scheduled) task in Linux/Unix systems that runs by itself at a specific time, date, or interval.
  It works in the background, so you donâ€™t need to run the command manually every time.
Examples:
Taking a backup every night at 12 AM
Updating a file every hour
Generating a report every month

Where are Cron Jobs defined?
   Cron jobs are defined in a file called crontab.

Q 51:
Create a cronjob which printsthe date and "Running" every minute

StepsT Do:
- go to he dokmentation 
-  lookfor the cronjob example copy it and psste (every minute)
- create a new yaml file 
- change the o/p to running and pod name
- run create -f
- check the cron file with get/ describe .

>   nano cronjob.yaml

>   kubectl create -f cronjob.yaml

>   kubectl get cronjob

>   kubectl describe cronjob show-date-job


################################################################

Q 53:(lab (193)
Create a networkpolicy and allow traffic from all the pods in the dev-tech namespace and 
from pods with he label "type=rewiew to the pods matching the label "app=postgres"

StepsToDo:
- first check the namespace whether it is created or not create it 
- create a label on the namespace 
- create a yaml file (nano)
- copy the data from the documentation network policy till matchlabels
- after copy the data from ingress
- create a file .yaml -f 

> kubectl create namespace dev-tech

> nano dev-tech-tc.yaml

  apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-dev-tech
    namespace: default
  spec:
    podSelector:
      matchLabels:
        app: postgress
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            app: dev-tech 
        podSelector:
          matchLabels:
            type: review

> kubectl create -f dev-tech-tc.yaml


################################################################
Q 54: 

Create a pod with container port 80. It  should check the pod running at endpoint/healthz on port 80 + verify +delete the pod 

pod name: health-pod
image: nginx

StepsToDo: 
- firs create apod with image ...
- create a yaml file 
- create a file -f (live update ) 
- check whether its work or not 
- verify it 

> kubectl run health-pod --image=nginx  --port=80 restart=Always --dry-run=client -o yaml > healthcheck.yaml

> vi healthcheck.yaml

	livenessProbe: 
	httpGet: 
	path:/healthz
	Port:80 
	initialDelaySeconds: 30

> kubectl create -f healthcheck.yaml

> kubectl describe pod health-pod  | grep -i liveness

> kubectl delete pod health-pod 

################################################################

Q55:  Monitors the logs of the pod "yellow" extract all the log lines matching with "not found" and write to the file/doc/failed.log

StepsToDo: 










___________________________________________________________________________________________

All the Definations

Satefulset:
Â    A StatefulSet is a Kubernetes controller that runs stateful applications â€” apps that need:
Â    - fixed pod names
Â    - fixed network addresses
Â    - their own persistent storage
Unlike Deployments, StatefulSet pods are not identical. Each pod has its own identity (like app-0, app-1, app-2) and keeps it even after restarting.

Mountpath:
